{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build GTP from Scratch, in code , spelled out\n",
    "\n",
    "- ChatGPT is a system that allows us to interact with an AI and give it text-based tasks.\n",
    "- it generates words sequentially, and it is a probabilistic system (for any same prompt, it can generate multiple answers)\n",
    "    - mine: so just like what we did when we used the . token as a start to generate different names in the previous notebooks\n",
    "- it is what we call a `Language Model` -what we have been building so far-\n",
    "    - it models the sequence of tokens and knows how they follow each other in a language\n",
    "        - in bigram we modeled the counts of each pair of tokens in the dataset\n",
    "        - in the MLP we fed it multiple token sequences (3 or 4 up to 8 tokens) -> result token\n",
    "    - from its perspective, what it is doing is that it is completing the sequence (the prompt we give it is the start of the sequence, and it completes it)\n",
    "\n",
    "- so what is the neural network under the hood that models the sequence of these words?\n",
    "    - it comes from a paper called [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017), it proposed the `Transformer` architecture\n",
    "        - so GPT is an abbreviation for `Generatively Pre-trained Transformer`\n",
    "    - if we read the paper, it reads like a random machine translation paper, that is because they didn't fully anticipate the impact of the model they proposed\n",
    "    - the architecture they produced in the context of machine translation, ended up taking over the rest of AI in the 5 years after\n",
    "        - this architecture with minor changes was copy-pasted into a huge amount of applications in AI (such as ChatGPT), it is trained on a good chunck of the internet, and there is a lot of pre-training and fine-tuning stages to it\n",
    "\n",
    "- mine: in Course 5 in deep learning specialization, we studied the language model (which is one-to-many architecture), and studied its behaviour\n",
    "    - during training, we feed it the actual sequence of tokens, and make it predict the next token in the sequence (we take the predicted probability of the actual next token and we maximize it -or more precisely minimize the negative log of it-)\n",
    "        - this is exactly what we have been doing so far\n",
    "    - during inference, we can \n",
    "        - feed it a sequence of tokens and check the probability of that sequence, or \n",
    "        - use it to generate tokens (give it none or the start token and make it generate the next token, then sample from the output distribution, and feed it back to the model to generate the next token, and so on)\n",
    "        - this is what we have been doing in the previous notebooks\n",
    "    - but then we studied the encoder-decoder architecture (which is many-to-many architecture), and its applications in image captioning and machine translation\n",
    "        - in image captioning, we feed the image to the encoder and get a fixed-size representation -that represents the image- and feed it to the decoder along with the start token to generate the caption\n",
    "            - also the behavior is different during training (we feed it the actual sequence of tokens along with the image representation and make it predict the next token in the sequence) and during inference (we feed it the image and the start token and make it generate the next token)\n",
    "        - in machine translation, we feed the source sentence to the encoder and get a fixed-size representation -that represents the source sentence- and feed it to the decoder along with the start token to generate the target sentence\n",
    "            - also the behavior is different during training and during inference\n",
    "        - the decoder in encoder-decoder architecture is similar to the language model (in one-to-many architecture), but there are 2 differences:\n",
    "            1. the decoder in encoder-decoder architecture takes an vector representation of some input (image in image captioning, source sentence in machine translation) and the start token, and generates the next token\n",
    "                - so we can think of the decoder in the encoder-decoder architecture as a conditional language model (it generates the output sequence conditioned on some input)\n",
    "                - unlike the language model, we gave it the start token or none and it generated the next tokens\n",
    "            2. in the encoder-decoder architecture, we want to generate the most likely output sequence given the input (image in image captioning, source sentence in machine translation), we don't just sample from the output distribution like we did with the language model in one-to-many architecture\n",
    "        - then we studied using the encoder-decoder architecture with the attention mechanism, (instead of using the same encoded vector representation of the input in every step of the decoder) we feed the decoder with a context vector that is a weighted sum of the encoded input \n",
    "            - like in image captioning, we feed the encoded image with different weights in every step of the decoder to tell it which part of the image to focus on when generating the next token\n",
    "            - in machine translation, we feed the encoded source sentence with different weights in every step of the decoder to tell it which part of the source sentence to focus on when generating the next token (so the decoder takes the context vector and the previous activation to generate the next token)\n",
    "        - then we studied the transformer architecture, which is much more complex than the previous architectures\n",
    "            - it processes the entire sequence at once (unlike the RNNs that process the sequence one token at a time)\n",
    "            - we know that the same word can have different meanings depending on the context it is used in -the surrounding words-, so the transformer uses something called `self-attention` to tune the representation of each word in the sequence based on the other words in the sequence\n",
    "                - and we do that multiple times (called multi-head attention) to have multiple perspectives on each word in the sequence\n",
    "                - read my notes for more details\n",
    "\n",
    "        \n",
    "- in this notebook, we will like to build out something like chatGPT, we will just focus on training a transformer-based language model (will be a character level language model), and we will train on a smaller dataset (Tiny Shakespeare dataset, which is a concatenation of all of Shakespeare's works)\n",
    "    - and the transformer will model how these characters follow each other \n",
    "    - so it can take a chunk of characters (context of characters) the transformer will look at the characters and predict the next character\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shakespear.txt\",\"r\",encoding='utf-8') as file:\n",
    "        text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are working with 1M characters roughly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary, encoding (numericalization), and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {char:i for i,char in enumerate(chars)}\n",
    "itos = {i:char for i,char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 14,\n",
       " 'C': 15,\n",
       " 'D': 16,\n",
       " 'E': 17,\n",
       " 'F': 18,\n",
       " 'G': 19,\n",
       " 'H': 20,\n",
       " 'I': 21,\n",
       " 'J': 22,\n",
       " 'K': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'W': 35,\n",
       " 'X': 36,\n",
       " 'Y': 37,\n",
       " 'Z': 38,\n",
       " 'a': 39,\n",
       " 'b': 40,\n",
       " 'c': 41,\n",
       " 'd': 42,\n",
       " 'e': 43,\n",
       " 'f': 44,\n",
       " 'g': 45,\n",
       " 'h': 46,\n",
       " 'i': 47,\n",
       " 'j': 48,\n",
       " 'k': 49,\n",
       " 'l': 50,\n",
       " 'm': 51,\n",
       " 'n': 52,\n",
       " 'o': 53,\n",
       " 'p': 54,\n",
       " 'q': 55,\n",
       " 'r': 56,\n",
       " 's': 57,\n",
       " 't': 58,\n",
       " 'u': 59,\n",
       " 'v': 60,\n",
       " 'w': 61,\n",
       " 'x': 62,\n",
       " 'y': 63,\n",
       " 'z': 64}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] # takes a string, output list of integers\n",
    "decode = lambda l: ''.join(itos[i] for i in l) # takes the list of integers and return the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46, 47, 47, 1, 58, 46, 43, 56, 43]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('hii there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hii there'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([46, 47, 47, 1, 58, 46, 43, 56, 43])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is only one of many possible tokenizers\n",
    "- but there are many other schemas that people came up with in practice\n",
    "    - for example, `google` uses `SentencePiece`, which is a byte pair encoding tokenizer\n",
    "        - it is a subword tokenizer, which is usually what adopted in practice (something in between character level and word level)\n",
    "    - `openAI` uses a library called `tiktoken`, that uses a byte pair encoding tokenizer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2') # we are getting the encoding used in gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- they have 50K tokens in their vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71, 4178, 612]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"hii there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we encoded the exact same string, we got a list of 3 integers (meaning the above text is 3 tokens)\n",
    "- so basically, you can trade-off the vocabulary size with the sequence length\n",
    "    - so you can have a very long sequences of integers with very small vocabularies\n",
    "    - or you can have a short sequences of integers with very large vocabularies\n",
    "- people typically use subword tokenization in practice, but we will keep our tokenization simple and use character level tokenization (very small vocabulary and very long sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we can tokenize the entire training set of shakespear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115394]), torch.int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text),dtype=torch.long)\n",
    "data.shape, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
       "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
       "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
       "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
       "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
       "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
       "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
       "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
       "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
       "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
       "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
       "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
       "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
       "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
       "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
       "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
       "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
       "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
       "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
       "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
       "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
       "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
       "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
       "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
       "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
       "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
       "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
       "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
       "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
       "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
       "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
       "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
       "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
       "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
       "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
       "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
       "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
       "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
       "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
       "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and validation sets\n",
    "- we will take the first 90% of the dataset as the training set and the last 10% as the validation set\n",
    "- this will help us understand to what extent the model is overfitting (whether the improvement in the loss is actually due to the model learning the data or just memorizing the data)\n",
    "- that is because we don't want a perfect memorization of this exact shakespear text, we want a neural network that creates shakespear-like text (generalizes to shakespear-like text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111540)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader: batches of chunks of data\n",
    "\n",
    "### Chunking the data \n",
    "- we would like to start plugging this text sequence (more precisely integer sequence) into the transformer, so that it can train on them and learn those patterns\n",
    "\n",
    "- it would be computationally expensive to feed the entire dataset sequence at once to the transformer\n",
    "    - so when we actually train the transformer on a lot of these datasets, we only work with chunks of the dataset with maximum length called `block_size` or `context_length`\n",
    "    - we basically will sample random little chunks out of the training set and train the model on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "# let's look at the first block size\n",
    "train_data[:block_size+1] # we take block_size+1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as we said in the first notebook, the above chunk has multiple examples packed into it (block_size examples)\n",
    "    - 47 likely comes after 18\n",
    "    - 56 likely comes after 18, 47\n",
    "    - 57 is likely to come after 18, 47, 56\n",
    "    - and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), The target is 47\n",
      "when input is tensor([18, 47]), The target is 56\n",
      "when input is tensor([18, 47, 56]), The target is 57\n",
      "when input is tensor([18, 47, 56, 57]), The target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), The target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), The target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), The target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), The target is 58\n"
     ]
    }
   ],
   "source": [
    "i = 0 # later will be random starting indices\n",
    "x = train_data[i:i+block_size]     # x is 0,1,2,3,4,5,6,7\n",
    "y = train_data[i+1:i+block_size+1] # y is 1,2,3,4,5,6,7,8 (basically x shifted by 1)\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # print all sequences up to (including) the current timestep\n",
    "    target = y[t] # print the label of the current timestep sequence\n",
    "    print(f\"when input is {context}, The target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the above are the 8 examples in the chunk we sampled from the training set\n",
    "- mine: the labels are the same as inputs but shifted by one token \n",
    "    - that is why we extracted block_size+1 tokens \n",
    "        - from 0 to block_size are the x's\n",
    "        - from 1 to block_size+1 are the y's (x but shifted by 1)\n",
    "- so notice that for each chunk, we train on examples of multiple contexts (with context between 1 up to block_size)\n",
    "    - we do that not just for computational reasons, but to make the transformer network used to seeing different contexts (all the way from as little as 1, up to block_size)\n",
    "    - this will be useful later in inference, because while we are sampling from the model, we can start the sampling generation with as little as 1 character of context, and it knows how to predict the next character, then use the 2 characters to predict the next, and so on up using block_size characters to predict the next one, `after block_size we will have to start truncating the context, because the transformer will never receive more than block_size characters of context when predicting the next character` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the above was the time dimension (sequence length), now the batch dimension \n",
    "### multiple chunks in a batch\n",
    "\n",
    "- as we are sampling these chunks of text, we are going to have a batch of these chunks (we will have many batches of multiple chunks of text)\n",
    "    - and that is just for efficiency reasons, since the GPUs are very good at parallel processing of data, so we make it process multiple chunks all at the same time to keep them busy (each chunk is processed independently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(3, (4,)) # sample 4 random integers from 0 to 2 (3 is exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---------------------------\n",
      "when input is tensor([24]), The target is 43\n",
      "when input is tensor([24, 43]), The target is 58\n",
      "when input is tensor([24, 43, 58]), The target is 5\n",
      "when input is tensor([24, 43, 58,  5]), The target is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]), The target is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]), The target is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]), The target is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]), The target is 39\n",
      "when input is tensor([44]), The target is 53\n",
      "when input is tensor([44, 53]), The target is 56\n",
      "when input is tensor([44, 53, 56]), The target is 1\n",
      "when input is tensor([44, 53, 56,  1]), The target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]), The target is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]), The target is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]), The target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]), The target is 1\n",
      "when input is tensor([52]), The target is 58\n",
      "when input is tensor([52, 58]), The target is 1\n",
      "when input is tensor([52, 58,  1]), The target is 58\n",
      "when input is tensor([52, 58,  1, 58]), The target is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]), The target is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]), The target is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]), The target is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]), The target is 46\n",
      "when input is tensor([25]), The target is 17\n",
      "when input is tensor([25, 17]), The target is 27\n",
      "when input is tensor([25, 17, 27]), The target is 10\n",
      "when input is tensor([25, 17, 27, 10]), The target is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]), The target is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]), The target is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]), The target is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]), The target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences we will process in parallel?\n",
    "block_size = 8 # what is the maximum context length for prediction\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,)) # the maximum starting index we can sample is len(data) - block_size - 1 (that is the end) but since the end is exclusive, we will add 1 to include it \n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    return x,y\n",
    "\n",
    "x_batch, y_batch = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x_batch.shape)\n",
    "print(x_batch)\n",
    "print('targets')\n",
    "print(y_batch.shape)\n",
    "print(y_batch)\n",
    "\n",
    "print('---------------------------')\n",
    "\n",
    "for ex in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = x_batch[ex][:t+1] # print all sequences up to (including) the current timestep\n",
    "        target = y_batch[ex][t] # print the label of the current timestep sequence\n",
    "        print(f\"when input is {context}, The target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so for each batch_size, we have batch_size * block_size examples (4*8=32) and they are completely independent as far as the transformer is concerned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Bigram language model\n",
    "\n",
    "- covered it in depth in the previous notebooks, so we will go faster \n",
    "- mine: connecting with what we have done so far\n",
    "    - the preprocessing we done above is exactly as we did in previous notebooks\n",
    "        - we processed each individual name, and that name packed multiple examples in it\n",
    "            - for example from the word \"`isabella` that\n",
    "                - it says that the character \"i\" is a very likely character to come first in the sequence of a name (mine: after the start token we will create)\n",
    "                - the character \"s\" is a very likely character to come after \"i\"\n",
    "                - the character \"a\" is a very likely character to come after \"is\"\n",
    "                - the character \"b\" is a very likely character to come after \"isa\"\n",
    "                - and so on all the way to \"a\" following \"isabell\" \n",
    "                - finally, after there is isabella, the word is very likely to end (mine: we will make it predict the end token we will create)\n",
    "        - now each individual chunk packs exactly the same information as each individual name in the above example\n",
    "    - then we took the bigram language model, which was a weak model because it looked at context of 2 characters only \n",
    "        - for example it will learn from the word \"isabella\" that\n",
    "            - \"i\" is likely to start a word\n",
    "            - \"s\" is likely to come after \"i\"\n",
    "            - \"a\" is likely to come after \"s\"\n",
    "            - \"b\" is likely to come after \"a\"\n",
    "            - and so on till \"a\" is likely to come after \"l\", and \"a\" is likely to finish the word\n",
    "        - and we said it is a weak model because when we look at a window of 2 characters, our capacity to learn the patterns in the data is limited (because the a might come after \"l\" in some word, and come after \"b\" in another word and so on, the context is not enough to understand the patterns in the data)\n",
    "    - then we formalize that in a neural network framework which allowed us to scale up the context (we used context 2 then context 3 up to context 8 with the mlps)\n",
    "        - but the mlp takes fixed context size, so if it was 3 for example, then the word isabella was expressed by the following examples\n",
    "            - ... -> i\n",
    "            - ..i -> s\n",
    "            - .is -> a\n",
    "            - isa -> b\n",
    "            - sab -> e\n",
    "            - abe -> l\n",
    "            - bel -> l\n",
    "            - ell -> a\n",
    "            - lla -> . (end token)\n",
    "        - so it is not yet able to make use of all the examples or information packed in the word \n",
    "        - we started the mlp simple then used batch norm then used progressive fusion (processing the characters not all at once but in a hierarchical manner) and the loss decreased\n",
    "    - then the RNNs were introduced, and they have the ability due to their sequential nature of processing to capture all the examples packed in the sequence theorectically\n",
    "        - but in practice, they have a lot of limitations, like it fails to capture long-range dependencies because of the vanishing gradient problem (the early tokens in the sequence have very little impact on the later tokens in the sequence, therfore the gradients that are backpropagated through the network are very small and the network doesn't learn anything)\n",
    "        - people then tried to solve that by using LSTMs and GRUs, as they have the ability thanks to the gates to carry on activations from the early tokens in the sequence to the later tokens in the sequence, therefore they are better at capturing long-range dependencies\n",
    "            - but still they have limitations, like they are slow to train because they are sequential in nature (they process the sequence one token at a time), and the performance of the model is limited by the capacity of the memory cell (the hidden state of the LSTM or GRU)\n",
    "            - so the performance decreases as the sequence length increases, for example in the encoder-decoder architectures, the GRUs and the LSTMs have to process and memorize the entire source sentence or the entire image representation in the encoder, and then generate the target sentence or the caption in the decoder\n",
    "                - so the performance of the model decreases as the length of the source sentence or the image representation increases\n",
    "            - then people started using them with the attention mechanism, which allows the decoder to focus on different parts of the source sentence or the image representation in every step of the decoder\n",
    "                - the encoder now doesn't have to summarize the entire source sentence or the image representation in a fixed-size vector, but it can give the decoder a context vector that is a weighted sum of the source sentence or the image representation\n",
    "    - then the transformers were introduced, and they are much better than RNNs, GRUs, and LSTMs for several reasons\n",
    "        - the key innovation in the transformer is the self-attention mechanism\n",
    "            - in the transformer, each element of the input sequence can attend to (or focus on) every other element of the sequence\n",
    "            - this means that the model can directly consider the relationships between distant elements in the sequence without relying on sequential steps of processing (like the RNNs, GRUs, and LSTMs)\n",
    "                - this allows the transformer to handle the entire sequence at once (we apply the self-attention mechanism to make them communicate with each other and update their representations based on the other elements in the sequence -all at once-, then we process them all at once)\n",
    "                - so it is like making the sequence elements talk to each other and capture the relationships without me -the model- having to do so\n",
    "            - this attention mechanism helps the transformer to also capture different contexts of data more effectively (as each token in the sequence can attend to different number of other tokens -with different importance- in the sequence)\n",
    "                - for example the $7^{th}$ token in the sequence will look at 6 tokens before and itself then gather information from them -context of 7 tokens-, the $3^{rd}$ token will look at 2 tokens before and itself then gather information from them, and so on\n",
    "                - so the self-attention mechanism which allows each token to look at the tokens before them already captures different contexts of data (so it can actually capture all the information packed in the sequence as we specified earlier)\n",
    "            - because transformers process the entire sequence simultaneously, tey don't inherently understand the order of the sequence, which the transformers solve by adding positional encodings to the input embeddings\n",
    "                - so the transformer can understand the order of the sequence by adding the positional encodings to the input embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_emb = nn.Embedding(5,5)\n",
    "dummy_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_indices = torch.tensor([[0,2,3,4],[1,2,2,1]])\n",
    "dummy_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6258,  0.0255,  0.9545,  0.0643, -0.5024],\n",
       "          [-0.4713,  0.0084, -0.6631, -0.2513,  1.0101],\n",
       "          [ 0.1215,  0.1584, -0.6300, -0.2221,  0.6924],\n",
       "          [-0.5075, -0.9239,  0.5467, -1.4948, -1.2057]],\n",
       " \n",
       "         [[-0.2026, -1.5671, -1.0980,  0.2360, -1.8002],\n",
       "          [-0.4713,  0.0084, -0.6631, -0.2513,  1.0101],\n",
       "          [-0.4713,  0.0084, -0.6631, -0.2513,  1.0101],\n",
       "          [-0.2026, -1.5671, -1.0980,  0.2360, -1.8002]]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " torch.Size([2, 4, 5]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_emb(dummy_indices),dummy_emb(dummy_indices).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- every single integer in (batch_size,sequence_length) will go to the corresponding row in the embedding matrix and bluck it out\n",
    "    - so instead of having integers (scalars) of shape (2,4), we got blucked out vectors of shape 5 and that is for each element in (2,4) so the shape is (2,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (the lookup table that maps each character to the logits of all possible next characters)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "    def forward(self,idx,targets):\n",
    "        \"\"\"\n",
    "        idx: the token indices, shape (batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        logits = self.token_embedding_table(idx) # the logits of shape (batch_size,sequence_length,emb_size)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we manually had a lookup table of shape vocab_size,vocab_size and we manually filled it with the counts of each pair of tokens, then we used these counts as logits to get the probabilities and sample the next character from them\n",
    "    - here, we used nn.Embedding as the table and treated its output as the logits then backprograte to update the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Bigram(vocab_size)\n",
    "out = model(x_batch,y_batch)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we got the logits of next characters for every one in the 4x8 positions\n",
    "- we will include the loss as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (the lookup table that maps each character to the logits of all possible next characters)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        \"\"\"\n",
    "        idx: the token indices, shape (batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        logits = self.token_embedding_table(idx) # the logits of shape (batch_size,sequence_length,emb_size)\n",
    "        # if the targets are not provided (during generation)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.reshape(-1,vocab_size),targets.reshape(-1)) # we need to collapse the batch_size and the sequence length dimensions together (flatten out the timesteps as individual examples), that is what the loss expects\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        \"\"\"\n",
    "        idx: token indices of some batch (batch_size,sequence_length), sequence_length can be anything (past context)\n",
    "        we will basically take the indices and expand the sequence length using generation (sampling) up to max_new_tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # inference the idx\n",
    "            logits, _ = self(idx) # batch_size,sequence_length,vocab_size\n",
    "            # focus only on the last timestep (the bigram only needs the last character to predict the next), but later we can feed all the previous characters\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1) # still (batch_size, vocab_size), but each example in the batch now is betwee 0 and 1 and sums to 1\n",
    "            # sample from the dsitribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # batch_size,1, sampled next indices for each example in the batch\n",
    "            # concatenate the sampled indices to the current indices (along the sequence length dimension)\n",
    "            idx = torch.cat((idx,idx_next),dim=1) # batch_size, sequence_length + 1 = new sequence length\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 65]), tensor(4.8786, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Bigram(vocab_size)\n",
    "logits, loss = model(x_batch,y_batch)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can guess what is the initial loss\n",
    "    - initially, all of them should have the same probability that is $\\frac{1}{vocab_size}$, so the loss should be $-log(1/vocab_size)$\n",
    "    - in our case, -log(1/65) = 4.17\n",
    "    - since our loss is higher than that, it means that the initial predictions are not super diffuse, and got a little bit of entropy\n",
    "- let's generate some text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we will use the newline character as the starting index (to kick off the generation)\n",
    "    - reasonable thing to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "start_idx = torch.zeros([1,1], dtype=torch.long)\n",
    "print(decode(model.generate(start_idx,100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it is rubbish at first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the bigram\n",
    "- in the makemore series we only used the SGD manually\n",
    "- but now we will use `AdamW` optimizer, which is a much more advanced and popular optimizer\n",
    "    - a typical good setting for the learning rate is roughly 3e-4, but for very small networks, we can get away with higher learning rates (like 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iter = 200):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iter)\n",
    "        # we will loop over random eval_iter batches and average the losses to get a more robust estimate\n",
    "        for k in range(eval_iter):\n",
    "            x,y = get_batch(split)\n",
    "            logits, loss = model(x,y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.7281, validation Loss: 4.7212\n",
      "Train Loss: 3.1239, validation Loss: 3.1254\n",
      "Train Loss: 2.6440, validation Loss: 2.6419\n",
      "Train Loss: 2.5278, validation Loss: 2.5189\n",
      "Train Loss: 2.4921, validation Loss: 2.4963\n",
      "2.459792137145996\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "eval_interval = 2000\n",
    "for i in range(10000):\n",
    "    # get the batch\n",
    "    x_batch,y_batch = get_batch('train')\n",
    "\n",
    "    # Forward prop & loss \n",
    "    logits, loss = model(x_batch,y_batch)\n",
    "\n",
    "    # backward prop\n",
    "    # reset the gradients from the previous step before the backprop (we used to do it manually)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    ## update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation phase each eval interval\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Train Loss: {losses['train']:.4f}, validation Loss: {losses['val']:.4f}\")\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Be l wloms ip, is aince oowoupis pr gedere, anenoweld bt; poup ecored be af FomeatscakevexENRI'des t nd cadeld cer:\n",
      "Weale? ighalveangry d mak\n",
      "\n",
      "\n",
      "JA linod t my Whansad\n",
      "ENELI thund?\n",
      "Whe hen ay han mandin heret, t foono kis beorcad Sicode ale\n",
      "AD:\n",
      "Alth gno ty iotorthind yootho-the; tonthexigacrodshe, CEL\n"
     ]
    }
   ],
   "source": [
    "start_idx = torch.zeros([1,1], dtype=torch.long)\n",
    "print(decode(model.generate(start_idx,300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- that is the best bigram can do\n",
    "    - whatever context we give it, it only looks at the last character to make the predictions about what comes next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mathematical Trick in self-attention\n",
    "- we want to get used to a mathematical trick that is used in the self-attention inside a transformer\n",
    "    - it is really at the heart of an efficient implementation of self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch_size, timestep, channels = features of each timestep in each example\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the above exmaple, we have 8 tokens (a sequence of 8), and they are currently not talking to each other (totally independent)\n",
    "    - we would like them to talk to each other, we would like to couple them in a very specific way\n",
    "        - the token at the $5^{th}$ location shouldn't communicate with the tokens at the $6^{th}$ , $7^{th}$, and $8^{th}$ locations, because those are future tokens in the sequence \n",
    "        - in other words, each token should look at the tokens that came before it in the sequence\n",
    "            - `information only flows from previous contexts to the current time step`, we can't get any information from the future because we are trying to predict the future (at least in the context of language modeling)\n",
    "    - so what is the easiest way for tokens to communicate? to be coupled together?\n",
    "        1. the simplist (and weakest) way is to take the average of the preceeding elements\n",
    "            - so the $5^{th}$ token will look at the average of features of the $1^{st}$, $2^{nd}$, $3^{rd}$, and $4^{th}$ tokens\n",
    "                - that average would be a feature vector that summarizes the context of the previous tokens\n",
    "                - the sum or average is the weakest form of coupling, as we have lost a lot of information about the placement and order of those tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C)) # stores the averages -of features- of the previous timesteps -tokens- for each example and timestep (it will be of dimension C, same as features)\n",
    "for ex in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[ex,:t+1] # get all the previous tokens up to -and indluding- our current one, shape (t,c)\n",
    "        xbow[ex,t] = torch.mean(x_prev,dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bag of words is a term that people use when they just averaging up things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for the first example, the first row in xbow is the average of the first row in x (which is the same)\n",
    "    - the second row in xbow is the average of the first and second rows in x (the average of the first and second token features)\n",
    "    - and so on\n",
    "\n",
    "- the mathematical trick is that we can do this averaging in a very efficient way using matrix multiplication\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "---------------\n",
      "b = \n",
      "tensor([[5., 7.],\n",
      "        [2., 0.],\n",
      "        [5., 3.]])\n",
      "----------------\n",
      "c = \n",
      "tensor([[12., 10.],\n",
      "        [12., 10.],\n",
      "        [12., 10.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b # (3,3) X (3,2) = (3,2)\n",
    "print('a =')\n",
    "print(a)\n",
    "print('---------------')\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "print('----------------')\n",
    "print('c = ')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we multiplied torch.ones with random numbers\n",
    "    - we see that each element in the first column in the matrix c is the sum of the first column in matrix b (since we multiply 3 ones with the first column in b and sum it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "---------------\n",
      "b = \n",
      "tensor([[5., 7.],\n",
      "        [2., 0.],\n",
      "        [5., 3.]])\n",
      "----------------\n",
      "c = \n",
      "tensor([[ 5.,  7.],\n",
      "        [ 7.,  7.],\n",
      "        [12., 10.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b # (3,3) X (3,2) = (3,2)\n",
    "print('a =')\n",
    "print(a)\n",
    "print('---------------')\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "print('----------------')\n",
    "print('c = ')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now when we wrapped the matrix of ones with torch.tril in order to make it lower triangular\n",
    "    - the first row has 1 one\n",
    "    - the second row has 2 ones\n",
    "    - the third row has 3 ones\n",
    "\n",
    "- now for the result matrix $c$\n",
    "    - the first row in $c$ is the first row in $a$\n",
    "    - the second row in $c$ is the sum of the first and second rows in $a$\n",
    "    - the third row in c is the sum of the first, second, and third rows in $a$\n",
    "    - and so on if the matrices are larger\n",
    "- that was possible because we used the lower triangular matrix of ones (and from the matrix multiplication operation, where we take each row in $a$ and multiply it with each row in $b$ to get the row in c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3)) / torch.tril(torch.ones(3,3)).sum(dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "---------------\n",
      "b = \n",
      "tensor([[5., 7.],\n",
      "        [2., 0.],\n",
      "        [5., 3.]])\n",
      "----------------\n",
      "c = \n",
      "tensor([[5.0000, 7.0000],\n",
      "        [3.5000, 3.5000],\n",
      "        [4.0000, 3.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a /= a.sum(dim=1,keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b # (3,3) X (3,2) = (3,2)\n",
    "print('a =')\n",
    "print(a)\n",
    "print('---------------')\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "print('----------------')\n",
    "print('c = ')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we get the average\n",
    "- so we can see how to use matrix multiplication and a weighting matrix a, to get the sum or average of b in an incremental fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei /= wei.sum(dim=1,keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 2]), torch.Size([8, 8]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, wei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow_vectorized = torch.zeros((B,T,C)) # stores the averages -of features- of the previous timesteps -tokens- for each example and timestep (it will be of dimension C, same as features)\n",
    "for ex in range(B):\n",
    "    xbow_vectorized[ex] = wei @ x[ex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow,xbow_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's vectorize it even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow_vectorized = torch.zeros((B,T,C)) # stores the averages -of features- of the previous timesteps -tokens- for each example and timestep (it will be of dimension C, same as features)\n",
    "xbow_vectorized = wei @ x # (8,8) @ (4,8,2) -> (4,8,8) @ (4,8,2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so we have a weighting matrix of shape (T,T) `between each token in the T tokens and all others` and our matrix x of shape (B,T,C)\n",
    "    - what will happen when we multiply wei @ a?\n",
    "    - (T,T) @ (B,T,C) => (B,T,T) @ (B,T,C) => (B,T,C)\n",
    "        - so the weighting matrix is broadcasted to the batch dimension, and then we multiply it with the matrix x \n",
    "        - so we have (T,T) @ (T,C) for each example in the batch\n",
    "\n",
    "- this is called `batched matrix multiplication` basically applying matrix multiplication to each example in the batch in parallel and independently\n",
    "    - so in general for any matrix (a,b,c) @ (a,c,d) => (a,b,d), that is doing the matrix multiplication (b,c) @ (c,d) for each example in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow,xbow_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so the trick is that we were able to use batch matrix multiplication to get the average of tokens in an incremental fashion\n",
    "    - basically we are doing a weighted aggregation, but the weights are designed in a way that makes it an aggregation (normalized equal weights to all past tokens for each token)\n",
    "\n",
    "- there is another identical vectorized implementation using the softmax\n",
    "    - the only difference is getting the weighting matrix\n",
    "    - we set the desired elements of tril -past elements- to be 0 and the rest to be -inf (to make the softmax ignore the future tokens), then we convert to probabilities using the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T,T))\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei.masked_fill(tril == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when we exponentiate the above matrix in the softmax, we get 1 for the 0s and 0 for the -infs\n",
    "    - then when we divide by the sum, we get equal probabilities for the 0s, therefore taking the average of the previous tokens, and 0 probabilities for the -infs, therefore ignoring the future tokens\n",
    "        - so the softmax exponentiation will convert it to ones -for past tokens- and zeros -for future tokens- then we divide by the sum to get the probabilities -exactly the same thing we did above-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow_vectorized_2 = wei @ x\n",
    "torch.allclose(xbow_vectorized_2,xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we will use the latter implementation in the self-attention, because it is more interpretable and scalable\n",
    "    - `torch.zeros((T,T))` means we basically begin the weights with 0s (we can think of it as the interaction strength), or like an affinity matrix -tell us how much each token from the past will contribute in the aggregation-\n",
    "        - so it is initially 0s and therefore giving all previous tokens same importance or relevance -ending up taking the average-, but they will be data dependent later and not constant at 0s\n",
    "        - they will start looking at each other, and some tokens will find other tokens more or less interesting to different amounts\n",
    "    - the line `wei.masked_fill(tril == 0, float('-inf'))` is basically discarding the future tokens (by setting them to -inf before the softmax normalization)\n",
    "        - we basically saying we are not aggregating any information from the future tokens\n",
    "    - wei @ x is the actual aggregation through matrix multiplication\n",
    "\n",
    "- `recap`\n",
    "    - so we can do weighted aggregation of the past tokens by using matrix multiplication of a lower triangular fashion, and the elements in the lower triangular part will tell us how much each past token will contribute to the aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up the network\n",
    "- we will not get the logits from the lookup table directly, but rather get the embeddings then add a linear layer (usually called `language model head`) that maps the embeddings to the logits\n",
    "- vocab_size is already defined for the whole notebook so no need to pass it to the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 65]), tensor(4.3632, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "n_embed = 32\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (the lookup table that maps each character to the logits of all possible next characters)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        \"\"\"\n",
    "        idx: the token indices, shape (batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        emb = self.token_embedding_table(idx) # embeddings of shape (batch_size,sequence_length,emb_size)\n",
    "        logits = self.lm_head(emb) # the logits of shape (batch_size,sequence_length,vocab_size)\n",
    "\n",
    "        # if the targets are not provided (during generation)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.reshape(-1,vocab_size),targets.reshape(-1)) # we need to collapse the batch_size and the sequence length dimensions together (flatten out the timesteps as individual examples), that is what the loss expects\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        \"\"\"\n",
    "        idx: token indices of some batch (the same one used in training) (batch_size,sequence_length)\n",
    "        we will basically take the indices and expand the sequence length using generation (sampling) up to max_new_tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # inference the idx\n",
    "            logits, _ = self(idx) # batch_size,sequence_length,vocab_size\n",
    "            # focus only on the last timestep (the bigram only needs the last character to predict the next), but later we can feed all the previous characters\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1) # still (batch_size, vocab_size), but each example in the batch now is betwee 0 and 1 and sums to 1\n",
    "            # sample from the dsitribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # batch_size,1, sampled next indices for each example in the batch\n",
    "            # concatenate the sampled indices to the current indices (along the sequence length dimension)\n",
    "            idx = torch.cat((idx,idx_next),dim=1) # batch_size, sequence_length + 1 = new sequence length\n",
    "        return idx\n",
    "    \n",
    "model = Bigram()\n",
    "logits, loss = model(x_batch,y_batch)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "- the indices we feed are encodings of the identities of the tokens (each index refers to a unique token)\n",
    "- we would also like to encode the position of the tokens in the sequence\n",
    "    - we will do that with an embedding layer that maps the position of the token (from 0 to block_size) to a vector of the same size as the token embeddings\n",
    "\n",
    "\n",
    "- we will also modify the generate method to truncate the context to block_size (we said that the transformer will never receive more than block_size tokens of context when predicting the next token)\n",
    "    - but now it is even more important because we are using the positional encoding, so if we have a position that is larger than block_size, we will get an error (out of range of the embedding table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 65]), tensor(4.4819, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "n_embed = 32\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (the lookup table that maps each character to the logits of all possible next characters)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
    "        self.positional_embedding = nn.Embedding(block_size,n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        \"\"\"\n",
    "        idx: the token indices, shape (batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        B,T = idx.shape\n",
    "        # get the token embeddings\n",
    "        emb = self.token_embedding_table(idx) # embeddings of shape (batch_size,sequence_length,emb_size)\n",
    "        # get the positional embeddings\n",
    "        pos_emb = self.positional_embedding(torch.arange(T,device=idx.device)) # (sequence_length,emb_size)\n",
    "        # add them together to get the final embeddings\n",
    "        x = emb + pos_emb # adding shapes (batch_size,sequence_length,emb_size) + (sequence_length,emb_size) -> broadcasting for the batch_size dimension\n",
    "        \n",
    "        \n",
    "        logits = self.lm_head(x) # the logits of shape (batch_size,sequence_length,vocab_size)\n",
    "\n",
    "        # if the targets are not provided (during generation)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.reshape(-1,vocab_size),targets.reshape(-1)) # we need to collapse the batch_size and the sequence length dimensions together (flatten out the timesteps as individual examples), that is what the loss expects\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        \"\"\"\n",
    "        idx: token indices of some batch (the same one used in training) (batch_size,sequence_length)\n",
    "        we will basically take the indices and expand the sequence length using generation (sampling) up to max_new_tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # truncate the sequence length to the block size\n",
    "            idx_cropped = idx[:,-block_size:]\n",
    "            # inference the idx\n",
    "            logits, _ = self(idx_cropped) # batch_size,sequence_length,vocab_size\n",
    "            # focus only on the last timestep (the bigram only needs the last character to predict the next), but later we can feed all the previous characters\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1) # still (batch_size, vocab_size), but each example in the batch now is betwee 0 and 1 and sums to 1\n",
    "            # sample from the dsitribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # batch_size,1, sampled next indices for each example in the batch\n",
    "            # concatenate the sampled indices to the current indices (along the sequence length dimension)\n",
    "            idx = torch.cat((idx,idx_next),dim=1) # batch_size, sequence_length + 1 = new sequence length\n",
    "        return idx\n",
    "    \n",
    "model = Bigram()\n",
    "logits, loss = model(x_batch,y_batch)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now x not only holds the token identity, but also the positions in which these tokens occur\n",
    "    - they are not useful in the bigram because we only look at one token to predict the next token after it\n",
    "\n",
    "- mine: the architecture now is x (token embeddings + positional encodings) -> linear layer -> logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The crux of the video: Self-attention\n",
    "\n",
    "- we will implement a small self-attention for a single individual head (as people call it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch_size, timestep, channels = features of each timestep in each example\n",
    "x = torch.randn(B,T,C) \n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "# use the tril to mask out the upper triangular part of the weight matrix\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we said that when we initialize the wei to be 0s (we initialize the affinities between the different tokens to be 0), that way we give them equal importance and then the softmax will turn them to equal probabilities and end up taking the average of all tokens\n",
    "    - but we said we don't actually want all this to be uniform, because differnt tokens will find different other tokens to be more or less interesting\n",
    "        - for example, if i am a vowel, then maybe i am looking for constants in my past and want to know what they are, so i want their information to flow to me\n",
    "- in other words, i want to gather information from the past, but i want to do it in a data-dependent way (meaning that the importance of each token in the past will be different for each token in the sequence)\n",
    "    - that is the problem that self-attention solves \n",
    "\n",
    "- the way self-attention solves this is the following\n",
    "    - every single token (at each timestep), will emit 2 vectors\n",
    "        - a query vector (q)\n",
    "            - roughly speaking, what i am looking for \n",
    "        - a key vector (k)\n",
    "            - roughly speaking, what do i contain\n",
    "    - the way we get affinities between these tokens now in the sequence, is basically doing a dot product between the keys and the queries \n",
    "        - so for a certain token, we multiply its query with all the keys of the other tokens, and that will tell us how much each token is interesting to the current token, and that dot product now becomes wei (that we initialized equally)\n",
    "        - then we discard future tokens for each token using the tril matrix\n",
    "        \n",
    "- let's implement a single head of self-attention\n",
    "    - we have another hyperparameter `head_size` which is the dimension of the query and key vectors\n",
    "    - we will have 2 weight matrices $W_q$ and $W_k$ that will map the token embeddings to the query and key vectors\n",
    "        - `nn.Linear(embedding_size,head_size,bias=False)` will do that for us\n",
    "        - (batch_size,sequence_length,embedding_size) @ (embedding_size,head_size) => (batch_size,sequence_length,head_size)\n",
    "        \n",
    "    - we will multiply the input embeddings (batch_size,sequence_length,embedding_size) with these matrices (embedding_size,head_size) to get the query and key vectors of shape (batch_size,sequence_length,head_size) using batched matrix multiplication\n",
    "        - we simply forward the embeddings to the linear layers we created to perform that multiplication\n",
    "        - so now all the tokens in all the positions of batch_size,sequence_length will produce a query and key vectors (in parallel and independently), so no communication has happened yet\n",
    "    - then we dot product each token's query with all the keys of the other tokens to get the affinities\n",
    "        - Q @ K.transpose(-1,-2), where k.transpose(-1,-2) will transpose the last 2 dimensions and keep the batch dimension as is (because we want to do batched matrix multiplication)\n",
    "            - so k will be from (batch_size,sequence_length,head_size) to (batch_size,head_size,sequence_length) \n",
    "            - then Q @ k.transpose(-1,-2) will be (batch_size,sequence_length,head_size) @ (batch_size,head_size,sequence_length) => (batch_size,sequence_length,sequence_length), which is the affinities or the wei matrix (how much each token is interesting to each other token) for each example in the batch\n",
    "    - then we cancel the future tokens for each token using the tril matrix \n",
    "    - then we apply the softmax to get the probabilities (more like the percentage of how much each token is interested in the previous tokens -the percentage of aggregation- and therefore aggregating its information to it)\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 16]), torch.Size([4, 8, 16]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_dummy = torch.randn(4,8,16) # batch_size, sequence_length, head_size\n",
    "q_dummy = torch.randn(4,8,16) # batch_size, sequence_length, head_size\n",
    "\n",
    "k_dummy.shape, q_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOSHIBA\\AppData\\Local\\Temp\\ipykernel_6612\\592131577.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3618.)\n",
      "  k_dummy.T.shape, k_dummy.transpose(-2,-1).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 8, 4]), torch.Size([4, 16, 8]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_dummy.T.shape, k_dummy.transpose(-2,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 8])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mine: another way to transpose batch of matrices\n",
    "k_dummy.mT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m q_dummy \u001b[38;5;241m@\u001b[39m k_dummy\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "q_dummy @ k_dummy.T # (4,8,16) @ (16,8,4) won't work, it is not even a thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q_dummy @ k_dummy.mT).shape # (4,8,16) @ (4,16,8) = (4,8,8), basically (8,16) @ (16,8) for each example in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notice that the batch size must match in order to apply matrix multiplication independently for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch_size, timestep, channels = features of each timestep in each example\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single head \n",
    "head_size = 16\n",
    "key = nn.Linear(C,head_size)\n",
    "query = nn.Linear(C,head_size)\n",
    "\n",
    "# get the keys and queries for all tokens in batch_size,sequence_length\n",
    "k = key(x) # (B,T,C) @ (B,C, head_size) -> (B,T,head_size)\n",
    "q = query(x) # same thing (B,T,C) -> (B,T,head_size)\n",
    "\n",
    "# get the affinities between each pair of tokens\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,head_size) @ (B,head_size,T) -> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "# use the tril to mask out the upper triangular part of the weight matrix\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x # (B,T,T) @ (B,T,C) -> (B,T,C) which is x but with the attention mechanism applied\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the wei are not zeros anymore, therefore not applying basic average aggregation\n",
    "    - they are now coming from the dot product between the queries and the keys\n",
    "- the weighted aggregation now is data dependent, meaning that the importance of each token in the past will be different for each token in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8368, 0.1632, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0585, 0.2252, 0.7164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4540, 0.0818, 0.3104, 0.1538, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0442, 0.1218, 0.1840, 0.3724, 0.2776, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0214, 0.1360, 0.1688, 0.0460, 0.5743, 0.0536, 0.0000, 0.0000],\n",
       "        [0.0592, 0.0265, 0.4947, 0.0623, 0.2254, 0.0120, 0.1199, 0.0000],\n",
       "        [0.0351, 0.0286, 0.3691, 0.1940, 0.1005, 0.0118, 0.0331, 0.2277]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as you can see, before, wei was uniform giving each past token an equal importance, but now it is different for each token\n",
    "- for wei[0] 'first example' we see the affinities of the past tokens for each token of the 8 tokens in the sequence\n",
    "    - in the last row we see the past token affinities for the 8th token, \n",
    "        - they are not uniform, meaning the 8th token is interested in more tokens than the others, according to the keys and queries\n",
    "        - so the 8th token's query may say i am a vowel and i am looking for constants at positions up to 4\n",
    "        - then all the other tokens get to emit keys, maybe one of them says i am a constant and i am in a position up to 4\n",
    "        - so their multiplication will be high -high affinity between the $8{th}$ token and that token in the wei matrix, and when we have high affinities, then the softmax end up aggregating alot of its information into the 8th token (we get to learn a lot about it)\n",
    "\n",
    "- one final modification, we don't aggregate the embeddings themselves, but rather something called the value vectors\n",
    "    - so in addition to the key and query vectors, each token will also emit a value vector (v) -what i am contributing with to the aggregation- or -what i am communicating to the other tokens-\n",
    "    - so we will have a value vector (v) for each token, the same way we had a query and key vector\n",
    "    - then we will use the wei matrix to aggregate the value vectors of the previous tokens into the current token's value vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch_size, timestep, channels = features of each timestep in each example\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single head \n",
    "head_size = 16\n",
    "key = nn.Linear(C,head_size)\n",
    "query = nn.Linear(C,head_size)\n",
    "value = nn.Linear(C,head_size)\n",
    "\n",
    "# get the keys and queries for all tokens in batch_size,sequence_length\n",
    "k = key(x) # (B,T,C) -> (B,T,head_size)\n",
    "q = query(x) # (B,T,C) -> (B,T,head_size)\n",
    "v = value(x) # (B,T,C) -> (B,T,head_size)\n",
    "\n",
    "# get the affinities between each pair of tokens\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,head_size) @ (B,head_size,T) -> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "# use the tril to mask out the upper triangular part of the weight matrix\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# aggregate the values instead of the embeddings themselves\n",
    "#out = wei @ x\n",
    "out = wei @ v # (B,T,T) @ (B,T, head_size) -> (B,T,head_size) same shape as v (the values) but weighted by the attention mechanism\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the output of the single head will be of shape (batch_size,sequence_length,head_size)\n",
    "- why do we aggregate the values instead of the embeddings themselves? why do we have value vectors in the first place?\n",
    "    - we can think of x as kind of like a private information to the token\n",
    "    - so it is like each token saying, my information is kept in vector x, and now for the purpose of the single head, here is what i am interested in (vector q), here is what i have (vector k), and if you find me interesting, here is what i will communicate to you (vector v)\n",
    "\n",
    "- that is basically the self-attention mechanism\n",
    "\n",
    "![self attention](assets/attentionmechanism.jfif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notes\n",
    "\n",
    "1. attention is a communication mechanism, we can think about it as a communication mechanism where we have a number of nodes in a directed graph\n",
    "\n",
    "![directed graph](assets/directed_graph.png)\n",
    "\n",
    "- what happens is every node has some vector of information, and it gets to aggregate information via a weighted sum from all of the nodes that point to it, depending on what data is stored in them\n",
    "- so our tokens are the nodes, and each token points at itself and the tokens after it (in other words, each token get pointed at by the tokens before it)\n",
    "\n",
    "![directed graph](assets/our_directed_graph.png)\n",
    "\n",
    "2. notice that there is no notion of space or position, so attention simply acts over a set of vectors, and so these nodes have no idea where they are positioned\n",
    "    - so we need to encode them positionally, and give them some information that is unique for each position in the sequence (so they know where they are)\n",
    "    - mine: so the matrix x (which derives q,k,v) will be the embeddings of the tokens (the identity of the tokens), added to it the positional encoding (the position of the tokens) \n",
    "\n",
    "2. notice that the calculation is independent for each example in the bacth (they are always processed independently)\n",
    "    - mine: the queries, keys, and values are calculated independently for each example in the batch, and therefore the wei matrix is calculated independently for each example in the batch\n",
    "    - so it is like having 4 separate graphs, and each graph's nodes are communicating with each other in a different way in independent and parallel manner\n",
    "3. in the case of language modeling, we don't want the tokens to communicate with future tokens, but this is not general\n",
    "    - in many cases, we may want all the tokens to communicate with each other, and we may want to have a full matrix of affinities \n",
    "        - like sentiment analysis with transformers, we might have a number of tokens, and we may want to have them all talk to each other fully, because later we will predict the sentiment of the whole sentence\n",
    "    - in those cases, we will use what is called an `encoder block` of self-attention, all it means that it is an encoder block, is that we will delete the line that discards future tokens -the masking line- `wei = wei.masked_fill(tril == 0, float('-inf'))` allowing all the nodes to communicate with each other\n",
    "        - what we are implementing here is something called a `decoder block`, because it is sort of like decoding language and it has got this auto-regressive format where we have to mask the future tokens for each token (otherwise they will give away the answer, and that is not mimicing the real world scenario)\n",
    "    - so `notice how we change the connectivity of the nodes depending on the use case, despite that attention doesn't care, attention supports arbitrary connectivity between nodes`\n",
    "5. there is also something called `cross-attention` so what is the difference between them?\n",
    "    - the reason this attention we implementing here is self-attention, is because the keys, queries, and values are all coming from the same source (x)\n",
    "        - in other words, the same source x produces the keys, queries, and values (these nodes -tokens- are self-attending)\n",
    "    - but attention can be much more general than that, for example in encoder-decoder transformers, we can have a case where the queries are produced from x, but the keys and values come from a whole separate external source (sometimes from encoder blocks that encodes some input that we would like to condition on)\n",
    "        - so the encoder produces the keys and values, those are nodes on the side (mine: the source sentence for example in machine translation), and the decoder produces queries -from the target sentence in machine translation- that read-off information from the side nodes\n",
    "        - in other words, cross-attention is used when there is a separate source of nodes we would like to pull information from into our nodes\n",
    "            - like in machine translation, we would like to pull information from the source sentence into the target sentence, so we use the encoder to produce the keys and values, and the decoder to produce the queries\n",
    "        - and it is self-attention if we have nodes that are the source of the keys, queries, and values (like in language modeling here, we want the token to communicate with itself and the tokens before it in order to predict the next token)\n",
    "\n",
    "6. if we come to the `attention is all you need paper` we will see that the equation of attention is $Attention(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "    - notice that we divide by $\\sqrt{d_k}$, where $d_k$ is the head size, why this is so important?\n",
    "        - this is called scaled attention, and it is important normalization to have \n",
    "        - if we have unit gaussian inputs (inputs with mean 0 and variance 1), then the dot product of the keys and queries -the wei matrix- will have a mean of 0 and variance of $d_k$ (the head size)\n",
    "            - but if we divide by the square root of $d_k$, then the dot product will have a mean of 0 and variance of 1, and the variance will be preserved\n",
    "            - mine: this is similar to what we did in the weight initialization in the MLPs, we divided by the square root of the input dimension to preserve the distribution\n",
    "        - why is this important? as we said, we would like before the softmax to have diffused values (meaning that the values are not too high or too low), because if we have too high values then the softmax will converge towards one-hot vectors (meaning it will sharpen the probabilities towards the maximum value), and this will basically aggregate the information from one token only\n",
    "            - so, the sclaing is used just to control the variance at initialization -making the variance small therefore the values are comparable therefore the softmax will assign comparable probabilities- therefore aggregating the information from all the tokens in the sequence at the beginning\n",
    "            - mine: then it will learn the importance of each token in the sequence and the variance will be data dependent\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(4,8,16)\n",
    "q = torch.randn(4,8,16)\n",
    "wei = q @ k.transpose(-2,-1)\n",
    "diffused_wei = q @ k.transpose(-2,-1) / (16**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0750), tensor(1.0539), tensor(18.7561), tensor(1.1723))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var(), q.var(), wei.var(), diffused_wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(8,8))\n",
    "\n",
    "# discard the future information\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "diffused_wei = diffused_wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "diffused_wei = F.softmax(diffused_wei, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0.3779,     0.5906,     0.0006,     0.0042,     0.0003,     0.0263,\n",
       "             0.0000,     0.0000]),\n",
       " tensor([0.3003, 0.3358, 0.0607, 0.0976, 0.0513, 0.1543, 0.0000, 0.0000]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "wei[0][5], diffused_wei[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notice how the diffused weights are comparable at the beginning, leading to the aggregation of information from most if not all tokens, unlike the weights before we are mainly aggregating the information from 1 or 2 tokens only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a single head of self-attention to the network\n",
    "- we will put the previous implementation in a class called `Head` and then add it to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" One head of Self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.query = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.value = nn.Linear(n_embed,head_size,bias=False)\n",
    "        # the tril is used to mask out -discard- the upper triangular part of the weight matrix -the future tokens-\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) # trill is not a parameter, so it is called a buffer in pytorch naming conventions, so we have to assign it the module using the register_buffer method of the nn.Module class\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # B,T,head_size\n",
    "        q = self.query(x) # B,T,head_size\n",
    "\n",
    "        # compute the attention scores \"Affinities\"\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,head_size) @ (B,head_size,T) => (B,T,T)\n",
    "        # discard the future tokens for each token\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        # apply softmax to get the attention weights\n",
    "        wei = F.softmax(wei,dim=-1) # (B,T,T)\n",
    "\n",
    "        # get the values\n",
    "        v = self.value(x) # B,T,head_size\n",
    "        out = wei @ v # (B,T,T) @ (B,T,head_size) => (B,T,head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we see something for the first time, we used the `register_buffer` method, what is that?\n",
    "    - it is a way to register a tensor as a buffer -a tensor that is not a parameter- and it will be saved and loaded with the model\n",
    "    - it takes the following arguments\n",
    "        - the name of the buffer\n",
    "        - the tensor itself\n",
    "        - whether it is a persistent buffer or not (if it is not persistent, it will not be saved and loaded with the model), and it is by default True (persistent)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we will plug the self-attention head to the bigram model (that is the simplist way to plug-in the self-attention component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 65]), tensor(4.2793, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "n_embed = 32\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (the lookup table that maps each character to the logits of all possible next characters)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
    "        self.positional_embedding = nn.Embedding(block_size,n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "        self.head = Head(n_embed) # we will keep the head size the same as the embedding dimension, just for now\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        \"\"\"\n",
    "        idx: the token indices, shape (batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        B,T = idx.shape\n",
    "        emb = self.token_embedding_table(idx) # embeddings of shape (batch_size,sequence_length,emb_size)\n",
    "        pos_emb = self.positional_embedding(torch.arange(T,device=idx.device)) # (sequence_length,emb_size)\n",
    "        x = emb + pos_emb # adding shapes (batch_size,sequence_length,emb_size) + (sequence_length,emb_size) -> broadcasting for the batch_size dimension\n",
    "        \n",
    "        # feed the input to the head\n",
    "        x = self.head(x) # (batch_size,sequence_length,head_size = emb_size)\n",
    "        \n",
    "        logits = self.lm_head(x) # the logits of shape (batch_size,sequence_length,vocab_size)\n",
    "\n",
    "        # if the targets are not provided (during generation)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.reshape(-1,vocab_size),targets.reshape(-1)) # we need to collapse the batch_size and the sequence length dimensions together (flatten out the timesteps as individual examples), that is what the loss expects\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        \"\"\"\n",
    "        idx: token indices of some batch (the same one used in training) (batch_size,sequence_length)\n",
    "        we will basically take the indices and expand the sequence length using generation (sampling) up to max_new_tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the sequence length to the block size\n",
    "            idx_cropped = idx[:,-block_size:]\n",
    "            # inference the idx\n",
    "            logits, _ = self(idx_cropped) # batch_size,sequence_length,vocab_size\n",
    "            # focus only on the last timestep (the bigram only needs the last character to predict the next), but later we can feed all the previous characters\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1) # still (batch_size, vocab_size), but each example in the batch now is betwee 0 and 1 and sums to 1\n",
    "            # sample from the dsitribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # batch_size,1, sampled next indices for each example in the batch\n",
    "            # concatenate the sampled indices to the current indices (along the sequence length dimension)\n",
    "            idx = torch.cat((idx,idx_next),dim=1) # batch_size, sequence_length + 1 = new sequence length\n",
    "        return idx\n",
    "    \n",
    "model = Bigram()\n",
    "logits, loss = model(x_batch,y_batch)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Bigram with self-attention\n",
    "\n",
    "- mine: x (token embeddings + positional encodings) -> self-attention head -> linear layer -> logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.2293, validation Loss: 4.2282\n",
      "Train Loss: 2.7106, validation Loss: 2.7294\n",
      "Train Loss: 2.5368, validation Loss: 2.5460\n",
      "Train Loss: 2.4894, validation Loss: 2.4981\n",
      "Train Loss: 2.4554, validation Loss: 2.4653\n",
      "Train Loss: 2.4385, validation Loss: 2.4560\n",
      "Train Loss: 2.4234, validation Loss: 2.4446\n",
      "Train Loss: 2.4029, validation Loss: 2.4295\n",
      "Train Loss: 2.4094, validation Loss: 2.4068\n",
      "Train Loss: 2.4011, validation Loss: 2.4121\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3 # we reduce the learning, because the self-attention can't tolerate large learning rates\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "\n",
    "for i in range(max_iters):\n",
    "    # get the batch\n",
    "    x_batch,y_batch = get_batch('train')\n",
    "\n",
    "    # Forward prop & loss \n",
    "    logits, loss = model(x_batch,y_batch)\n",
    "\n",
    "    # backward prop\n",
    "    # reset the gradients from the previous step before the backprop (we used to do it manually)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    ## update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation phase each eval interval\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Train Loss: {losses['train']:.4f}, validation Loss: {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we got a little bit of improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ant I nd.\n",
      "\n",
      "t hralvo me.\n",
      "\n",
      "SAs fr rm ockioded I odearde,\n",
      "Prind at flor, otho wrtothe shaisour de yot me pleve onveme\n",
      "CA: d sar mieven woupto wousaderusuerde-uwea ow veen.\n",
      "Wheit thinggomt t.\n",
      "\n",
      "AMasevee icks merof ther ipen yontee loud ese no y, st, Goround, I ter.\n",
      "\n",
      "G ies CAy hay, sor.\n",
      "MNG ISinoul,\n",
      "\n",
      "K: r\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros([1,1], dtype=torch.long)\n",
    "print(decode(model.generate(idx,300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the text is still not amazing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "\n",
    "- it is just applying multiple attention heads in parallel, and concatenating their results\n",
    "    - each head will have a shape of (batch_size,sequence_length,head_size), and we will concatenate them along the last dimension to get a tensor of shape (batch_size,sequence_length,head_size*num_heads)\n",
    "\n",
    "![multi-head attention](assets/multi_head_attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self,x):\n",
    "        # pass the x to each head, result will be a list of tensors of shape (B,T,head_size), we concatenate them on the last dimension\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now add it to the Bigram model\n",
    "    - we will have 4 heads each of head_size n_embed//4 (so that when we concatenate them we get the same size as the embeddings)\n",
    "\n",
    "- now we will use the multi-head attention in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 65]), tensor(4.1828, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "n_embed = 32\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (the lookup table that maps each character to the logits of all possible next characters)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
    "        self.positional_embedding = nn.Embedding(block_size,n_embed)\n",
    "        self.multi_head = MultiHeadAttention(4,n_embed//4)  # 4 heads each will produce shape (B,T,n_embed//4) -> concatenated will be (B,T,n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "        \n",
    "    def forward(self,idx,targets=None):\n",
    "        \"\"\"\n",
    "        idx: the token indices, shape (batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        B,T = idx.shape\n",
    "        emb = self.token_embedding_table(idx) # embeddings of shape (batch_size,sequence_length,emb_size)\n",
    "        pos_emb = self.positional_embedding(torch.arange(T,device=idx.device)) # (sequence_length,emb_size)\n",
    "        x = emb + pos_emb # adding shapes (batch_size,sequence_length,emb_size) + (sequence_length,emb_size) -> broadcasting for the batch_size dimension\n",
    "        \n",
    "        # feed the input to the head\n",
    "        x = self.multi_head(x) # (batch_size,sequence_length,emb_size)\n",
    "        \n",
    "        logits = self.lm_head(x) # the logits of shape (batch_size,sequence_length,vocab_size)\n",
    "\n",
    "        # if the targets are not provided (during generation)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.reshape(-1,vocab_size),targets.reshape(-1)) # we need to collapse the batch_size and the sequence length dimensions together (flatten out the timesteps as individual examples), that is what the loss expects\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        \"\"\"\n",
    "        idx: token indices of some batch (the same one used in training) (batch_size,sequence_length)\n",
    "        we will basically take the indices and expand the sequence length using generation (sampling) up to max_new_tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the sequence length to the block size\n",
    "            idx_cropped = idx[:,-block_size:]\n",
    "            # inference the idx\n",
    "            logits, _ = self(idx_cropped) # batch_size,sequence_length,vocab_size\n",
    "            # focus only on the last timestep (the bigram only needs the last character to predict the next), but later we can feed all the previous characters\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1) # still (batch_size, vocab_size), but each example in the batch now is betwee 0 and 1 and sums to 1\n",
    "            # sample from the dsitribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # batch_size,1, sampled next indices for each example in the batch\n",
    "            # concatenate the sampled indices to the current indices (along the sequence length dimension)\n",
    "            idx = torch.cat((idx,idx_next),dim=1) # batch_size, sequence_length + 1 = new sequence length\n",
    "        return idx\n",
    "\n",
    "  \n",
    "model = Bigram()\n",
    "logits, loss = model(x_batch,y_batch)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.2103, validation Loss: 4.2105\n",
      "Train Loss: 2.6623, validation Loss: 2.6749\n",
      "Train Loss: 2.4980, validation Loss: 2.5083\n",
      "Train Loss: 2.4299, validation Loss: 2.4370\n",
      "Train Loss: 2.3762, validation Loss: 2.3869\n",
      "Train Loss: 2.3449, validation Loss: 2.3578\n",
      "Train Loss: 2.3183, validation Loss: 2.3386\n",
      "Train Loss: 2.2944, validation Loss: 2.3214\n",
      "Train Loss: 2.2916, validation Loss: 2.2893\n",
      "Train Loss: 2.2772, validation Loss: 2.2895\n",
      "Train Loss: 2.2684, validation Loss: 2.2897\n",
      "Train Loss: 2.2393, validation Loss: 2.2870\n",
      "Train Loss: 2.2539, validation Loss: 2.2672\n",
      "Train Loss: 2.2406, validation Loss: 2.2508\n",
      "Train Loss: 2.2338, validation Loss: 2.2531\n",
      "Train Loss: 2.2085, validation Loss: 2.2611\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3 # we reduce the learning, because the self-attention can't tolerate large learning rates\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "max_iters = 8000\n",
    "eval_interval = 500\n",
    "\n",
    "for i in range(max_iters):\n",
    "    # get the batch\n",
    "    x_batch,y_batch = get_batch('train')\n",
    "\n",
    "    # Forward prop & loss \n",
    "    logits, loss = model(x_batch,y_batch)\n",
    "\n",
    "    # backward prop\n",
    "    # reset the gradients from the previous step before the backprop (we used to do it manually)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    ## update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation phase each eval interval\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Train Loss: {losses['train']:.4f}, validation Loss: {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the loss got better, which means it helped to use multiple communication channels (multiple heads) \n",
    "    - obviously, these tokens have a lot to talk about :D (a lot of different information to aggregate)\n",
    "- so, it helps to create multiple independent channels of communication, gather lots of different types of information, and decode the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LUS:\n",
      "Nomis st we thy daeqou,-hir ty mar.\n",
      "NUS:\n",
      "Anot ghis this Yome! ha hiem tordloundig pry usttulselt the per, youve badapesthel,-ld lou and my\n",
      "What a and 'is shous gras as is you kne del the cu tand to dice.\n",
      "Sor elay vitiels;\n",
      "Me, To gre the shout? fillaing, mer we, what hicpr.\n",
      "I proy ou fooor I so \n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros([1,1], dtype=torch.long)\n",
    "print(decode(model.generate(idx,300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is all you need Architecture\n",
    "\n",
    "![transformer](assets/attention_architecture.png)\n",
    "\n",
    "we are starting see some components of what we already implemented \n",
    "    - the token encoding (the embeddings)\n",
    "    - the positional encoding\n",
    "    - the `masked` multi-head attention (the self-attention with the masking of the future tokens)\n",
    "\n",
    "- notes\n",
    "    - we notice that the decoder also has a multi-head attention -not masked- which is a cross attention to the encoder -which we will not implement here-\n",
    "    - then there is that `feed-forward` block, `add & norm` (residual connection followed by layer normalization)\n",
    "    - all of that is grouped into a block `decoder block` that is repeated multiple times\n",
    "\n",
    "- we want to start in a similar fashion, by adding computation to the network\n",
    "    - what we did so far is getting the logits from the multi-head attention\n",
    "        - so the multi-head attention did the communication, but we went too fast and calculated the logits on them\n",
    "        - so, the tokens looked at each other but didn't really have a lot of time to `think on` what they found from the other tokens\n",
    "\n",
    "    - that is what the  `feed forward` block tries to solve \n",
    "        - it will take the output of the multi-head attention of shape (batch_size,sequence_length,n_embed) and apply the linear layer on the last dimension (map it from n_embed to n_embed) then apply the relu\n",
    "        - so you can see that it is per-token level (we map the n_embed for each token to another vector of n_embed), basically allowing each token to think on the data it gathered individually\n",
    "        - we will implement it (just a linear layer followed by a ReLU)\n",
    "            - it will be a nn.Linear(n_embed,n_embed) followed by a relu\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 65]), tensor(4.2134, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed,n_embed),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (the lookup table that maps each character to the logits of all possible next characters)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
    "        self.positional_embedding = nn.Embedding(block_size,n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size) # language model head\n",
    "        self.multi_head = MultiHeadAttention(4,n_embed//4)  # multi-head attention\n",
    "        self.ffwd = FeedForward(n_embed) # feed forward layer\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        \"\"\"\n",
    "        idx: the token indices, shape (batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        B,T = idx.shape\n",
    "        emb = self.token_embedding_table(idx) # embeddings of shape (batch_size,sequence_length,emb_size)\n",
    "        pos_emb = self.positional_embedding(torch.arange(T,device=idx.device)) # (sequence_length,emb_size)\n",
    "        x = emb + pos_emb # adding shapes (batch_size,sequence_length,emb_size) + (sequence_length,emb_size) -> broadcasting for the batch_size dimension\n",
    "        \n",
    "        # feed the input to the head\n",
    "        x = self.multi_head(x) # (batch_size,sequence_length,emb_size)\n",
    "\n",
    "        # feed the output of the head to the feed forward layer\n",
    "        x = self.ffwd(x)\n",
    "        \n",
    "        logits = self.lm_head(x) # the logits of shape (batch_size,sequence_length,vocab_size)\n",
    "\n",
    "        # if the targets are not provided (during generation)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.reshape(-1,vocab_size),targets.reshape(-1)) # we need to collapse the batch_size and the sequence length dimensions together (flatten out the timesteps as individual examples), that is what the loss expects\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        \"\"\"\n",
    "        idx: token indices of some batch (the same one used in training) (batch_size,sequence_length)\n",
    "        we will basically take the indices and expand the sequence length using generation (sampling) up to max_new_tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the sequence length to the block size\n",
    "            idx_cropped = idx[:,-block_size:]\n",
    "            # inference the idx\n",
    "            logits, _ = self(idx_cropped) # batch_size,sequence_length,vocab_size\n",
    "            # focus only on the last timestep (the bigram only needs the last character to predict the next), but later we can feed all the previous characters\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1) # still (batch_size, vocab_size), but each example in the batch now is betwee 0 and 1 and sums to 1\n",
    "            # sample from the dsitribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # batch_size,1, sampled next indices for each example in the batch\n",
    "            # concatenate the sampled indices to the current indices (along the sequence length dimension)\n",
    "            idx = torch.cat((idx,idx_next),dim=1) # batch_size, sequence_length + 1 = new sequence length\n",
    "        return idx\n",
    "    \n",
    "model = Bigram()\n",
    "logits, loss = model(x_batch,y_batch)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.1972, validation Loss: 4.1970\n",
      "Train Loss: 2.6546, validation Loss: 2.6640\n",
      "Train Loss: 2.5058, validation Loss: 2.5062\n",
      "Train Loss: 2.4258, validation Loss: 2.4302\n",
      "Train Loss: 2.3723, validation Loss: 2.3812\n",
      "Train Loss: 2.3387, validation Loss: 2.3397\n",
      "Train Loss: 2.3038, validation Loss: 2.3221\n",
      "Train Loss: 2.2796, validation Loss: 2.2864\n",
      "Train Loss: 2.2760, validation Loss: 2.2809\n",
      "Train Loss: 2.2577, validation Loss: 2.2702\n",
      "Train Loss: 2.2393, validation Loss: 2.2608\n",
      "Train Loss: 2.2243, validation Loss: 2.2618\n",
      "Train Loss: 2.2074, validation Loss: 2.2484\n",
      "Train Loss: 2.1927, validation Loss: 2.2479\n",
      "Train Loss: 2.1876, validation Loss: 2.2436\n",
      "Train Loss: 2.1873, validation Loss: 2.2181\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "lr = 1e-3 # we reduce the learning, because the self-attention can't tolerate large learning rates\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "max_iters = 8000\n",
    "eval_interval = 500\n",
    "\n",
    "for i in range(max_iters):\n",
    "    # get the batch\n",
    "    x_batch,y_batch = get_batch('train')\n",
    "\n",
    "    # Forward prop & loss \n",
    "    logits, loss = model(x_batch,y_batch)\n",
    "\n",
    "    # backward prop\n",
    "    # reset the gradients from the previous step before the backprop (we used to do it manually)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    ## update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation phase each eval interval\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Train Loss: {losses['train']:.4f}, validation Loss: {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so, as we can see, the transfoermer architecture intersperses the communication (multi-head attention) with the computation (feed-forward block)\n",
    "    - it has blocks that communicate then compute, and it groups them and repeats them multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer block (communication + computation)\n",
    "\n",
    "- this is basically the decoder (except for the cross attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    \"\"\" Transformer Block: Communiaction followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, num_heads):\n",
    "        super().__init__()\n",
    "        # we will make the head size so that the output of the multi-head attention has dimension n_embed\n",
    "        head_size = n_embed // num_heads\n",
    "        # the communication is done using multi-head attention\n",
    "        self.self_attn = MultiHeadAttention(num_heads, head_size) # communication, output is of shape (B,T,head_size*num_heads = n_embed)\n",
    "        # the computation is done using a feed forward layer\n",
    "        self.ffwd = FeedForward(n_embed) # computation, takes the (B,T,n_embed) and outputs (B,T,n_embed), allowing each token to think on the information it has\n",
    "\n",
    "    def forward(self,x):\n",
    "        # communication\n",
    "        x = self.self_attn(x)\n",
    "        # computation\n",
    "        x = self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is how the transformer structures the sizes typically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 65]), tensor(4.1831, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (the lookup table that maps each character to the logits of all possible next characters)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size,n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(n_embed,num_heads=4),\n",
    "            block(n_embed,num_heads=4),\n",
    "            block(n_embed,num_heads=4),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        \"\"\"\n",
    "        idx: the token indices, shape (batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        B,T = idx.shape\n",
    "        emb = self.token_embedding_table(idx) # embeddings of shape (batch_size,sequence_length,emb_size)\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(T,device=idx.device)) # (sequence_length,emb_size)\n",
    "        x = emb + pos_emb # adding shapes (batch_size,sequence_length,emb_size) + (sequence_length,emb_size) -> broadcasting for the batch_size dimension\n",
    "        \n",
    "        # feed the input to the blocks\n",
    "        x = self.blocks(x) # (batch_size,sequence_length,emb_size)\n",
    "        \n",
    "        logits = self.lm_head(x) # the logits of shape (batch_size,sequence_length,vocab_size)\n",
    "\n",
    "        # if the targets are not provided (during generation)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.reshape(-1,vocab_size),targets.reshape(-1)) # we need to collapse the batch_size and the sequence length dimensions together (flatten out the timesteps as individual examples), that is what the loss expects\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        \"\"\"\n",
    "        idx: token indices of some batch (the same one used in training) (batch_size,sequence_length)\n",
    "        we will basically take the indices and expand the sequence length using generation (sampling) up to max_new_tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the sequence length to the block size\n",
    "            idx_cropped = idx[:,-block_size:]\n",
    "            # inference the idx\n",
    "            logits, _ = self(idx_cropped) # batch_size,sequence_length,vocab_size\n",
    "            # focus only on the last timestep (the bigram only needs the last character to predict the next), but later we can feed all the previous characters\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1) # still (batch_size, vocab_size), but each example in the batch now is betwee 0 and 1 and sums to 1\n",
    "            # sample from the dsitribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # batch_size,1, sampled next indices for each example in the batch\n",
    "            # concatenate the sampled indices to the current indices (along the sequence length dimension)\n",
    "            idx = torch.cat((idx,idx_next),dim=1) # batch_size, sequence_length + 1 = new sequence length\n",
    "        return idx\n",
    "    \n",
    "model = Bigram()\n",
    "logits, loss = model(x_batch,y_batch)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the above model\n",
    "    - we encode the tokens and the positions\n",
    "    - we apply multiple transformer blocks (each block has a multi-head attention and a feed-forward block)\n",
    "        - communication then computation many times\n",
    "    - then we apply a linear layer to get the logits (decode)\n",
    "\n",
    "- the above architecture does not give us good results\n",
    "    - that is because it gets deeper and deeper, and deep neural nets suffer from optimization issues\n",
    "    - so we need the residual connections and the layer normalization to help with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.1717, validation Loss: 4.1746\n",
      "Train Loss: 3.1150, validation Loss: 3.1260\n",
      "Train Loss: 2.7538, validation Loss: 2.7414\n",
      "Train Loss: 2.5552, validation Loss: 2.5523\n",
      "Train Loss: 2.4857, validation Loss: 2.4854\n",
      "Train Loss: 2.4515, validation Loss: 2.4482\n",
      "Train Loss: 2.3938, validation Loss: 2.4082\n",
      "Train Loss: 2.3658, validation Loss: 2.3590\n",
      "Train Loss: 2.3557, validation Loss: 2.3475\n",
      "Train Loss: 2.3246, validation Loss: 2.3210\n",
      "Train Loss: 2.3016, validation Loss: 2.3108\n",
      "Train Loss: 2.2821, validation Loss: 2.3042\n",
      "Train Loss: 2.2665, validation Loss: 2.2889\n",
      "Train Loss: 2.2412, validation Loss: 2.2793\n",
      "Train Loss: 2.2311, validation Loss: 2.2641\n",
      "Train Loss: 2.2368, validation Loss: 2.2441\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "lr = 1e-3 # we reduce the learning, because the self-attention can't tolerate large learning rates\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "max_iters = 8000\n",
    "eval_interval = 500\n",
    "\n",
    "for i in range(max_iters):\n",
    "    # get the batch\n",
    "    x_batch,y_batch = get_batch('train')\n",
    "\n",
    "    # Forward prop & loss \n",
    "    logits, loss = model(x_batch,y_batch)\n",
    "\n",
    "    # backward prop\n",
    "    # reset the gradients from the previous step before the backprop (we used to do it manually)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    ## update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation phase each eval interval\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Train Loss: {losses['train']:.4f}, validation Loss: {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we got worse results as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual connections and layer normalization (Add & Norm)\n",
    "\n",
    "- these 2 dramatically help with the depth of these networks, and make sure they remain optimizable\n",
    "\n",
    "- mine: why do we need residual connections and normalization? specially for deep neural networks?\n",
    "    - during training, as the parameters are updated, the distribution of the activations in the network changes, and the activations can become very large or very small, and this can lead to the vanishing or exploding gradient problem\n",
    "        - from another perspective, as the distribution of the activations changes as we go deeper in the network, it becomes harder for the network to learn effectively, as each layer needs to continuously adapt to new distributions\n",
    "    - we studied then the importance of good initialization of the network parameters, a well-calibrated initialization (like dividing by the square root of the input dimension for linear layers, and multiplying by some gain before the activation function squashes the activations) all these things help to preserve the distribution of the activations therefore preserving the distribution of the gradients as well (avoiding the vanishing or exploding gradient problem)\n",
    "\n",
    "    -  good initialization was extremely essential to do and the neural networks (specifically the deep neural networks) were not very forgiving if we didn't make perfect and exact calibration of the weights\n",
    "        - but as the neural networks got deeper and more complex, it became very difficult to perfectly calibrate the weights to make the logits have the same distribution (it was like balancing a pencil on its tip :D)\n",
    "\n",
    "    \n",
    "    - so people have came up with modern innovations that mitigated this, like `residual connections` and also what is called `normalization layers` \n",
    "        - think about the residual connections having a residual pathway, and we forked off from the residual pathway to do some computation (mine: additional block) and then project back to the residual pathway via addition, this ensures that the activations in the residual pathway are not changed much (unlike earlier when we had no residual pathway and the only path we have is to pass it through multiple subsequent layers, this increases the chances of changing the distribution alot)\n",
    "            - so when we do x = block result + x, this mitigates the chances of changing the distribution (unlike x = block result only)\n",
    "            - from another perspective, when we offer the residual pathway for the activations to flow through, this will offer the gradients a direct path to flow as well, and this will help with the vanishing gradient problem\n",
    "                - preserving the activations and the gradients distribution & vanishing and exploding gradients are 2 faces of the same coin\n",
    "        - the normalization layers are layers that normalize the activations to have a mean of 0 and variance of 1, so the activations are always in a good range, and the network can learn to adjust the activations as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual connections\n",
    "- we talked about the residual connections in the deep learning specialization\n",
    "    - we discussed how they can easily learn the identity function, and each additional block can add to the identity function\n",
    "    - so, previously, each additional block will change the output of the previous block whether to the best or worst, but now it can only add to the output of the previous block (even if it didn't learn anything useful, the previous output will not be lost)\n",
    "\n",
    "![residual connections](assets/residual_connection.png)\n",
    "\n",
    "- another way to think about it, we have a residual pathway, and we forked off from the residual pathway to do some computation (mine: additional block) and then project back to the residual pathway via addition\n",
    "    - and we studied that during backpropagation, addition routes the gradient equally to both branches, so the gradients from the loss basically hop through every addition node all the way to the input, and also fork off to each block and update them\n",
    "    - so, basically we have that gradient super highway tha goes directly from the supervision all the way to the input (and also to each block)\n",
    "    - and the blocks are usually initialized in the beginning so they contribute very little (if anything) to the residual pathway\n",
    "        - so in the beginning, they are sort of almost like not there , and during the optimization, they will learn to contribute with something useful\n",
    "\n",
    "- mine: so, we will fork-off the residual pathway to do the mulit-head attention then add it again to the residual pathway, then fork-off again to do the feed-forward block then add it again to the residual pathway\n",
    "    - in order to be able to add back the blocks results to the residual pathway, they must have the same shape, and if they are not the same shape, we can use a linear layer to project them to the same shape as the residual pathway\n",
    "    - so, we will modify the ff_block and the multi-head attention to project their outputs to the residual pathway\n",
    "        - note that their shapes are already ready for that, but we will make it so that it is general in case we want to change their shapes to output something other than n_embed\n",
    "        - that is indeed what we will do, we will change the number of output features of the feed-forward block to be n_embed*4 (instead of n_embed) to be consistent with the paper, then we will project it back to n_embed using a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed,n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # pass the x to each head, result will be a list of tensors of shape (B,T,head_size), we concatenate them on the last dimension\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.proj(out)\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 *n_embed),\n",
    "            nn.ReLU(),\n",
    "            # let's put the projection here\n",
    "            nn.Linear(4 * n_embed,n_embed),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now let's modify the block to include the residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    \"\"\" Transformer Block: Communiaction followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, num_heads):\n",
    "        super().__init__()\n",
    "        # we will make the head size so that the output of the multi-head attention has dimension n_embed\n",
    "        head_size = n_embed // num_heads\n",
    "        # the communication is done using multi-head attention\n",
    "        self.self_attn = MultiHeadAttention(num_heads, head_size)\n",
    "        # the computation is done using a feed forward layer\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # communication\n",
    "        x = x + self.self_attn(x) # x += self.self_attn(x) -additional block-\n",
    "        # computation\n",
    "        x = x + self.ffwd(x) # x += self.ffwd(x) - additional block-\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notice how we fork-off to do the self-attention blocks and the feed-forward blocks then add the results again to the residual pathway\n",
    "    - we fork-off to do some block, then come back to the residual pathway (mine: we project the output of the blocks if they are not of the same shape as the residual pathway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 65]), tensor(4.6007, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (the lookup table that maps each character to the logits of all possible next characters)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size,n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(n_embed,num_heads=4),\n",
    "            block(n_embed,num_heads=4),\n",
    "            block(n_embed,num_heads=4),\n",
    "        )\n",
    "        self.ml_head = nn.Linear(n_embed,vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        \"\"\"\n",
    "        idx: the token indices, shape (batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        B,T = idx.shape\n",
    "        emb = self.token_embedding_table(idx) # embeddings of shape (batch_size,sequence_length,emb_size)\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(T,device=idx.device)) # (sequence_length,emb_size)\n",
    "        x = emb + pos_emb # adding shapes (batch_size,sequence_length,emb_size) + (sequence_length,emb_size) -> broadcasting for the batch_size dimension\n",
    "        \n",
    "        # feed the input to the blocks\n",
    "        x = self.blocks(x) # (batch_size,sequence_length,emb_size)\n",
    "        \n",
    "        logits = self.ml_head(x) # the logits of shape (batch_size,sequence_length,vocab_size)\n",
    "\n",
    "        # if the targets are not provided (during generation)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.reshape(-1,vocab_size),targets.reshape(-1)) # we need to collapse the batch_size and the sequence length dimensions together (flatten out the timesteps as individual examples), that is what the loss expects\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        \"\"\"\n",
    "        idx: token indices of some batch (the same one used in training) (batch_size,sequence_length)\n",
    "        we will basically take the indices and expand the sequence length using generation (sampling) up to max_new_tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the sequence length to the block size\n",
    "            idx_cropped = idx[:,-block_size:]\n",
    "            # inference the idx\n",
    "            logits, _ = self(idx_cropped) # batch_size,sequence_length,vocab_size\n",
    "            # focus only on the last timestep (the bigram only needs the last character to predict the next), but later we can feed all the previous characters\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1) # still (batch_size, vocab_size), but each example in the batch now is betwee 0 and 1 and sums to 1\n",
    "            # sample from the dsitribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # batch_size,1, sampled next indices for each example in the batch\n",
    "            # concatenate the sampled indices to the current indices (along the sequence length dimension)\n",
    "            idx = torch.cat((idx,idx_next),dim=1) # batch_size, sequence_length + 1 = new sequence length\n",
    "        return idx\n",
    "    \n",
    "model = Bigram()\n",
    "logits, loss = model(x_batch,y_batch)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3 # we reduce the learning, because the self-attention can't tolerate large learning rates\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "max_iters = 8000\n",
    "eval_interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3802, validation Loss: 4.3655\n",
      "Train Loss: 2.3987, validation Loss: 2.4098\n",
      "Train Loss: 2.2801, validation Loss: 2.3041\n",
      "Train Loss: 2.1948, validation Loss: 2.2187\n",
      "Train Loss: 2.1440, validation Loss: 2.1861\n",
      "Train Loss: 2.1058, validation Loss: 2.1539\n",
      "Train Loss: 2.0781, validation Loss: 2.1402\n",
      "Train Loss: 2.0579, validation Loss: 2.1106\n",
      "Train Loss: 2.0382, validation Loss: 2.0987\n",
      "Train Loss: 2.0196, validation Loss: 2.0932\n",
      "Train Loss: 2.0038, validation Loss: 2.0832\n",
      "Train Loss: 1.9874, validation Loss: 2.0901\n",
      "Train Loss: 1.9722, validation Loss: 2.0728\n",
      "Train Loss: 1.9486, validation Loss: 2.0539\n",
      "Train Loss: 1.9317, validation Loss: 2.0627\n",
      "Train Loss: 1.9346, validation Loss: 2.0377\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "for i in range(max_iters):\n",
    "    # get the batch\n",
    "    x_batch,y_batch = get_batch('train')\n",
    "\n",
    "    # Forward prop & loss \n",
    "    logits, loss = model(x_batch,y_batch)\n",
    "\n",
    "    # backward prop\n",
    "    # reset the gradients from the previous step before the backprop (we used to do it manually)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    ## update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation phase each eval interval\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Train Loss: {losses['train']:.4f}, validation Loss: {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we got down to a lower loss thanks to the residual connections (allowed us to successfuly deepen the network with multiple blocks), and notice that the network is starting to get big enough that our training loss is getting ahead of validation loss (starting to see a little bit of overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PENCE:\n",
      "It not bame;\n",
      "And God your ver:\n",
      "All I your passon but king frumakner, aull reamence wemblew-on, I grabence cadervers light:\n",
      "Go him this mishourgent sGARENCENTER:\n",
      "I wook, my swen there, did. Tants, blow?\n",
      "\n",
      "SWARWY:\n",
      "You wold stare honoubleeperemed:\n",
      "I man Claremaknow this efent:\n",
      "There my reverbid:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros([1,1], dtype=torch.long)\n",
    "print(decode(model.generate(idx,300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the generation is still bad, but it starts to almost look like english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer normalization\n",
    "\n",
    "- Layer Norm is a paper that came out in 2016, [layer normalization paper](https://arxiv.org/abs/1607.06450)\n",
    "\n",
    "\n",
    "- mine: theoretical recap (Read notebook 4)\n",
    "- we studied batch norm in previous notebooks\n",
    "    - in batch norm, we normalize the across the examples for each feature, assume we have $x_{batch}$ of shape (batch_size,features)\n",
    "        - we calculte the mean and std across the batch dimension for each feature, shapes (1,features) \n",
    "        - then we standardize the features by subtracting the mean and dividing by the std\n",
    "            - x - mean / std, (batch_size,features) - (1,features) / (1,features) => (batch_size,features), the mean and std are broadcasted to the batch dimension\n",
    "            - now each feature has a mean of 0 and std of 1\n",
    "        - then we move the features to a different mean and scale them up or down to a different std using $\\gamma$ and $\\beta$\n",
    "            - $\\gamma$ and $\\beta$ are learnable parameters, and they are of shape (1,features)\n",
    "            - $\\gamma x + \\beta$, (1,features) * (batch_size,features) + (1,features) => (batch_size,features)\n",
    "            - so we can learn a different mean and std for each feature\n",
    "        - the initial values of $\\gamma$ and $\\beta$ are 1 and 0 respectively, so that the distribution is not changed (still gaussian after the standardization), at least in the beginning\n",
    "\n",
    "    - we said there are some disadvantages to batch norm\n",
    "        - because we normalize through the batch, we are mathematically coupling these examples together (the mean and std are calculated across the batch)\n",
    "            - now the logits are no longer independent for each example, as they will change slightly depending on the other examples in the randomly sampled batch (because the mean and std are calculated across the batch) and the logits will sort of jitter around depending on the other examples in the batch\n",
    "            - and during inference, we don't have a batch, so we have to calculate the mean and std across the whole training set, then use them during inference -or use the running mean and std-\n",
    "        - so, for that reason, no one likes this layer, it causes a huge amount of bugs, and therefore people tried to avoid it and proposed other alternatives like `layer normalization` and `group normalization`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 8])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps = 1e-5, momentum = 0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True # because we will need to know if we are in training or evaluation mode\n",
    "        # initialize the learnable parameters\n",
    "        self.gamma = torch.ones(1,dim)\n",
    "        self.beta = torch.zeros(1,dim)\n",
    "        # initialize the running mean and var (called Buffers in PyTorch), we used the variance to follow the paper\n",
    "        self.running_mean = torch.zeros(1,dim)\n",
    "        self.running_var = torch.ones(1,dim)\n",
    "\n",
    "    def __call__(self, x):  # shape of x is (m, dim)\n",
    "        if self.training:\n",
    "            # use the mean and std of the batch\n",
    "            mean = x.mean(dim=0, keepdim=True) # mean of the logits over the batch for each neuron, shape (1, dim)\n",
    "            var = x.var(dim=0, keepdim=True) # std of the logits over the batch for each neuron, shape (1, dim)\n",
    "        else:\n",
    "            # use the running mean and std\n",
    "            mean = self.running_mean \n",
    "            var = self.running_var\n",
    "        \n",
    "        # standardize\n",
    "        x_standardized = (x - mean) / torch.sqrt(var + self.eps) # shape (m, dim)\n",
    "        # rescale\n",
    "        self.out = x_standardized * self.gamma + self.beta # shape (m, dim)\n",
    "\n",
    "        # update the running mean and std if we are in training mode\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
    "\n",
    "        return self.out # we don't have to save the ouput in .out attribute, but we do so for later visualization\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "dummy = torch.randn(100,8)\n",
    "bn = BatchNorm1d(8)\n",
    "out = bn(dummy)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([     0.0000,      0.0000,      0.0000,     -0.0000,     -0.0000,\n",
       "             -0.0000,      0.0000,     -0.0000]),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.mean(dim=0), out.var(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the mean and variance for each column (neuron) are 0 and 1 respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mine\n",
    "- `layer normalization`\n",
    "    - While batch normalization normalizes each feature across the batch (i.e., it computes the mean and standard deviation for each feature across the batch dimension), layer normalization normalizes the features of each individual sample\n",
    "    - after standardizing the features, $\\gamma$ and $\\beta$ scale and shift the normalized values of the features within each sample. they allow the model to learn the optimal scale and offset for each feaure within the context of the sample itself\n",
    "        - since after normalization, the mean and std of all the features for each sample are 0 and 1 respectively, so $\\gamma$ and $\\beta$ can learn the optimal scale and offset for the features within the example\n",
    "\n",
    "- motivation behind layer normalization\n",
    "    - now the logits are independent for each example (the mean and std calculated for each individual example), so the logits are not coupled together\n",
    "        - therefore, we can easily apply layer normalization during inference (we don't need a batch to calculate the mean and std across it), so there is no need of a second stage to calculate the mean and std or even use the running mean and std \n",
    "    - Commonly used in RNNs, Transformers, and other models where batch normalization would be difficult to apply effectively due to varying sequence lengths or small batch sizes.\n",
    "\n",
    "- Similarities between batch norm and layer norm: Both Batch Norm and Layer Norm help in preserving the distribution of activations and stabilizing gradients, making the network less sensitive to weight initialization and allowing for higher learning rates.\n",
    "\n",
    "\n",
    "| Aspect                      | Batch Normalization (Batch Norm)                        | Layer Normalization (Layer Norm)                        |\n",
    "|-----------------------------|---------------------------------------------------------|---------------------------------------------------------|\n",
    "| Normalization Dimension     | Across the batch for each feature                       | Across the features for each individual sample          |\n",
    "| Dependency on Batch Size    | Sensitive to batch size; performance may degrade with small batches | Independent of batch size; works well even with batch size of one |\n",
    "| Ideal Use Cases             | CNNs, deep feedforward networks, architectures where batch size is large | RNNs, Transformers, models dealing with sequential or variable-sized data |\n",
    "| Computation During Inference| Uses running mean and variance computed during training | Uses per-sample statistics; no need for running estimates |\n",
    "\n",
    "\n",
    "- implementation differences\n",
    "    - we will delete all the variables related to running statistics\n",
    "        - the momentum\n",
    "        - running mean\n",
    "        - running variance\n",
    "    - we will delete the training flag (we needed it to know wether to use the batch statistics during training or the running statistics during inference)\n",
    "        - the behavior is now the same during training and inference\n",
    "    - we will change the mean and std from dim = 0 (accross the batch or each feature) to dim = -1 (across the features for each individual sample) \n",
    "\n",
    "- shape analysis (on sequential data) \n",
    "    - we have x of shape (batch_size,sequence_length,features)\n",
    "    - we will calculate the mean and std across the features for each individual sample, so we will use dim = -1, the mean and std will be of shape (batch_size,sequence_length,1)\n",
    "    - then we will standardize the features by subtracting the mean and dividing by the std, so the mean and std will be broadcasted for all the features in each sample\n",
    "    - then we will scale and shift the normalized values of the features within each sample using $\\gamma$ and $\\beta$, so $\\gamma$ and $\\beta$ will be of shape (1,1,features) and they will be broadcasted across the batch dimension and the sequence length dimension\n",
    "    \n",
    "TODO: do more research about layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "torch.Size([100, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 8])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d:\n",
    "    def __init__(self, dim, eps = 1e-5, momentum = 0.1):\n",
    "        self.eps = eps\n",
    "        # initialize the learnable parameters\n",
    "        self.gamma = torch.ones(1,dim)\n",
    "        self.beta = torch.zeros(1,dim)\n",
    "        \n",
    "\n",
    "    def __call__(self, x):  # shape of x is (m, dim)\n",
    "        # use the mean and std of the batch\n",
    "        mean = x.mean(dim=1, keepdim=True) # mean of the logits over the batch for each neuron, shape (m, 1)\n",
    "        var = x.var(dim=1, keepdim=True) # std of the logits over the batch for each neuron, shape (m, 1)\n",
    "        print(mean.shape)\n",
    "        print(var.shape)\n",
    "    \n",
    "        # standardize\n",
    "        x_standardized = (x - mean) / torch.sqrt(var + self.eps) # shape (m, dim)\n",
    "        # rescale\n",
    "        self.out = x_standardized * self.gamma + self.beta # shape (m, dim)\n",
    "\n",
    "        return self.out # we don't have to save the ouput in .out attribute, but we do so for later visualization\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "dummy = torch.randn(100,8)\n",
    "bn = LayerNorm1d(8)\n",
    "out = bn(dummy)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0425, -0.1134, -0.0339,  0.0080,  0.2189, -0.0068, -0.0921, -0.0232]),\n",
       " tensor([1.0071, 0.7030, 0.8447, 1.0312, 0.7555, 1.0201, 0.9427, 0.6926]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice that the columns are not normalized\n",
    "out.mean(dim=0), out.var(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
       "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
       "              0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0000,\n",
       "             -0.0000,      0.0000,      0.0000,     -0.0000,     -0.0000,\n",
       "              0.0000,     -0.0000,      0.0000,     -0.0000,     -0.0000,\n",
       "              0.0000,     -0.0000,      0.0000,      0.0000,     -0.0000,\n",
       "             -0.0000,      0.0000,     -0.0000,      0.0000,      0.0000,\n",
       "              0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,\n",
       "             -0.0000,      0.0000,      0.0000,     -0.0000,      0.0000,\n",
       "              0.0000,     -0.0000,     -0.0000,      0.0000,      0.0000,\n",
       "             -0.0000,     -0.0000,     -0.0000,      0.0000,      0.0000,\n",
       "             -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
       "              0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,\n",
       "              0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0000,\n",
       "              0.0000,     -0.0000,      0.0000,      0.0000,      0.0000,\n",
       "              0.0000,     -0.0000,      0.0000,      0.0000,     -0.0000,\n",
       "              0.0000,     -0.0000,     -0.0000,      0.0000,      0.0000,\n",
       "              0.0000,     -0.0000,     -0.0000,      0.0000,     -0.0000,\n",
       "             -0.0000,     -0.0000,     -0.0000,      0.0000,     -0.0000,\n",
       "              0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0000]),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but the rows are normalized (for every individual example, the features have mean 0 and std 1 at the beginning)\n",
    "out.mean(dim=1), out.var(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in PyTorch, we have `nn.LayerNorm` that does that for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporate Add & Norm to the model\n",
    "\n",
    "![attention architecture](assets/attention_architecture.png)\n",
    "\n",
    "- we said that very few details about the transformer architecture have changed in the last 5 years\n",
    "    - in the image above, add & norm is applied after the transformation (after the block)\n",
    "    - but it became more common to apply the layer norm before the block (before the multi-head attention and the feed-forward block)\n",
    "        - mine: so we do layer norm -> block -> add to the residual pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    \"\"\" Transformer Block: Communiaction followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, num_heads):\n",
    "        super().__init__()\n",
    "        # we will make the head size so that the output of the multi-head attention has dimension n_embed\n",
    "        head_size = n_embed // num_heads\n",
    "        # the communication is done using multi-head attention\n",
    "        self.self_attn = MultiHeadAttention(num_heads, head_size)\n",
    "        # the computation is done using a feed forward layer\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        # layer normalization (mine: we will use 2 different layer norms because we want to normalize the features of each token in the multi-head attention and the feed forward layer separately)\n",
    "        self.ln1 = nn.LayerNorm(n_embed) # layer normalization before the multi-head attention \n",
    "        self.ln2 = nn.LayerNorm(n_embed) # layer normalization before the feed forward layer\n",
    "\n",
    "    def forward(self,x):\n",
    "        # communication (norm -> multi-head attention -> residual connection)\n",
    "        x = x + self.self_attn(self.ln1(x))\n",
    "        # computation (norm -> feed forward -> residual connection)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we added layer normalization layers right before the multi-head attention and the feed-forward block\n",
    "- since our inputs are of shape (batch_size,sequence_length,n_embed), we will apply the layer normalization on the last dimension (n_embed)\n",
    "    - so the layer normalization will normalize the features, and the batch dimension as well as the sequence length will be untouched\n",
    "        - so think of it as a per-token transformation, that just normalizes the features and make them unit gaussian at initialization (then the network will learn the mean and variance)\n",
    "\n",
    "- there should be a layer norm at the end of the transformer (right before the final linear layer that decodes into vocab_size logits)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 65]), tensor(4.3122, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (the lookup table that maps each character to the logits of all possible next characters)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size,n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(n_embed,num_heads=4),\n",
    "            block(n_embed,num_heads=4),\n",
    "            block(n_embed,num_heads=4),\n",
    "            # layer normalization before the final linear layer\n",
    "            nn.LayerNorm(n_embed),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        \"\"\"\n",
    "        idx: the token indices, shape (batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        B,T = idx.shape\n",
    "        emb = self.token_embedding_table(idx) # embeddings of shape (batch_size,sequence_length,emb_size)\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(T,device=idx.device)) # (sequence_length,emb_size)\n",
    "        x = emb + pos_emb # adding shapes (batch_size,sequence_length,emb_size) + (sequence_length,emb_size) -> broadcasting for the batch_size dimension\n",
    "        \n",
    "        # feed the input to the blocks then layer norm right before the final linear layer\n",
    "        x = self.blocks(x) # (batch_size,sequence_length,emb_size)\n",
    "        \n",
    "        logits = self.lm_head(x) # the logits of shape (batch_size,sequence_length,vocab_size)\n",
    "\n",
    "        # if the targets are not provided (during generation)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.reshape(-1,vocab_size),targets.reshape(-1)) # we need to collapse the batch_size and the sequence length dimensions together (flatten out the timesteps as individual examples), that is what the loss expects\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        \"\"\"\n",
    "        idx: token indices of some batch (the same one used in training) (batch_size,sequence_length)\n",
    "        we will basically take the indices and expand the sequence length using generation (sampling) up to max_new_tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the sequence length to the block size\n",
    "            idx_cropped = idx[:,-block_size:]\n",
    "            # inference the idx\n",
    "            logits, _ = self(idx_cropped) # batch_size,sequence_length,vocab_size\n",
    "            # focus only on the last timestep (the bigram only needs the last character to predict the next), but later we can feed all the previous characters\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1) # still (batch_size, vocab_size), but each example in the batch now is betwee 0 and 1 and sums to 1\n",
    "            # sample from the dsitribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # batch_size,1, sampled next indices for each example in the batch\n",
    "            # concatenate the sampled indices to the current indices (along the sequence length dimension)\n",
    "            idx = torch.cat((idx,idx_next),dim=1) # batch_size, sequence_length + 1 = new sequence length\n",
    "        return idx\n",
    "    \n",
    "model = Bigram()\n",
    "logits, loss = model(x_batch,y_batch)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3 # we reduce the learning, because the self-attention can't tolerate large learning rates\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "max_iters = 8000\n",
    "eval_interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.2364, validation Loss: 4.2348\n",
      "Train Loss: 2.4111, validation Loss: 2.4204\n",
      "Train Loss: 2.2663, validation Loss: 2.2850\n",
      "Train Loss: 2.1814, validation Loss: 2.2050\n",
      "Train Loss: 2.1318, validation Loss: 2.1701\n",
      "Train Loss: 2.0874, validation Loss: 2.1365\n",
      "Train Loss: 2.0646, validation Loss: 2.1204\n",
      "Train Loss: 2.0420, validation Loss: 2.0910\n",
      "Train Loss: 2.0253, validation Loss: 2.0833\n",
      "Train Loss: 2.0115, validation Loss: 2.0742\n",
      "Train Loss: 1.9975, validation Loss: 2.0605\n",
      "Train Loss: 1.9779, validation Loss: 2.0706\n",
      "Train Loss: 1.9659, validation Loss: 2.0586\n",
      "Train Loss: 1.9459, validation Loss: 2.0581\n",
      "Train Loss: 1.9275, validation Loss: 2.0566\n",
      "Train Loss: 1.9408, validation Loss: 2.0384\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "for i in range(max_iters):\n",
    "    # get the batch\n",
    "    x_batch,y_batch = get_batch('train')\n",
    "\n",
    "    # Forward prop & loss \n",
    "    logits, loss = model(x_batch,y_batch)\n",
    "\n",
    "    # backward prop\n",
    "    # reset the gradients from the previous step before the backprop (we used to do it manually)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    ## update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation phase each eval interval\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Train Loss: {losses['train']:.4f}, validation Loss: {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so now we have pretty complete transformer that is similar to the decoder block in the transformer architecture\n",
    "    - it is a decoder only transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HENRY: I dess pake,\n",
      "The faslese astot him, good his, and as lienght your condlanle 'tis suve\n",
      "A king the steapiraft! Tomere;\n",
      "\n",
      "DUKE VINNEN:\n",
      "How lord Entsel? 'featcend?\n",
      "\n",
      "WICK:\n",
      "Oglach not Nor is An, verarm.\n",
      "He; whim the\n",
      "For dest.\n",
      "Whats,\n",
      "Abry GRIO:\n",
      "Abpyon his I from a leasure a preavity.\n",
      "\n",
      "DUCUCIPRUMPRIET\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros([1,1], dtype=torch.long)\n",
    "print(decode(model.generate(idx,300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "\n",
    "- what does dropout do?\n",
    "    - it randomly every forward & backward, it shuts off some subset of neurons -randomly drops them to zero- therefore training without them\n",
    "    - what this does exactly is that because the mask of what being dropped out is changed every forward and backward pass (meaning every single forward and backward pass, different neurons are dropped out), it ends up kind of training different neural networks (kind of training an ensemble of sub-networks)\n",
    "        - then at test time, everything is fully enabled, and kind of all these sub-networks are merged into a single ensemble\n",
    "\n",
    "    - it has a regularization effect and we will add it because we will scale-up the model and we don't want to worry about overfitting\n",
    "    \n",
    "- drop out is something that we can add right before the block output is back into the residual pathway (mine: after the projection of the block output to the residual pathway)\n",
    "    - so we will do that for the multi-head attention and the feed-forward blocks\n",
    "\n",
    "- we can also dropout some of the affinities (in the wei matrix), right before using it to aggregate the values\n",
    "    - so this will randomly prevent some of the nodes from communicating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed,n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # pass the x to each head, result will be a list of tensors of shape (B,T,head_size), we concatenate them on the last dimension\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 *n_embed),\n",
    "            nn.ReLU(),\n",
    "            # let's put the projection here\n",
    "            nn.Linear(4 * n_embed,n_embed),\n",
    "            # dropout\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of Self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.query = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.value = nn.Linear(n_embed,head_size,bias=False)\n",
    "        # the tril is used to mask out -discard- the upper triangular part of the weight matrix -the future tokens-\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) # trill is not a parameter, so it is called a buffer in pytorch naming conventions, so we have to assign it the module using the register_buffer method of the nn.Module class\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # B,T,head_size\n",
    "        q = self.query(x) # B,T,head_size\n",
    "\n",
    "        # compute the attention scores \"Affinities\"\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,head_size) @ (B,head_size,T) => (B,T,T)\n",
    "        # discard the future tokens for each token\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        # apply softmax to get the attention weights\n",
    "        wei = F.softmax(wei,dim=-1) # (B,T,T)\n",
    "\n",
    "        # dropout some of the affinities randomly\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # get the values\n",
    "        v = self.value(x) # B,T,head_size\n",
    "        out = wei @ v # (B,T,T) @ (B,T,head_size) => (B,T,head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale-up the model\n",
    "\n",
    "- we will organize the implementation a little bit \n",
    "    - first of all naming Bigram to GPTLanguageModel (it's no longer a bigram model :D )\n",
    "    - make the number of heads and the number of layers as parameters instead of fixed numbers (4 heads and 3 blocks)\n",
    "- then I will move everything to a script `gpt.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 65]), tensor(4.3854, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers = 4\n",
    "n_heads = 4\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size,n_embed)\n",
    "        self.blocks = nn.Sequential(*[block(n_embed,num_heads=n_heads) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        \"\"\"\n",
    "        idx: the token indices, shape (batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        B,T = idx.shape\n",
    "        emb = self.token_embedding_table(idx) # embeddings of shape (batch_size,sequence_length,emb_size)\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(T,device=idx.device)) # (sequence_length,emb_size)\n",
    "        x = emb + pos_emb # adding shapes (batch_size,sequence_length,emb_size) + (sequence_length,emb_size) -> broadcasting for the batch_size dimension\n",
    "        \n",
    "        # feed the input to the blocks\n",
    "        x = self.blocks(x) # (batch_size,sequence_length,emb_size)\n",
    "\n",
    "        # layer normalization before the language model head\n",
    "        x = self.ln_f(x)\n",
    "        # feed the output of the blocks to the language model head\n",
    "        logits = self.lm_head(x) # the logits of shape (batch_size,sequence_length,vocab_size)\n",
    "\n",
    "        # if the targets are not provided (during generation)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.reshape(-1,vocab_size),targets.reshape(-1)) # we need to collapse the batch_size and the sequence length dimensions together (flatten out the timesteps as individual examples), that is what the loss expects\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        \"\"\"\n",
    "        idx: token indices of some batch (the same one used in training) (batch_size,sequence_length)\n",
    "        we will basically take the indices and expand the sequence length using generation (sampling) up to max_new_tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the sequence length to the block size\n",
    "            idx_cropped = idx[:,-block_size:]\n",
    "            # inference the idx\n",
    "            logits, _ = self(idx_cropped) # batch_size,sequence_length,vocab_size\n",
    "            # focus only on the last timestep (the bigram only needs the last character to predict the next), but later we can feed all the previous characters\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1) # still (batch_size, vocab_size), but each example in the batch now is betwee 0 and 1 and sums to 1\n",
    "            # sample from the dsitribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # batch_size,1, sampled next indices for each example in the batch\n",
    "            # concatenate the sampled indices to the current indices (along the sequence length dimension)\n",
    "            idx = torch.cat((idx,idx_next),dim=1) # batch_size, sequence_length + 1 = new sequence length\n",
    "        return idx\n",
    "    \n",
    "model = GPTLanguageModel()\n",
    "logits, loss = model(x_batch,y_batch)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- after running the script `gpt.py`, we got around 1.48 loss on the validation set!\n",
    "    - the generation is a lot more recognizable and always like someone speaking (just like the original text)\n",
    "        - except of course the text is nonsensical when you actually read it :D\n",
    "        - but this is just a transformer trained on the character level for 1 Million characters that come from shakespeare\n",
    "            - so it the generation wouldn't make sense at this scale\n",
    "            - but still pretty good demonstration of the power of the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mine: Recape: Architecture evolution\n",
    "\n",
    "- we started with the very basic and naive bigram model\n",
    "    - x -> token embeddings -> logits\n",
    "- then we scaled it up a little\n",
    "    - x -> token embeddings + positional encodings -> linear layer -> logits\n",
    "- then we studied the self-attention mechanism and added it (mine: the features now are the aggregated information from the previous tokens for each token)\n",
    "    - x -> token embeddings + positional encodings -> self-attention -> linear layer -> logits\n",
    "- then we added multi-head attention (multiple communication channels, allowing the tokens to communicate different types of information at each head in parallel)\n",
    "    - x -> token embeddings + positional encodings -> multi-head attention -> linear layer -> logits\n",
    "- then we studied the feed-forward block (computation block) \n",
    "    - so the multi-head attention did the communication, but we went too fast and calculated the logits on them\n",
    "        - in other words, the tokens looked at each other but didn't really have a lot of time to `think on` what they found from the other tokens \n",
    "        - that is why we add the feed-forward block after the multi-head attention\n",
    "            - our feed-forward block is a linear layer followed by a ReLU and maps the n_embed to n_embed (to allow each token to think on the data it gathered individually)\n",
    "    - x -> token embeddings + positional encodings -> multi-head attention -> feed-forward block -> linear layer -> logits\n",
    "\n",
    "- then we grouped the multi-head attention and the feed-forward into a single block -that does communication then computation- \n",
    "    - this is basically the decoder block in the transformer architecture, but without the cross-attention to the encoder\n",
    "    - that is because we want to repeat this block multiple times\n",
    "    - shape analysis of this block\n",
    "        - it takes the embeddings added to it the positional encodings, shape (batch_size,sequence_length,n_embed)\n",
    "        - applies multi-head attention with head_size = n_embed // number of heads (so that their concatenation will be of the same size as the embeddings)\n",
    "            - so the output of the multi-head attention will be of shape (batch_size,sequence_length,n_embed)\n",
    "        - then applies the feed-forward block that maps the n_embed to n_embed, output of shape (batch_size,sequence_length,n_embed)\n",
    "            - to allow each token to think on the data it gathered individually\n",
    "    - so we tried this architecture x -> token embeddings + positional encodings -> transformer block x 3 -> linear layer -> logits\n",
    "    - we didn't get good results, because the network is getting deeper and deeper, and deep neural nets suffer from optimization issues\n",
    "\n",
    "- then we studied add & norm blocks (residual connections and layer normalization)\n",
    "    - residual connections: allows us to deepen the network without optimization issues, and allows each block to add to the output of the previous block\n",
    "        - we added the residual connections inside the transformer block\n",
    "            - inside the block we have x, then we fork-off to do the multi-head attention then add the result back to the residual pathway, then fork-off to do the feed-forward block then add the result back to the residual pathway\n",
    "            - so we projected the output of the multi-head attention and the feed-forward block to the same shape as the residual pathway (n_embed) using a linear layer (they were already of the same shape, but this is general in case we want to change their shapes)\n",
    "                - and indeed we changed the feed-forward block to output n_embed*4 instead of n_embed to be consistent with the paper, then we projected it back to n_embed using a linear layer\n",
    "            \n",
    "        - now using the same architecture above, x -> token embeddings + positional encodings -> transformer block (with residual connections) x 3 -> linear layer -> logits and got pretty good results\n",
    "    - layer normalization\n",
    "        - we added layer normalization inside the transformer block (a layer normalization before the multi-head attention and another one before the feed-forward block)\n",
    "            - this became more common in the last 5 years, to apply layer normalization before the block\n",
    "                - so we have block input -> layer norm -> block -> project it to the residual pathway (instead of add & norm after the block)\n",
    "        - we also added layer normalization at the end right before the final linear layer (language model head)\n",
    "        - then we trained again x -> token embeddings + positional encodings -> transformer block (with residual connections and layer normalization) x 3 -> layer norm -> linear layer -> logits and got better results\n",
    "\n",
    "- we then added dropout\n",
    "    - we added it inside the transformer block (right before the blocks output is added to the residual pathway)\n",
    "    - we also added it inside the multi-head attention block (on the affinities -the wei matrix- right before using it to aggregate the values)\n",
    "        - this randomly prevents some of the nodes from communicating\n",
    "    - we did so to scale-up the model without worrying about overfitting\n",
    "\n",
    "- then we made the number of heads and the number of blocks as parameters instead of fixed numbers (4 heads and 3 blocks previously)\n",
    "    - and now we have our gpt :D\n",
    "\n",
    "- so, we started with the very basic bigram model, and ended up with a pretty complete transformer that is similar to the decoder block in the transformer architecture (except for the cross-attention to the encoder)\n",
    "\n",
    "- our implemented architecture\n",
    "![transformer](assets/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Vs decoder vs both?\n",
    "\n",
    "- decoder\n",
    "    - what we implemented here is a decoder only transformer\n",
    "        - so there is no encoder or cross attention\n",
    "    - our block only has a self-attention and the feed-forward block\n",
    "    - the reason we have a decoder only, is because we are just generating text and it is unconditioned on anything (just blabbering on according to a given dataset)\n",
    "        - mine: that is what i said earlier on the difference between a language model (decoder only) and an encoder-decoder architecture (a conditional language model)\n",
    "    - and what makes it a decoder is that we are using a masked multi-head attention (masking the future tokens)\n",
    "        - so it has this auto-regressive property where we can just go and sample from it\n",
    "    - check our implementation above\n",
    "    - so we have nothing to encode, there is no conditioning, we just have a text file and we want to imitate it (that is why we are using a decoder only transformer as done in GTP)\n",
    "\n",
    "\n",
    "- encoder-decoder\n",
    "    - the reason that the original paper had an encoder-decoder architecture, is because it is a machine translation paper\n",
    "        - it is expected to encode a source sentence then decoder its translation in the target language\n",
    "        - so it reads in the source sentence and conditions on it, and then start the generation with a special token \"start of sentence\" token\n",
    "    - example\n",
    "        - Encode: Les rseaux de neurones sont gniaux! \n",
    "        - Decode: `<SOS>` Neural Networks are awesome! `<EOS>`\n",
    "        - so the decoder will do exactly what we did here, but this generation will be conditioned on some additional information \"the french sentence\"\n",
    "        - so the encoder reads the source sentence, we are going to take the french sentence and we are going to create tokens from it and put a transformer block on it -the encoder block- \n",
    "            - and this time there is no masking, so all the tokens are allowed to communicate with each other \n",
    "        - once they have encoded it (mine: applying the transformer block on it several times), they basically come out of the last block and then the decoder (which does language modeling) will take that output to the cross-attention block (a normal multi-head attention, not masked, but with the queries coming from the target sentence, and the keys and values coming from the encoder output)\n",
    "            - and those keys and values (from the encoder's last block) will be fed to the cross attention to every single block of the decoder -check the image below-\n",
    "            - so it is conditioning the decoding on having seen the fully encoded french prompt -the source sentence-\n",
    "\n",
    "![attention architecture spanned](assets/attention_architecture_spanned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nano-GTP\n",
    "\n",
    "can be found in [here](https://github.com/karpathy/nanoGPT)\n",
    "\n",
    "- it is basically 2 files of interest\n",
    "    - train.py\n",
    "        - all the boiler plate code for training the model, just like what we have but a lit more complicated, as we also do\n",
    "            - saving and loading the checkpoints\n",
    "            - decaying the learning rate\n",
    "            - using distributed training across multiple GPUs\n",
    "    - model.py\n",
    "        - looks very similar to what we done here (almost identical), except for few differences\n",
    "            - it has a causal self attention block (parallel multi-head attention)\n",
    "                - we have a single head then we create multiple heads of them in here and concatenate them\n",
    "                - but there, all of it is implemented in a batched manner (so k,q,v for example is (batch_size,num_heads,sequence_length,head_size) instead of (batch_size,sequence_length,head_size))\n",
    "                - `all the heads are treated as a batch dimension as well`\n",
    "            - using the `gelu` non-linearity instead of the `relu`\n",
    "                - just because OpenAI used it and he wanted to laod their checkpoints\n",
    "            - separating our the parameters into that should be weight decayed and those that shouldn't \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ChatGPT\n",
    "\n",
    "- to train a chatGPT there are roughly 2 stages \n",
    "    - `pretraining stage` (what we did here)\n",
    "        - we train on a large chunk of the internet and just trying to get a first decoder only transformer to babble text \n",
    "            - we did so but only 10M parameters on 1M tokens (if we will use OpenAI vocabulary -subword tokenizer- then it is probably around 300K tokens on the subword level)\n",
    "                - so 10M parameters on 300K tokens\n",
    "            - as for GPT-3, they trained several models with different sizes \n",
    "                - the biggest one has 175B parameters! on 300B tokens!\n",
    "                - so it is a massive infrastructure challenge (we are talking about thousands of GPUs having to talk to each other to train a model of that size)\n",
    "        - at the end of the first stage,  we don't get something that responds to the questions with answers\n",
    "            - we just get a document completer, something that babbles internet (arbitrary news articles, wikipedia and so on)\n",
    "            - so if we gave it a question after the first stage, it would potentially give us more questions or ignore it or follow with whatever it looks like some close document would do in the training data\n",
    "            - so we have undefined behavior\n",
    "    - the `fine-tuning stage`\n",
    "        - we take the pre-trained model and we fine-tune it on a specific task\n",
    "        - in here we actually allign it to be an assistant\n",
    "        - here is how Chatgpt fine-tuned their models [here](https://openai.com/index/chatgpt/), they basically fine-tuned it on 3 steps\n",
    "            1. collect training data that looks specifically like the task we want to do\n",
    "                - so they have documents that have to format where the question is on top and the answer is below\n",
    "                - they have a large number of these but not on the scale of the internet (mayble thousands of examples)\n",
    "                - so we are trying to slowly allign it so that it expects a question on the top and complete the answer \n",
    "            2. they then let the model respond with different answers and let different readers look at them and rank them for their preference\n",
    "                - they use that to train a reward model, so they can predict how much a human would like the response and let the model later choose the best response\n",
    "            3. then they run a PPO (Proximal Policy Optimization) reinforcement learning algorithm to train the model to maximize the reward\n",
    "        - this takes the model from a document completer to a question answerer\n",
    "        - a lot of this data isn't available publicly, and it is much harder to replicate this stage\n",
    "        - so basically if we wanted to make it alligned to a specific way to do some task (question answerer, or a chatbot, or detect sentiment), basically anytime we want something more than a document completer, we have to complete further stages of fine-tuning which we did not cover\n",
    "        - the fine-tuning can be simple supervised learning, or something more fancy like what OpenAI did\n",
    "\n",
    "- more on that on the next notebook\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
