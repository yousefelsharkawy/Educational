{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization in LLMs \n",
    "\n",
    "- it is necessary to understand in some detail, a lot of oddness in LLMs typically traces back to tokenization\n",
    "\n",
    "- what is tokenization?\n",
    "    - our dataset is text, and we can't just feed it to the model as it is, we need to convert it to a format that the model can understand (numbers)\n",
    "    - so far we did a very naive tokenization, that is on the character level, so our vocabulary mapped each character to a unique number\n",
    "        - then we used an embedding table to convert them into unique vectors (now each character is represented by a vector/embedding)\n",
    "        - in other words, we used the integer as a lookup key to get the corresponding row, to get the embedding vector that feeds into the transformer\n",
    "        - these embeddings are trainable, so the model learns to represent and group the characters together in the embedding space based on the context (similar characters are closer to each other in the embedding space)\n",
    "\n",
    "    - in practice, we said that people use a lot more complicated schemes for constructing these token vocabularies\n",
    "        - we will be dealing on a chunk level -mine: subword level-, and algorithms such as BPE (Byte Pair Encoding) are used to construct the vocabulary\n",
    "\n",
    "    - the paper [Language models are unsupervised multitask learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) introduced byte level encoding as a method of tokenization in the context of LLMs (it is the GPT-2 Paper)\n",
    "        - in the paper, the vocabulary has 50,257 tokens, and the context size is 1024 tokens\n",
    "            - so in the attention layer of the transformer, the tokens can see up to 1024 tokens back in the sequence (mine: the last token will see the  1023 tokens before it and itself)\n",
    "\n",
    "    - we will build our own tokenizer. Luckily, Byte pair encoding is not super complicated, and we can build it from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "- Why we need good tokenization? \n",
    "    - tokenization is at the heart of a lot of weirdness in LLMs, so don't brush it off\n",
    "        - in other words, a lot of the issues that may look like just issues with the architecture or the LLM itself, are actually issues with tokenization\n",
    "    - examples on these issues: \n",
    "    - why can't LLM spell words? `Tokenization`\n",
    "    - why can't LLM do super simple string processing tasks like reversing a string? `Tokenization`\n",
    "    - why is LLM worse at non-english languages? `Tokenization`\n",
    "    - why is LLM bad at simple arithmetic? `Tokenization`\n",
    "    - why did GPT-2 have more than necessary trouble coding in python? `Tokenization`\n",
    "    - why did my LLM abrubtly halt when it sees the string `<endoftext>`? `Tokenization`\n",
    "    - what is this weird warning i get about a \"trailing whitespace\"? `Tokenization`\n",
    "    - why the LLM break if I ask it about \"SolidGoldMagikarp\"? `Tokenization`\n",
    "    - why should I prefer to use YAML over JSON with LLMs? `Tokenization`\n",
    "    - why is LLM not actually end-to-end Language modeling? `Tokenization`\n",
    "    - what is the real root of suffering? `Tokenization` :D\n",
    "\n",
    "- so tokenization is at the heart of many issues, all the above issues can be traced to a large extent to tokenization\n",
    "- good tokenization webApp, the [tiktokenizer](https://tiktokenizer.vercel.app/) \n",
    "    - what is good about it is that tokenization is running live in the browser\n",
    "\n",
    "![image.png](assets/gpt2_tokenizer_example.png)\n",
    "\n",
    "- for the english sentence\n",
    "    - we see the word \"tokenization\" is broken down into 2 tokens: `token` and `ization`\n",
    "    - the token ' is' is a -space is- single token \n",
    "    - the token ' at' -space+at- is a single token\n",
    "    - the token ' the' -space+the- is a single token\n",
    "    - so notice that the space is part of the token chunk\n",
    "\n",
    "- as for the arithmetic example\n",
    "    - 127 is a token,  'space +' is a token, 'space 6' is a token, and the rest of the number '77' is a token\n",
    "    - it is all completely arbitrary (sometimes we have multiple digits in a single token, and sometimes we have individual digits as separate tokens)\n",
    "\n",
    "- we also see that Egg by itself at the beginning of the sentence is 2 tokens, and space+Egg is a single token\n",
    "- lowercase egg is a single token :D (a different token, so it is case sensitive)\n",
    "- EGG (all capital) is also a different token (2 tokens arbitrarily)\n",
    "    - so for the same word, depending on if it's at the beginning of the sentence or not, and depending on upper/lower/mixed case, we have different tokens (different identities)\n",
    "    - and the LLM has to learn from raw data -from all the internet text that it's going to be training on- that these are actually all the same word, and the differences are very small -it has to group these identities together in the embedding space-\n",
    "\n",
    "\n",
    "- in korean, we see why non-english languages work slightly worse in ChatGPT\n",
    "    - we will notice that non-english languages work slightly worse in chatGPT, part of that is because the training dataset for chatGPT is much larger for English than for other languages, but the same is true for the tokenizer!\n",
    "        - we will see that we will train the tokenizer, and that there is a training set for it as well, and there is a lot more english than non-english\n",
    "        - what ends up happening is that we are going to have a lot more longer tokens for english than non-english (mine: the english tokens span more characters than the non-english tokens)\n",
    "            - mine: notice above that the korean sentence is almost tokenized character by character\n",
    "            - so if we have a single sentence in English and we tokenize it, it will be only 10 tokens for example, but if we tokenize the same sentence in other languages, it will be much more tokens (the token in other languages span fewer characters, in other words the chunks are much more broken up into several tokens)\n",
    "            - so we are using a lot more tokens for the exact same thing\n",
    "            - and what this does is bloating up the sequence length (mine: we already talked about when the tokens span fewer characters -like character level tokenization- the sequence length will be much longer)\n",
    "                - and in the attention, if the sentence span a lot of tokens, fewer sentences will fit into the context window, so we are running out of context\n",
    "                    - unlike english, where the sentence is small number of tokens, the transformer will see more context before predicting the next token\n",
    "        - mine: to summarize, the context in english span few tokens, so the transformer -which fits 1024 tokens into its context, allowing them to attend and communicate with each other in the self-attention- will fit a lot more sentences, so it is easier to capture long range dependencies in english. As for non-english languages, the same number of tokens -1024- will span fewer characters, so what will fit into the self-attention context is much less\n",
    "           \n",
    "\n",
    "- the final example we have is python snippet\n",
    "\n",
    "    ![image-2.png](assets/gpt2_python_tokenization.png)\n",
    "\n",
    "    - notice that all individual spaces are all separate tokens, token 220\n",
    "        - so 220 220 220 space+if \n",
    "        - so when the transformer consumes or generates this text, it needs to handle all these spaces individually (all spaces feed in one by one)\n",
    "            - so it is extremely wasteful tokenizing it this way\n",
    "        - that is why GPT-2 is not good with Python, it has nothing to do with coding or the language model itself, it is just that we are wasting a lot of the context size with space tokens (we are bloating out the code on way too many sequence to fit into the context window)\n",
    "\n",
    "\n",
    "\n",
    "- mine: notice how it is important to fit as many information as possible in the context length, since it takes fixed number of tokens, the only way to do that is for the token to span longer chunks of text, so we can fit more information in the context window, so the transformer can learn better from the context\n",
    "    - that is because of the attention mechanism in the context window, all tokens attend and communicate with each other, so it is like the transformer's ready and immediate memory\n",
    "\n",
    "\n",
    "- so the above example's information was spanned in 300 tokens in GPT-2 tokenizer, let's see what GPT-4 tokenizer will do (the tokenizer for GPT4 is called cl100k_base)\n",
    "\n",
    "![image-3.png](assets/gpt4_tokenizer.png)\n",
    "\n",
    "- we see that the count drops to 185 (we used fewer tokens to represent the same information)\n",
    "    - that is because the tokenizer spans longer chunks of text\n",
    "    - remember that we said when the tokenizer span longer chunks of text, we have a lot more tokens (becase we need fewer tokens to represent small information -like character level tokenization- and we need more tokens to represent larger information)\n",
    "        - and indeed the vocabulary size in the GPT-4 tokenizer is roughly double that of GPT-2 tokenizer (we went from 50k to 100k tokens)\n",
    "        - that is a good thing, because the same text is squishd into half as many tokens, so we can fit more information into the context window (we are roughly able to see twice as much text as a context for what token to predict next)\n",
    "\n",
    "- however, just increasing the number of tokens is not strictly better infinitely \n",
    "    - because as we increase the number of tokens, now our embedding table is getting a lot larger, and at the output where we are trying to predict the next token out of all possible tokens grows as well\n",
    "    - so there is a sweet spot in the middle (just right) regarding the vocabulary size where the information is appropriately densed and still fairly efficient\n",
    "\n",
    "- notice the handling of the white space in the GPT-4 tokenizer, all the spaces in python are represented as a single token \n",
    "    - 3 spaces as single token then space+if as a single token\n",
    "    - 7 spaces as a single token\n",
    "    - this was deliberate by openAI when they designed the gpt-4 tokenizer\n",
    "        - grouping a lot more whitespace into a single character densifies python code and therefore, we can attend to more code before it when we try to predict the next token in the sequence \n",
    "        - so the improvment in python coding ability from GPT-2 to GPT-4 is not just a matter of the architecture and details of the opitmization, but also from the design of the tokenizer and how it groups characters into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what makes this tricky is that we want to tokenize not just english text, we want to support different languages and also special characters (like emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text representation in computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요 👋 (Hello in English!)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"안녕하세요 👋 (Hello in English!)\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how is this text represented in python in the first place?\n",
    "    - textual data in python is handled with `str` objects, strings are immutable sequences of unicode code points\n",
    "\n",
    "- mine: historical overview\n",
    "    - people first represented text in computers using ASCII, which represented the english characters and some symbols in integers from 0 to 255 (1 byte)\n",
    "        - so each character is represented by a single byte\n",
    "    - then people wanted to represent more characters, so they came up with unicode, which is a standard that defines a lot more characters and symbols\n",
    "        - for each character, there is a unique integer representation (unicode code point)\n",
    "            -  These code points for range from 0 to 0x10FFFF (1,114,112 decimal), but not all numbers are assigned to characters yet (currently, there are around 150k characters defined)\n",
    "            - so the maximum hex number is 1,114,112, which is represented by 21 bits, which primarily needs 3 bytes (24 bits) to represent any unicode character\n",
    "            - but due to some alignment issues (the architecture of the computer), it is more efficient to represent the unicode characters in 4 bytes (32 bits)\n",
    "        - but using 4 bytes to represent each character -that is called UTF-32- is very inefficient, because most of the characters are in the lower range -like the english characters- and they can be represented by 1 byte\n",
    "        - UTF-8 is a variable-length encoding, where characters can be 1 to 4 bytes long. This allows for efficient encoding of common characters (like those in ASCII) while still being able to represent every Unicode character.\n",
    "            - For example, the character 'A' in UTF-8 is represented as 0x41 (1 byte), just like in ASCII. But a less common character, like '😊', is represented as 0xF0 0x9F 0x98 0x8A (4 bytes).\n",
    "        - This variable-length nature of UTF-8 allows it to be both space-efficient and capable of representing all Unicode characters.\n",
    "        - UTF-16 is another variable-length encoding, but it uses 2 or 4 bytes per character. UTF-32, on the other hand, uses a fixed 4 bytes per character as we said\n",
    "\n",
    "\n",
    "\n",
    "- the way we can access the unicode code point (integer representation) of a character in python is by using the `ord` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 128075, 1610)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('h'), ord('👋'), ord('ي')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ord() expected a character, but string of length 5 found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ord(\"hello\")\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notice that the unicode code points are defined for each character not for multiple characters at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 72,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 69,\n",
       " 110,\n",
       " 103,\n",
       " 108,\n",
       " 105,\n",
       " 115,\n",
       " 104,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(c) for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128075"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([ord(c) for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so characters are already represented as integers in python (computers)\n",
    "\n",
    "- so, why can't we just simply use these integers and not have any tokenization at all? mine: that is character level tokenization, as we use a unique number of each character\n",
    "    - why don't we just use the code point as it is and feed it to the model?\n",
    "    - the vocabulary in that case will be quite long, this is a vocabulary of 150k tokens\n",
    "    - another more important reason is that the standard is very much alive and it is constantly being updated, so it is not a stable representation\n",
    "\n",
    "- so, to find something better we turn to encodings\n",
    "    - unicode defines 3 types of encodings\n",
    "        - UTF-8\n",
    "        - UTF-16\n",
    "        - UTF-32\n",
    "    - these encodings are the way by which we can take unicode code points and translate them into byte streams\n",
    "    - utf-8 is by far the most common, it takes every single code point, and it translates it to a binary stream between 1 to 4 bytes for each code point (so it is a variable length encoding)\n",
    "\n",
    "    ![image.png](assets/utf_8_encoding.png)\n",
    "\n",
    "    - utf-32 is nice because it is a fixed length encoding, but has many other downsides\n",
    "\n",
    "    \n",
    "- so the full spectrum of pros and cons of all these different 3 encodings are beyond the science of this notebook, but you may read more in [utf-8 everywhere manifesto](https://utf8everywhere.org/), which states why UTF-8 is significantly preferred over the other encodings\n",
    "    - it is the only one that is backward compatible with ASCII\n",
    "\n",
    "\n",
    "- so, let's see the encoding of the above text in UTF-8 (let's convert its unicode code points to UTF-8 byte stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTF-8 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94 \\xf0\\x9f\\x91\\x8b (Hello in English!)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translate from unicode code point to bytes (using utf-8 encoding)\n",
    "text.encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- these are the bytes that make up the text (not very human readable, as they are in hexadecimal)\n",
    "- but if we put them in a list, we can read them as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(b'\\xec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 72,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 69,\n",
       " 110,\n",
       " 103,\n",
       " 108,\n",
       " 105,\n",
       " 115,\n",
       " 104,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(text.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(text.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we get the raw bytes that represent the above string according to the UTF-8 encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTF-16 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255,\n",
       " 254,\n",
       " 72,\n",
       " 197,\n",
       " 85,\n",
       " 177,\n",
       " 88,\n",
       " 213,\n",
       " 56,\n",
       " 193,\n",
       " 148,\n",
       " 198,\n",
       " 32,\n",
       " 0,\n",
       " 61,\n",
       " 216,\n",
       " 75,\n",
       " 220,\n",
       " 32,\n",
       " 0,\n",
       " 40,\n",
       " 0,\n",
       " 72,\n",
       " 0,\n",
       " 101,\n",
       " 0,\n",
       " 108,\n",
       " 0,\n",
       " 108,\n",
       " 0,\n",
       " 111,\n",
       " 0,\n",
       " 32,\n",
       " 0,\n",
       " 105,\n",
       " 0,\n",
       " 110,\n",
       " 0,\n",
       " 32,\n",
       " 0,\n",
       " 69,\n",
       " 0,\n",
       " 110,\n",
       " 0,\n",
       " 103,\n",
       " 0,\n",
       " 108,\n",
       " 0,\n",
       " 105,\n",
       " 0,\n",
       " 115,\n",
       " 0,\n",
       " 104,\n",
       " 0,\n",
       " 33,\n",
       " 0,\n",
       " 41,\n",
       " 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the bytes in utf-16 encoding\n",
    "list(text.encode('utf-16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we see how we have 0s alot -for the english part of the string-, which tend to get a sense that this is a bit wasteful encoding (specially for simple ascii characters)\n",
    "    - that is because english characters' unicodes are in the lower range, so they can be represented by 1 byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(text.encode('utf-16')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(text.encode('utf-16')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and indeed it used more bytes (sequence length is 58) to represent the same information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTF-32 encoding (fixed length encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255,\n",
       " 254,\n",
       " 0,\n",
       " 0,\n",
       " 72,\n",
       " 197,\n",
       " 0,\n",
       " 0,\n",
       " 85,\n",
       " 177,\n",
       " 0,\n",
       " 0,\n",
       " 88,\n",
       " 213,\n",
       " 0,\n",
       " 0,\n",
       " 56,\n",
       " 193,\n",
       " 0,\n",
       " 0,\n",
       " 148,\n",
       " 198,\n",
       " 0,\n",
       " 0,\n",
       " 32,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 75,\n",
       " 244,\n",
       " 1,\n",
       " 0,\n",
       " 32,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 40,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 72,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 101,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 108,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 108,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 111,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 32,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 105,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 110,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 32,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 69,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 110,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 103,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 108,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 105,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 115,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 104,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 33,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 41,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(text.encode('utf-32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 255)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(text.encode('utf-32'))), max(list(text.encode('utf-32')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notice that this is very wasteful, even for koeran characters\n",
    "    - mine: for korean characters, the unicode code points are in a range that can be represented by 2 bytes, but we are using 4 bytes so that is why we have 2 values then 2 zeros for each character (to make up the 4 bytes)\n",
    "    - as for the english characters, the range there only needs 1 byte to represent the character (like in ASCII), but we are using 4 bytes to represent each character\n",
    "        - that is why we have 1 value then 3 zeros for each character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so we would like to stick with UTF-8 encoding\n",
    "    - mine: all of them are byte streams, and each byte can be represented by an integer between 0 and 255, but the difference between these encodings is the sequence length\n",
    "    - UTF-8 is the most efficient, it assigns 1 byte for the common characters (like the english characters), and it can go up to 4 bytes for the less common characters (like the emojis)\n",
    "\n",
    "- what if we used the raw bytes as tokens?\n",
    "    - that would imply a vocabulary size of 256 possible tokens (since each byte can be represented by an integer between 0 and 255)\n",
    "    - so it is very small vocabulary, and we have long sequences (even the UTF-8 encoding, some characters can be represented by 1 byte -therefore 1 token- and some characters can be represented by 4 bytes -therefore 4 tokens-)\n",
    "    - mine: so notice that it is even worse than character level tokenization -using code point integers as they are-, the vocabulary size here is much smaller 256 -compared to the 150k tokens in the unicode code points- and so the sequence length here is longer (there each character is represented by a single number therefore a single token, but here each character can be represented by up to 4 bytes, so it can be represented by up to 4 integers so up to 4 tokens per character!!!)\n",
    "    - mine: even if a character is represented by variable number of bytes (therefore multiple tokens), that is not a problem, the transformer will learn on them then generate them as they are then we use the UTF-8 decoder to convert them back to the original string (as it is variable length encoder-decoder), but the problem as we said is that the vocabulary is very small, and the sequence length is very long \n",
    "\n",
    "- so, we don't want to use the raw bytes of the UTF-8 encoding (mine: using the byte values -integers- to index into the embedding table)\n",
    "    - so we want to support larger vocabulary size that we can tune as a hyperparameter, and we also want to stick with the UTF-8 encoding of these strings\n",
    "    - the answer is to use the byte pair encoding (BPE) algorithm, which allows us to compress these byte sequences into a variable amount\n",
    "\n",
    "- it would be wonderful if we could feed in the raw bytes, that means tokenization-free language modeling, and some papers suggested that it is achievable using auto-regressive transformers, but it is not proven out yet in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Byte Pair Encoding (BPE)\n",
    "\n",
    "- it is used to compress any sequences of elements -here sequences of bytes- into a variable amount\n",
    "\n",
    "- example: if we have 4 elements in the vocabulary a,b,c,d \n",
    "    - and we have a sequence of them `aaabdaaabac`\n",
    "\n",
    "1. iteratively find the pair of tokens that occurred the most frequently in the sequence\n",
    "    - in this case, the pair `aa` occurred the most frequently\n",
    "\n",
    "2. replace this pair with a new token \n",
    "    - say we replace them with `z = aa`, so the sequence becomes `zabdzabac`\n",
    "    - now the vocabulary is `z=aa,b,d,a,c`, the element z spans 2 elements therefore the sequence gets shorter\n",
    "\n",
    "3. repeat the process until we reach the desired vocabulary size\n",
    "    - say that we want a vocabulary size of 8, now we have 5 tokens, so we need to repeat the process 3 more times\n",
    "    - now the pair `ab` is the most frequent, so we replace it with `y`, so the sequence becomes `zydzyac`\n",
    "    - now the pair `yz` is the most frequent, so we replace it with `x`, so the sequence becomes `xdxac`\n",
    "\n",
    "- so now we have a vocabulary of 8 tokens\n",
    "    - a\n",
    "    - b\n",
    "    - c\n",
    "    - d\n",
    "    - x = yz\n",
    "    - y = ab\n",
    "    - z = aa\n",
    "- and the final sequence is `xdxac` which is 5 tokens long\n",
    "\n",
    "\n",
    "- in the exact same way, we start with the bytes of the UTF-8 encoding of the text (mine: common characters are represented by 1 byte, and less common characters are represented by 2, 3, or 4 bytes, and the vocabulary size is 256)\n",
    "    - and we iteratively find the most common pair of bytes, and we replace them with a new token, and so on until we reach the desired vocabulary size\n",
    "    - mine: i think we will use it to reach a sweet spot, between the character level tokenization (150k vocabulary size and 1 token per character) and the byte level tokenization (256 vocabulary size and up to 4 tokens per character)\n",
    "        - when we use the byte pair encoding, we will have a vocabulary size that is larger than 256, but smaller than 150k, and the tokens will span multiple characters `for the most common characters` and fewer characters `for the less common characters`\n",
    "        - i think that is why at the beginning of the notebook, we said for non-english tokens, the tokens span fewer characters -as they are not common in the tokenization training set- and that english tokens span more characters -as they are common in the tokenization training set- and so the byte pair encoding will group the common characters together in single tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 40\n",
      "안 [236, 149, 136]\n",
      "녕 [235, 133, 149]\n",
      "하 [237, 149, 152]\n",
      "세 [236, 132, 184]\n",
      "요 [236, 154, 148]\n",
      "  [32]\n",
      "👋 [240, 159, 145, 139]\n",
      "  [32]\n",
      "( [40]\n",
      "H [72]\n",
      "e [101]\n",
      "l [108]\n",
      "l [108]\n",
      "o [111]\n",
      "  [32]\n",
      "i [105]\n",
      "n [110]\n",
      "  [32]\n",
      "E [69]\n",
      "n [110]\n",
      "g [103]\n",
      "l [108]\n",
      "i [105]\n",
      "s [115]\n",
      "h [104]\n",
      "! [33]\n",
      ") [41]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# get the gpt-2 tokenizer\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "print(len(\"안녕하세요 👋 (Hello in English!)\"), len(\"안녕하세요 👋 (Hello in English!)\".encode('utf-8')))\n",
    "for char in \"안녕하세요 👋 (Hello in English!)\":\n",
    "    print(char, list(char.encode('utf-8')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mine: notice how the above 27 characters are represented in sequence of length 40 in the utf-8 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[168,\n",
       " 243,\n",
       " 230,\n",
       " 167,\n",
       " 227,\n",
       " 243,\n",
       " 47991,\n",
       " 246,\n",
       " 168,\n",
       " 226,\n",
       " 116,\n",
       " 168,\n",
       " 248,\n",
       " 242,\n",
       " 50169,\n",
       " 233,\n",
       " 357,\n",
       " 15496,\n",
       " 287,\n",
       " 3594,\n",
       " 8133]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tokenizer.encode(\"안녕하세요 👋 (Hello in English!)\")))\n",
    "tokenizer.encode(\"안녕하세요 👋 (Hello in English!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mine:\n",
    "- the 27 characters got compressed into 21 tokens using the byte pair encoding! (so shorter vocabulary size than character level tokenization and also shorter sequence length :D), how is that possible? check below\n",
    "\n",
    "![image.png](assets/tok_example_1.png)\n",
    "\n",
    "- notice below that each token in the byte pair encoding span different characters\n",
    "    - sometimes the character is represented by multiple tokens (which is worse than character level tokenization, as there each character is represented by a single token)\n",
    "\n",
    "    ![image-2.png](assets/tok_example_2.png)\n",
    "    - sometimes we have a single token representing multiple characters! (which is much better than character level tokenization, as there each token represents a single character)\n",
    "    \n",
    "    ![image-3.png](assets/tok_example_3.png)\n",
    "\n",
    "    \n",
    "- so the result is that we have tokens spanning multiple characters for the most common characters (that make up most of the tokenizer training set)\n",
    "- this clearly shows the problem of non-english tokens spanning feweer characters as they are not common in the tokenization training set, which leads to less context in the self-attention context window and therefore worse performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation (training the tokenizer using byte pair encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "length: 533\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 616\n"
     ]
    }
   ],
   "source": [
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "utf_8_bytes = text.encode(\"utf-8\") # raw bytes using utf-8 encoding (hexadecimal representation)\n",
    "utf_8_bytes = list(map(int, utf_8_bytes)) # convert to a integer representation (easier to work with)\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length:\", len(text))\n",
    "print('---')\n",
    "print(utf_8_bytes)\n",
    "print(\"length:\", len(utf_8_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is the original text it is 533 characters long\n",
    "- and then we have the bytes (encoded in UTF-8) of this text\n",
    "    - it has a length of 616 bytes (which means a lot of these characters are represented by more than 1 byte)\n",
    "\n",
    "- now as we said, we can't use the sequence of bytes as it is (very long sequence), so we will use the BPE algorithm to compress this utf-8 integer sequence into a smaller sequence of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the most common pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((101, 32), 20),\n",
       " ((240, 159), 15),\n",
       " ((226, 128), 12),\n",
       " ((105, 110), 12),\n",
       " ((115, 32), 10),\n",
       " ((32, 97), 10),\n",
       " ((97, 110), 10),\n",
       " ((32, 116), 9),\n",
       " ((116, 104), 8),\n",
       " ((159, 133), 7),\n",
       " ((159, 135), 7),\n",
       " ((97, 114), 7),\n",
       " ((239, 189), 6),\n",
       " ((128, 140), 6),\n",
       " ((140, 240), 6),\n",
       " ((104, 101), 6),\n",
       " ((101, 114), 6),\n",
       " ((109, 101), 6),\n",
       " ((114, 32), 6),\n",
       " ((110, 100), 6),\n",
       " ((32, 105), 6),\n",
       " ((111, 114), 6),\n",
       " ((116, 32), 6),\n",
       " ((110, 103), 6),\n",
       " ((32, 115), 5),\n",
       " ((115, 116), 5),\n",
       " ((100, 101), 5),\n",
       " ((110, 32), 5),\n",
       " ((117, 115), 5),\n",
       " ((44, 32), 5),\n",
       " ((97, 109), 4),\n",
       " ((114, 105), 4),\n",
       " ((32, 102), 4),\n",
       " ((101, 97), 4),\n",
       " ((100, 32), 4),\n",
       " ((110, 116), 4),\n",
       " ((32, 111), 4),\n",
       " ((32, 119), 4),\n",
       " ((111, 117), 4),\n",
       " ((32, 85), 4),\n",
       " ((85, 110), 4),\n",
       " ((110, 105), 4),\n",
       " ((105, 99), 4),\n",
       " ((99, 111), 4),\n",
       " ((111, 100), 4),\n",
       " ((104, 97), 4),\n",
       " ((116, 101), 4),\n",
       " ((103, 32), 4),\n",
       " ((115, 44), 4),\n",
       " ((116, 105), 4),\n",
       " ((32, 240), 3),\n",
       " ((118, 101), 3),\n",
       " ((116, 114), 3),\n",
       " ((101, 115), 3),\n",
       " ((116, 111), 3),\n",
       " ((111, 32), 3),\n",
       " ((114, 116), 3),\n",
       " ((116, 115), 3),\n",
       " ((111, 102), 3),\n",
       " ((32, 112), 3),\n",
       " ((114, 115), 3),\n",
       " ((46, 32), 3),\n",
       " ((108, 108), 3),\n",
       " ((108, 32), 3),\n",
       " ((114, 101), 3),\n",
       " ((97, 116), 3),\n",
       " ((32, 109), 3),\n",
       " ((32, 98), 3),\n",
       " ((32, 100), 3),\n",
       " ((101, 110), 3),\n",
       " ((108, 101), 3),\n",
       " ((33, 32), 2),\n",
       " ((114, 121), 2),\n",
       " ((121, 32), 2),\n",
       " ((32, 110), 2),\n",
       " ((105, 107), 2),\n",
       " ((107, 101), 2),\n",
       " ((119, 101), 2),\n",
       " ((102, 32), 2),\n",
       " ((112, 114), 2),\n",
       " ((114, 111), 2),\n",
       " ((111, 103), 2),\n",
       " ((103, 114), 2),\n",
       " ((114, 97), 2),\n",
       " ((109, 109), 2),\n",
       " ((105, 100), 2),\n",
       " ((97, 108), 2),\n",
       " ((110, 111), 2),\n",
       " ((103, 104), 2),\n",
       " ((104, 116), 2),\n",
       " ((115, 117), 2),\n",
       " ((117, 112), 2),\n",
       " ((112, 112), 2),\n",
       " ((112, 111), 2),\n",
       " ((101, 226), 2),\n",
       " ((102, 116), 2),\n",
       " ((119, 104), 2),\n",
       " ((101, 118), 2),\n",
       " ((110, 115), 2),\n",
       " ((108, 105), 2),\n",
       " ((102, 111), 2),\n",
       " ((32, 114), 2),\n",
       " ((32, 99), 2),\n",
       " ((99, 97), 2),\n",
       " ((98, 101), 2),\n",
       " ((100, 105), 2),\n",
       " ((104, 111), 2),\n",
       " ((116, 97), 2),\n",
       " ((100, 97), 2),\n",
       " ((112, 108), 2),\n",
       " ((105, 116), 2),\n",
       " ((100, 111), 2),\n",
       " ((101, 112), 2),\n",
       " ((111, 110), 2),\n",
       " ((128, 153), 2),\n",
       " ((105, 111), 2),\n",
       " ((239, 188), 1),\n",
       " ((188, 181), 1),\n",
       " ((181, 239), 1),\n",
       " ((189, 142), 1),\n",
       " ((142, 239), 1),\n",
       " ((189, 137), 1),\n",
       " ((137, 239), 1),\n",
       " ((189, 131), 1),\n",
       " ((131, 239), 1),\n",
       " ((189, 143), 1),\n",
       " ((143, 239), 1),\n",
       " ((189, 132), 1),\n",
       " ((132, 239), 1),\n",
       " ((189, 133), 1),\n",
       " ((133, 33), 1),\n",
       " ((133, 164), 1),\n",
       " ((164, 240), 1),\n",
       " ((133, 157), 1),\n",
       " ((157, 240), 1),\n",
       " ((133, 152), 1),\n",
       " ((152, 240), 1),\n",
       " ((133, 146), 1),\n",
       " ((146, 240), 1),\n",
       " ((133, 158), 1),\n",
       " ((158, 240), 1),\n",
       " ((133, 147), 1),\n",
       " ((147, 240), 1),\n",
       " ((133, 148), 1),\n",
       " ((148, 226), 1),\n",
       " ((128, 189), 1),\n",
       " ((189, 32), 1),\n",
       " ((135, 186), 1),\n",
       " ((186, 226), 1),\n",
       " ((135, 179), 1),\n",
       " ((179, 226), 1),\n",
       " ((135, 174), 1),\n",
       " ((174, 226), 1),\n",
       " ((135, 168), 1),\n",
       " ((168, 226), 1),\n",
       " ((135, 180), 1),\n",
       " ((180, 226), 1),\n",
       " ((135, 169), 1),\n",
       " ((169, 226), 1),\n",
       " ((135, 170), 1),\n",
       " ((170, 33), 1),\n",
       " ((159, 152), 1),\n",
       " ((152, 132), 1),\n",
       " ((132, 32), 1),\n",
       " ((32, 84), 1),\n",
       " ((84, 104), 1),\n",
       " ((32, 118), 1),\n",
       " ((110, 97), 1),\n",
       " ((102, 101), 1),\n",
       " ((97, 119), 1),\n",
       " ((32, 104), 1),\n",
       " ((119, 111), 1),\n",
       " ((114, 108), 1),\n",
       " ((108, 100), 1),\n",
       " ((100, 119), 1),\n",
       " ((119, 105), 1),\n",
       " ((101, 46), 1),\n",
       " ((32, 87), 1),\n",
       " ((87, 101), 1),\n",
       " ((32, 107), 1),\n",
       " ((107, 110), 1),\n",
       " ((111, 119), 1),\n",
       " ((119, 32), 1),\n",
       " ((117, 103), 1),\n",
       " ((32, 226), 1),\n",
       " ((128, 156), 1),\n",
       " ((156, 115), 1),\n",
       " ((128, 157), 1),\n",
       " ((157, 32), 1),\n",
       " ((117, 114), 1),\n",
       " ((115, 111), 1),\n",
       " ((116, 119), 1),\n",
       " ((119, 97), 1),\n",
       " ((32, 40), 1),\n",
       " ((40, 119), 1),\n",
       " ((115, 226), 1),\n",
       " ((128, 148), 1),\n",
       " ((148, 108), 1),\n",
       " ((32, 117), 1),\n",
       " ((115, 105), 1),\n",
       " ((119, 99), 1),\n",
       " ((99, 104), 1),\n",
       " ((114, 95), 1),\n",
       " ((95, 116), 1),\n",
       " ((103, 115), 1),\n",
       " ((105, 103), 1),\n",
       " ((116, 63), 1),\n",
       " ((63, 41), 1),\n",
       " ((41, 46), 1),\n",
       " ((32, 66), 1),\n",
       " ((66, 117), 1),\n",
       " ((117, 116), 1),\n",
       " ((97, 98), 1),\n",
       " ((98, 115), 1),\n",
       " ((114, 117), 1),\n",
       " ((115, 101), 1),\n",
       " ((101, 44), 1),\n",
       " ((105, 118), 1),\n",
       " ((118, 105), 1),\n",
       " ((115, 97), 1),\n",
       " ((100, 45), 1),\n",
       " ((45, 112), 1),\n",
       " ((112, 97), 1),\n",
       " ((97, 103), 1),\n",
       " ((103, 101), 1),\n",
       " ((32, 83), 1),\n",
       " ((83, 116), 1),\n",
       " ((114, 100), 1),\n",
       " ((108, 117), 1),\n",
       " ((111, 122), 1),\n",
       " ((122, 101), 1),\n",
       " ((101, 109), 1),\n",
       " ((110, 110), 1),\n",
       " ((110, 101), 1),\n",
       " ((101, 120), 1),\n",
       " ((120, 101), 1),\n",
       " ((111, 116), 1),\n",
       " ((109, 111), 1),\n",
       " ((97, 32), 1),\n",
       " ((32, 108), 1),\n",
       " ((116, 116), 1),\n",
       " ((116, 108), 1),\n",
       " ((105, 109), 1),\n",
       " ((109, 105), 1),\n",
       " ((103, 46), 1),\n",
       " ((32, 73), 1),\n",
       " ((73, 32), 1),\n",
       " ((110, 226), 1),\n",
       " ((153, 116), 1),\n",
       " ((98, 108), 1),\n",
       " ((108, 97), 1),\n",
       " ((105, 108), 1),\n",
       " ((102, 105), 1),\n",
       " ((111, 108), 1),\n",
       " ((104, 105), 1),\n",
       " ((109, 121), 1),\n",
       " ((121, 115), 1),\n",
       " ((32, 101), 1),\n",
       " ((32, 51), 1),\n",
       " ((51, 48), 1),\n",
       " ((48, 32), 1),\n",
       " ((32, 121), 1),\n",
       " ((121, 101), 1),\n",
       " ((97, 102), 1),\n",
       " ((153, 115), 1),\n",
       " ((110, 99), 1),\n",
       " ((99, 101), 1),\n",
       " ((112, 116), 1),\n",
       " ((110, 46), 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stats(iteger_sequence):\n",
    "    counts = {}\n",
    "    for pair in zip(iteger_sequence, iteger_sequence[1:]): # pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(utf_8_bytes)\n",
    "sorted(stats.items(), key=lambda x: x[1], reverse=True) # stat.items returns (key, value) pairs, and by default it sorts by the first element of the pair, but we want to sort by the second element (the value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get) # stats will return the keys to the function, we don't want the max on the keys, so we provide it with a function stats.get which will map each key to the value, then the max function will get the maximum value\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so 101,32 is the most common pair in the sequence with 20 occurences\n",
    "\n",
    "- if we would like to talk a look of what exactly that pair is, we can use `chr` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 108, 108, 111]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('hello'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'e', 'l', 'l', 'o']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode 104 from utf-8\n",
    "[chr(x) for x in [104, 101, 108, 108, 111]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101),chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### replace the most common pair with a new token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so basically 'e+space' are the most common pair in the sequence\n",
    "- now we want to replace this pair with a new token, with a new id of 256 (since these tokens currently go from 0 to 255)\n",
    "    - so when we create a new token, it will have an id of 256\n",
    "\n",
    "- we will iterate over this sequence and replace all the occurences of this pair with the new token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 99, 9, 1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge(sequence, pair, id_):\n",
    "    \"\"\"\n",
    "    replace each pair occurences in the sequence with the new id (idx)\n",
    "    \"\"\"\n",
    "    new_sequence = []\n",
    "    i  = 0\n",
    "    while i < len(sequence):\n",
    "        # if we are not at the last character -to avoid out of boundary index-, and the following two characters are the pair \n",
    "        if i < len(sequence) - 1 and sequence[i] == pair[0] and sequence[i+1] == pair[1]:\n",
    "            new_sequence.append(id_)\n",
    "            i += 2 # +2 since we subtituted for this character and the one after it\n",
    "        else:\n",
    "            new_sequence.append(sequence[i])\n",
    "            i += 1\n",
    "    return new_sequence\n",
    "\n",
    "merge([5,6,6,7,9,1],(6,7),99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Length: 596\n"
     ]
    }
   ],
   "source": [
    "tokens2 = merge(utf_8_bytes, top_pair, 256)\n",
    "print(tokens2)\n",
    "print(\"Length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we got down from 616 to 596, so decreased by 616-596 = 20 numbers, which makes sense as we replaced 20 occurences of the pair (40 numbers) with a single number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "- we will loop on the above functions to do it iteratively\n",
    "- and it is up to us to how many iterations we want to do\n",
    "    - the more we iterate, the larger will be the vocabulary size, and the shorter will be the sequence length\n",
    "    - and there is a sweet spot in the middle that works best in practice\n",
    "    - so it is a hyperparameter that we tune\n",
    "        - gpt-2 used 50k tokens, gpt-4 used 100k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the training text longer to have more representative token statistics\n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"A Programmer’s Introduction to Unicode March 3, 2017 · Coding · 22 Comments  Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺\\u200c🇳\\u200c🇮\\u200c🇨\\u200c🇴\\u200c🇩\\u200c🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view.  I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And More… Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It’s not just that Unicode contains a much larger number of characters, although that’s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere “character set” to be. We’ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, it’s hard not to find oneself asking, “Why do we need all this? Is this really necessary? Couldn’t it be simplified?”  However, Unicode aims to faithfully represent the entire world’s writing systems. The Unicode Consortium’s stated goal is “enabling people around the world to use computers in any language”. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there’s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, it’s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesn’t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one text—which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, you’ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but don’t be discouraged—think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Let’s start with some general orientation. The basic elements of Unicode—its “characters”, although that term isn’t quite right—are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix “U+”, such as U+0041 “A” latin capital letter a or U+03B8 “θ” greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of them—about 12% of the codespace—are actually assigned, to date. There’s plenty of room for growth! Unicode also reserves an additional 137,468 code points as “private use” areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, it’s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. It’s arranged in tiles for visual coherence; each small square is 16×16 = 256 code points, and each large square is a “plane” of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the “Basic Multilingual Plane”, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no more—Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15–16 are reserved entirely for private use.  Scripts Let’s zoom in on the first three planes, since that’s where the action is:  Map of scripts in Unicode planes 0–2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibility—it’s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usage—in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0–2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0–2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1–2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings We’ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = “Unicode Transformation Format”), but it’s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, you’ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether it’s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000–U+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080–U+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800–U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000–U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128–255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idioms—such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)—will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, it’s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isn’t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the “characters” in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clusters—more about those later), not bytes. When you measure the “length” of a string, you’ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that you’re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000–U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000–U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called “surrogates”. All the code points in the range U+D800–U+DFFF—or in other words, the code points that match the binary prefixes 110110 and 110111 in the table above—are reserved specifically for UTF-16 encoding, and don’t represent any valid characters on their own. They’re only meant to occur in the 2-word encoding pattern above, which is called a “surrogate pair”. Surrogate code points are illegal in any other context! They’re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different “encodings”; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didn’t originally plan for. Surrogates were then introduced, as—to put it bluntly—a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesn’t support UTF-8—only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! 😊)  By the way, UTF-16’s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesn’t match the system’s endianness, the BOM will be decoded as U+FFFE, which isn’t a valid code point.)  Combining Marks In the story so far, we’ve been focusing on code points. But in Unicode, a “character” can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabet—and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called “combining marks”, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character “Á” can be expressed as a string of two code points: U+0041 “A” latin capital letter a plus U+0301 “◌́” combining acute accent. This string automatically gets rendered as a single character: “Á”.  Now, Unicode does also include many “precomposed” code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 “Á” latin capital letter a with acute or U+1EC7 “ệ” latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they don’t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by ͖͟ͅr͞aṋ̫̠̖͈̗d͖̻̹óm̪͙͕̗̝ļ͇̰͓̳̫ý͓̥̟͍ ̕s̫t̫̱͕̗̰̼̘͜a̼̩͖͇̠͈̣͝c̙͍k̖̱̹͍͘i̢n̨̺̝͇͇̟͙ģ̫̮͎̻̟ͅ ̕n̼̺͈͞u̮͙m̺̭̟̗͞e̞͓̰̤͓̫r̵o̖ṷs҉̪͍̭̬̝̤ ̮͉̝̞̗̟͠d̴̟̜̱͕͚i͇̫̼̯̭̜͡ḁ͙̻̼c̲̲̹r̨̠̹̣̰̦i̱t̤̻̤͍͙̘̕i̵̜̭̤̱͎c̵s ͘o̱̲͈̙͖͇̲͢n͘ ̜͈e̬̲̠̩ac͕̺̠͉h̷̪ ̺̣͖̱ḻ̫̬̝̹ḙ̙̺͙̭͓̲t̞̞͇̲͉͍t̷͔̪͉̲̻̠͙e̦̻͈͉͇r͇̭̭̬͖,̖́ ̜͙͓̣̭s̘̘͈o̱̰̤̲ͅ ̛̬̜̙t̼̦͕̱̹͕̥h̳̲͈͝ͅa̦t̻̲ ̻̟̭̦̖t̛̰̩h̠͕̳̝̫͕e͈̤̘͖̞͘y҉̝͙ ̷͉͔̰̠o̞̰v͈͈̳̘͜er̶f̰͈͔ḻ͕̘̫̺̲o̲̭͙͠ͅw̱̳̺ ͜t̸h͇̭͕̳͍e̖̯̟̠ ͍̞̜͔̩̪͜ļ͎̪̲͚i̝̲̹̙̩̹n̨̦̩̖ḙ̼̲̼͢ͅ ̬͝s̼͚̘̞͝p͙̘̻a̙c҉͉̜̤͈̯̖i̥͡n̦̠̱͟g̸̗̻̦̭̮̟ͅ ̳̪̠͖̳̯̕a̫͜n͝d͡ ̣̦̙ͅc̪̗r̴͙̮̦̹̳e͇͚̞͔̹̫͟a̙̺̙ț͔͎̘̹ͅe̥̩͍ a͖̪̜̮͙̹n̢͉̝ ͇͉͓̦̼́a̳͖̪̤̱p̖͔͔̟͇͎͠p̱͍̺ę̲͎͈̰̲̤̫a̯͜r̨̮̫̣̘a̩̯͖n̹̦̰͎̣̞̞c̨̦̱͔͎͍͖e̬͓͘ ̤̰̩͙̤̬͙o̵̼̻̬̻͇̮̪f̴ ̡̙̭͓͖̪̤“̸͙̠̼c̳̗͜o͏̼͙͔̮r̞̫̺̞̥̬ru̺̻̯͉̭̻̯p̰̥͓̣̫̙̤͢t̳͍̳̖ͅi̶͈̝͙̼̙̹o̡͔n̙̺̹̖̩͝ͅ”̨̗͖͚̩.̯͓  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, children’s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\tאֶת דַלְתִּי הֵזִיז הֵנִיעַ, קֶטֶב לִשְׁכַּתִּי יָשׁוֹד Normal writing (no niqqud):\\tאת דלתי הזיז הניע, קטב לשכתי ישוד Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, “ह” + “\\u200bि” = “हि” (“h” + “i” = “hi”). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, it’s also possible to dynamically compose them by concatenating their jamo. For example, “ᄒ” + “ᅡ” + “ᆫ” = “한” (“h” + “a” + “n” = “han”). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express “the same” string—different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character “Á” either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: “ǡ” (dot, then macron) is different from “ā̇” (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesn’t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter “ệ” can be expressed in five different ways:  Fully precomposed: U+1EC7 “ệ” Partially precomposed: U+1EB9 “ẹ” + U+0302 “◌̂” Partially precomposed: U+00EA “ê” + U+0323 “◌̣” Fully decomposed: U+0065 “e” + U+0323 “◌̣” + U+0302 “◌̂” Fully decomposed: U+0065 “e” + U+0302 “◌̂” + U+0323 “◌̣” Unicode refers to set of strings like this as “canonically equivalent”. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a “find in file” operation and the user searches for “ệ”, it should, by default, find occurrences of any of the five versions of “ệ” above!  Normalization Forms To address the problem of “how to handle canonically equivalent strings”, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The “NFD” normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesn’t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The “NFC” form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The “K” here refers to compatibility decompositions, which cover characters that are “similar” in some sense but not visually identical. However, I’m not going to cover that here.  Grapheme Clusters As we’ve seen, Unicode contains various cases where a thing that a user thinks of as a single “character” might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single “user-perceived character”.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. It’s approximately “a base code point followed by any number of combining marks”, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: they’re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you can’t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limit—say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldn’t want to enforce that by just truncating bytes. At a minimum, you’d want to “round down” to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And More… There’s much more that could be said about Unicode from a programmer’s perspective! I haven’t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issues—how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps I’ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and “characters”. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and it’s clear that we’re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)—C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fonts—set of fonts intended to cover all assigned code points\"\n",
    "utf_8_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "utf_8_bytes = list(map(int, utf_8_bytes)) # convert to a list of integers in range 0-255 for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging pair: (101, 32) to a new token: 256\n",
      "Merging pair: (105, 110) to a new token: 257\n",
      "Merging pair: (115, 32) to a new token: 258\n",
      "Merging pair: (116, 104) to a new token: 259\n",
      "Merging pair: (101, 114) to a new token: 260\n",
      "Merging pair: (99, 111) to a new token: 261\n",
      "Merging pair: (116, 32) to a new token: 262\n",
      "Merging pair: (226, 128) to a new token: 263\n",
      "Merging pair: (44, 32) to a new token: 264\n",
      "Merging pair: (97, 110) to a new token: 265\n",
      "Merging pair: (111, 114) to a new token: 266\n",
      "Merging pair: (100, 32) to a new token: 267\n",
      "Merging pair: (97, 114) to a new token: 268\n",
      "Merging pair: (101, 110) to a new token: 269\n",
      "Merging pair: (257, 103) to a new token: 270\n",
      "Merging pair: (261, 100) to a new token: 271\n",
      "Merging pair: (121, 32) to a new token: 272\n",
      "Merging pair: (46, 32) to a new token: 273\n",
      "Merging pair: (97, 108) to a new token: 274\n",
      "Merging pair: (259, 256) to a new token: 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(sequence, pair, id_):\n",
    "    \"\"\"\n",
    "    replace each pair occurences in the sequence with the new id (idx)\n",
    "    \"\"\"\n",
    "    new_sequence = []\n",
    "    i  = 0\n",
    "    while i < len(sequence):\n",
    "        # if we are not at the last character -to avoid out of boundary index- \n",
    "        if i < len(sequence) - 1 and sequence[i] == pair[0] and sequence[i+1] == pair[1]:\n",
    "            new_sequence.append(id_)\n",
    "            i += 2 # +2 since we subtituted for this character and the one after it\n",
    "        else:\n",
    "            new_sequence.append(sequence[i])\n",
    "            i += 1\n",
    "    return new_sequence\n",
    "\n",
    "vocab_size = 276 # the desired final vocab size (including the 256 tokens we already have) -hyperparameter-\n",
    "num_merges = vocab_size - 256 # we already have 256 tokens\n",
    "sequence = list(utf_8_bytes) # one way in python to copy a list -by calling the list constructor on the list itself- so that we don't lose the original sequence\n",
    "\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(sequence)\n",
    "    top_pair = max(stats, key=stats.get)\n",
    "    id_ = 256 + i\n",
    "    print(\"Merging pair:\", top_pair, \"to a new token:\", id_)\n",
    "    sequence = merge(sequence, top_pair, id_)\n",
    "    # store the pair and the new id\n",
    "    merges[top_pair] = id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens before merging: 24597\n",
      "Tokens after merging: 19438\n",
      "Compression ratio: 1.27X\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens before merging:\", len(utf_8_bytes))\n",
    "print(\"Tokens after merging:\", len(sequence))\n",
    "print(\"Compression ratio:\", f'{len(utf_8_bytes) / len(sequence):.2f}X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we started of with 24K bytes, and after merging for 20 times, we now have 19K tokens\n",
    "    - and the compression ratio is 1.27X\n",
    "\n",
    "- the more we increase the vocabulary size, the more we will be able to compress the sequence\n",
    "\n",
    "- that was the training of the tokenizer if you will"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot 2024-02-14 at 8.23.33 AM.png](data:image/png;base64,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)\n",
    "\n",
    "- notice that tokenizers are a completely separate object from the LLM itself\n",
    "    - everything in this lecture is not really touching the LLM iself\n",
    "    - we are just training the tokenizer, which is a completely separate preprcessing stage usually\n",
    "        - so, the tokenizer will have its own training set, just like the LLM has a potentially different training set\n",
    "        - we would run it a single time in the beginning, and once it is trained -the vocabulary is built and we have the merges-, we can do both encoding and decoding using it for any text (check the above image)\n",
    "            - we use the trained tokenizer to encoder the raw text (in unicode code points then UTF-8 encodings) into the actual token sequence that we will feed into the LLM\n",
    "            - we can then decode the output of the LLM (which will also be a token sequence) back into raw text (in unicode code points)\n",
    "            - so the tokenizer once its trained, it acts as a translation layer between the raw text and the token sequence that the LLM can understand\n",
    "            - and then the LLM will be trained as a step 2 afterwards\n",
    "\n",
    "\n",
    "- typically in a state-of-the-art application, we might take all of the training data for the language model and run it through the tokenizer, and translate everything into a massive token sequence\n",
    "    - then we can then throw away the raw text and just keep the token sequence (those are stored on disk, and that is what the LLM is actually reading when it is training on them)\n",
    "\n",
    "- so all in all, this is a completely separate stage. it usually has its own entire training set, we may want to have those training sets be different between the tokenizer and the LLM\n",
    "    - for example, when we train the tokenizer, we don't just care about the performance of english text, we care about many different languages and also code \n",
    "    - so we may look into different kinds of mixtures of different kinds of languages and code, because the amount of different language that we have in the tokenizer training set will determine the most common pairs and how many merges of it will be and therefore determines the density of which this type of data will be represented in the token space\n",
    "        - so if we add some amount of data, like a ton of Japanese data in the tokenizer training set, then more japanese tokens will get merged (so the tokens will span multiple japanese characters) and therefore japanese will have shorter sequences (which means more information in the finite context length of the transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding-decoding using the tokenizer we trained\n",
    "- now we have the vocabulary and the merges -they are necessary as they tell us what tokens the new tokens are subtituting for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (105, 110): 257,\n",
       " (115, 32): 258,\n",
       " (116, 104): 259,\n",
       " (101, 114): 260,\n",
       " (99, 111): 261,\n",
       " (116, 32): 262,\n",
       " (226, 128): 263,\n",
       " (44, 32): 264,\n",
       " (97, 110): 265,\n",
       " (111, 114): 266,\n",
       " (100, 32): 267,\n",
       " (97, 114): 268,\n",
       " (101, 110): 269,\n",
       " (257, 103): 270,\n",
       " (261, 100): 271,\n",
       " (121, 32): 272,\n",
       " (46, 32): 273,\n",
       " (97, 108): 274,\n",
       " (259, 256): 275}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the rest we can use chr to decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding\n",
    "- given a token sequence, let's decode it to get the raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{256: (101, 32),\n",
       " 257: (105, 110),\n",
       " 258: (115, 32),\n",
       " 259: (116, 104),\n",
       " 260: (101, 114),\n",
       " 261: (99, 111),\n",
       " 262: (116, 32),\n",
       " 263: (226, 128),\n",
       " 264: (44, 32),\n",
       " 265: (97, 110),\n",
       " 266: (111, 114),\n",
       " 267: (100, 32),\n",
       " 268: (97, 114),\n",
       " 269: (101, 110),\n",
       " 270: (257, 103),\n",
       " 271: (261, 100),\n",
       " 272: (121, 32),\n",
       " 273: (46, 32),\n",
       " 274: (97, 108),\n",
       " 275: (259, 256)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_pair = {token:pair for pair,token in merges.items()}\n",
    "token_to_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair(token):\n",
    "    result = []\n",
    "    if token in token_to_pair:\n",
    "        a,b = token_to_pair[token]\n",
    "        result.extend(get_pair(a))\n",
    "        result.extend(get_pair(b))\n",
    "    else:\n",
    "        result.append(token)\n",
    "    return result\n",
    "\n",
    "def my_decode(token_sequence):\n",
    "    uncompressed_tokens = [] # mine: stores the utf-8 encodings -integer representation- \n",
    "    for token in token_sequence:\n",
    "        if token in token_to_pair:\n",
    "            uncompressed_tokens.extend(get_pair(token))\n",
    "        else:\n",
    "            uncompressed_tokens.append(token)\n",
    "\n",
    "    return bytes(uncompressed_tokens).decode('utf-8') # converts to bytes and then decode to utf-8\n",
    "\n",
    "decoded_text = my_decode(sequence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text == text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Karpathy's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'e ',\n",
       " 257: b'in',\n",
       " 258: b's ',\n",
       " 259: b'th',\n",
       " 260: b'er',\n",
       " 261: b'co',\n",
       " 262: b't ',\n",
       " 263: b'\\xe2\\x80',\n",
       " 264: b', ',\n",
       " 265: b'an',\n",
       " 266: b'or',\n",
       " 267: b'd ',\n",
       " 268: b'ar',\n",
       " 269: b'en',\n",
       " 270: b'ing',\n",
       " 271: b'cod',\n",
       " 272: b'y ',\n",
       " 273: b'. ',\n",
       " 274: b'al',\n",
       " 275: b'the '}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a vocabulary that stores the bytes representation of all 256 values\n",
    "vocab = {idx:bytes([idx]) for idx in range(256)}\n",
    "\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] # for the merged tokens index store their merged bytes representation\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notice that we don't need recursion as the compounded tokens (when a pair is merged then their result is merged again) already expressed in srings (since we looped on merged in order), so each compounded token has already all its children defined in the vocabulary\n",
    "- notice the b before the strings, indicating that these are bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then we just loop on the tokens sequence and index the vocab to get the corresponding string\n",
    "def decode(tokens_sequence):\n",
    "    # call the vocab on all indices to get the bytes representation of all utf-8 encodings\n",
    "    tokens = b\"\".join(vocab[idx] for idx in tokens_sequence)\n",
    "    text = tokens.decode('utf-8')\n",
    "    return text\n",
    "\n",
    "decoded_text = decode(sequence)\n",
    "decoded_text == text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notice that we put b before the string, to indicate that this is a byte string (in my implementation, i converted it to bytes then decoded it)\n",
    "    - but here we already had the vocabulary as bytes, so we just put b before the string to indicate that it is a byte string, then decode it\n",
    "\n",
    "- both implementations above have an issue, and it has something to do with utf-8 decoding\n",
    "    - try decoding something with 128, it will throw an error\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m decode([\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m153\u001b[39m])\n",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(tokens_sequence)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(tokens_sequence):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# call the vocab on all indices to get the bytes representation of all utf-8 encodings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(vocab[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tokens_sequence)\n\u001b[1;32m----> 5\u001b[0m     text \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "decode([128,153])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'’'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([226, 128,153])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 128 refers to the byte 0x80, which is ivalid start byte\n",
    "- what does this mean?\n",
    "    - we said that there is a specific encoding schema for utf-8\n",
    "    - one of the rules is that they have to have a specific prefix \n",
    "\n",
    "        ![image.png](assets/utf_8_encoding.png)\n",
    "        - all starting bytes can either be 0xxxxxxx or 110xxxxx or 1110xxxx or 11110xxx\n",
    "    - but 128 is 10000000, which is not a valid byte in utf-8\n",
    "- the way to fix this is in the decode function there is a parameter called `errors`\n",
    "    - it is set to `strict` by default, which means that if it sees an invalid byte, it will throw an error\n",
    "    - but there are several options, one of them is `replace`, which will replace the invalid byte with a special character (�) called the replacement character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then we just loop on the tokens sequence and index the vocab to get the corresponding string\n",
    "def decode(tokens_sequence):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in tokens_sequence)\n",
    "    text = tokens.decode('utf-8',errors='replace')\n",
    "    return text\n",
    "\n",
    "decode([128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_decode(token_sequence):\n",
    "    uncompressed_tokens = [] # mine: stores the utf-8 encodings -integer representation- \n",
    "    for token in token_sequence:\n",
    "        if token in token_to_pair:\n",
    "            uncompressed_tokens.extend(get_pair(token))\n",
    "        else:\n",
    "            uncompressed_tokens.append(token)\n",
    "\n",
    "    return bytes(uncompressed_tokens).decode('utf-8',errors='replace') # converts to bytes and then decode to utf-8\n",
    "\n",
    "my_decode([128]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so not every byte sequence is a valid utf-8 sequence, and if it happens that the large language model for example predicts the tokens in a bad manner, they might not fall into a valid utf-8, therefore we won't be able to decode them\n",
    "    - so the standard practice is to use `replace`, which we can also find in openAi code that they released\n",
    "    - so whenever we see that character in the output, we know that something went wrong and the LLM output is not valid sequence of tokens (valid utf-8)\n",
    "        - mine: notice that the output of the LLM is compressed utf-8 using byte pair encoding, so we decompress it to get the utf-8, then we decode it to get the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding\n",
    "\n",
    "- the other way around, given UTF-8 encodings,  encode it to get the token sequence (mine: compressed utf-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (105, 110): 257,\n",
       " (115, 32): 258,\n",
       " (116, 104): 259,\n",
       " (101, 114): 260,\n",
       " (99, 111): 261,\n",
       " (116, 32): 262,\n",
       " (226, 128): 263,\n",
       " (44, 32): 264,\n",
       " (97, 110): 265,\n",
       " (111, 114): 266,\n",
       " (100, 32): 267,\n",
       " (97, 114): 268,\n",
       " (101, 110): 269,\n",
       " (257, 103): 270,\n",
       " (261, 100): 271,\n",
       " (121, 32): 272,\n",
       " (46, 32): 273,\n",
       " (97, 108): 274,\n",
       " (259, 256): 275}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 108, 108, 111, 32, 119, 266, 108, 100, 33]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # first convert the text to utf-8 encodings in integer representation\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while True:\n",
    "        # use get stats to get all pairs \n",
    "        stats = get_stats(tokens)\n",
    "        # get the most eligible pair \n",
    "        pair = min(stats, key=lambda x: merges.get(x, float(\"inf\"))) # get the id from the merges dict if it exists, inf if they are not in merges, then get the minimum id (should be merged first)\n",
    "        # notice that in the end all of them will have inf -not mergable- and min will get one of them\n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "encode(\"hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we first conver the text to byte integers \n",
    "    - now some of them are merged, even multiple times\n",
    "    - so, we would like to do all these merges in the beginning before the later merges (because a later merge might depend on a previous merge)\n",
    "        - so we call get_stats to get all the pairs in the tokens (we don't care about their counts, we just care about the pairs themselves), this gets us the possible merge candidates\n",
    "        - now we want the pairs that exist in merges, and we want the minimum (so that we merge in order as we said)\n",
    "            - the way to do this is by calling min on the stats pairs -all possible pairs-, we use key to decide the values to compare by in order to get the minimum\n",
    "            - inside it we make a call to merges, if this pair in merges then get its id (notice that beginning pairs have ids smaller than the later pairs), otherwise if it is not in merges then assign it a value infinity\n",
    "\n",
    "- however we may pass a single character, so there are no pairs and the function will throw an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 8\u001b[0m, in \u001b[0;36mencode\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m stats \u001b[38;5;241m=\u001b[39m get_stats(tokens)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# get the most eligible pair \u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(stats, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: merges\u001b[38;5;241m.\u001b[39mget(x, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m))) \u001b[38;5;66;03m# get the id from the merges dict if it exists, inf if they are not in merges, then get the minimum id (should be merged first)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# notice that in the end all of them will have inf -not mergable- and min will get one of them\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pair \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m merges:\n",
      "\u001b[1;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "encode(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # first convert the text to the list of byte integers\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) >= 2:\n",
    "        # use get stats to get all pairs \n",
    "        stats = get_stats(tokens)\n",
    "        # get the most eligible pair \n",
    "        pair = min(stats, key=lambda x: merges.get(x, float(\"inf\")))\n",
    "        # notice that in the end all of them will have inf -not mergable- and min will get one of them\n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "encode(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making sure it works\n",
    "\n",
    "- if we take a string, encode it then decode it back, we expect to get the same string back\n",
    "    - I think in general this is the case\n",
    "\n",
    "- but notice that going backwards, not all token sequences are valid utf-8 byte streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "text2 == text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "valtext = \"Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters.\"\n",
    "valtext2 = decode(encode(valtext))\n",
    "print(valtext2 == valtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it also works with text it has not seen -been trained on-\n",
    "- this gives us some confidence that it is correctly implemented\n",
    "\n",
    "- that was the basics of the byte pair encoding algorithm\n",
    "    - we saw how we can take some training set and train the tokenizer on them (build the tokenizer from them)\n",
    "        - the parameters of the tokenizer really are just the dictionary of merges\n",
    "    - once we have it, we can encode and decode between text and token sequences\n",
    "        - mine so we have text -(utf-8 encode)> utf-8 -(encode using the vocabulary)-> token sequence that we feed into the LLM and get out of the LLM ,wich is compressed utf-8 -(decode using the vocabulary)-> utf-8 -(utf-8 decode)> text \n",
    "\n",
    "\n",
    "![byte pair encoding](assets/byte_pair_encoding.png)\n",
    "\n",
    "- mine: notice that the LLM can be evaluated on any text in the world (since we work on UTF-8 encodings which can encode any text in the world, and what BPE does is just compressing common sequences of utf-8 bytes in extra tokens)\n",
    "    - Byte Pair Encoding (BPE) is a practical middle ground between character and word level language modeling which effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences\n",
    "    - This input representation allows us to combine the empirical\n",
    "    benefits of word-level LMs with the generality of byte-level\n",
    "    approaches. Since our approach can assign a probability to\n",
    "    any Unicode string, this allows us to evaluate our LMs on\n",
    "    any dataset regardless of pre-processing, tokenization, or\n",
    "    vocab size\n",
    "\n",
    "- now we will look at some of the state-of-the-art LLMs and their tokenizers\n",
    "    - we will see that the picture complexifies very quickly\n",
    "    - and we will go through the details of this complexification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-of-the-art LLMs' tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (GPT series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forced Splits using Regex patterns \n",
    "\n",
    "- they didn't apply PBE naively as we did\n",
    "    - suppose we have common words like dog, it occurs frequently in the text right next to all kinds of punctuations as an example\n",
    "        - dog., dog!, dog?, and so on\n",
    "    - and naively applying BPE, we would have tokens for each of these\n",
    "        - so we would have dog. token and dog! token and dog? token and so on\n",
    "        - so we are wasting a lot of tokens on common words variations, not only that but we are combining semantics with punctuation -so we are clustering things that shouldn't be clustered- (like we are saying the same semantics but with different punctuations are different tokens)\n",
    "            - this is suboptimal\n",
    "        - so we want to top down in a manual way to force that some types of character categories should never be merged together\n",
    "            - so we want to enforce some merging rules on top of the BPE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', '123', ' how', ' are', ' you', '?', ' I', \"'m\", ' fine', '??!!!']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pattern, \"Hello world123 how are you? I'm fine??!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- they used the regex library\n",
    "    - re.compile is a function that takes a regex pattern and compiles it into a regex object\n",
    "    - then we can use this object to match the pattern in the text\n",
    "        - like in re.findall, we pass the regex object and the text, and it will return all the matches of the pattern in the text, organizes them in a list\n",
    "\n",
    "    - when we look at the pattern itself\n",
    "        - r\"\"\"\"\"\" is a raw string, which means that we don't want to interpret any of the backslashes in the string as escape characters\n",
    "        - it is made up of alot of ORs (the vertical bars `|`), which means that we are looking for any of these patterns\n",
    "        - the patterns themselves are\n",
    "            - 's\n",
    "                - this is a common contraction in english, like it is, it's\n",
    "            - 't\n",
    "                - another common contraction in english, like do not, don't\n",
    "            - 're\n",
    "                - another common contraction in english, like they are, they're\n",
    "            - 've\n",
    "                - another common contraction in english, like have, they've\n",
    "            - 'm\n",
    "                - another common contraction in english, like I am, I'm\n",
    "            - 'll\n",
    "                - another common contraction in english, like will, I'll\n",
    "            - 'd\n",
    "                - another common contraction in english, like would, I'd\n",
    "            - ` ?\\p{L}+`\n",
    "                - space+? means an optional space, then \\p{L} means any letter in any language, then + means one or more of these letters\n",
    "                - mine: so i think this matches any word in any language proceeded by a space or not -and if proceeded by a space it will be included in the match-\n",
    "            - ` ?\\p{N}+`\n",
    "                - space+? means an optional space, then \\p{N} means any number, then + means one or more of these numbers\n",
    "                - mine: so this extracts any number proceeded by a space or not -and if proceeded by a space it will be included in the match-\n",
    "            - ` ?[^\\s\\p{L}\\p{N}]+`\n",
    "                - this is saying optional space, followed by something that is not a space, not a letter, and not a number (one or more of it)\n",
    "                    - in more details [^...] is called the negated character class, it matches anything that is not in the class (not inside it)\n",
    "                    - so, basically this is trying to match any punctuation\n",
    "            - `\\s+(?!\\S)`\n",
    "                - this matches any whitespace that is not followed by a non-whitespace character\n",
    "                - in details, \\s+ matches one or more whitespace characters, then (?!\\S) is a negative lookahead -general form is (?!...) which says that the next character should not be whatever is inside it- and inside it is \\S which means any non-whitespace character (mine: \\S is with capital S, which means non-whitespace, unlike \\s which is whitespace)\n",
    "                - so this is basically matching any whitespace up to -but not including- the last whitespace character\n",
    "                - why is this important?\n",
    "                    - see how white spaces are always included at the beginning of each pattern (at the beginning of words, numbers, and punctuations)\n",
    "                    - so they did that so that the last space can join whatever is after it (say a word or a number)\n",
    "                - so any extra white space will be caught by this pattern, and we leave one white space to be included at the beginning of the next pattern\n",
    "                - so notice how gpt-2 likes to prepend a space to different patterns (they say that it significantly improves the compression efficiency)\n",
    "\n",
    "            - `\\s+`\n",
    "                - this is just matching one or more white spaces\n",
    "                - so this catches any trailing spaces or newline characters\n",
    "                - mine: this is in case we matches everything and there is a trailing space at the end\n",
    "\n",
    "            - mine: notice that the order of these patterns matter, we are doing an OR so the engine will try to match with any of the patterns from left to right and as soon as it finds a match it will stop and not consider the remaining alternative patterns\n",
    "                \n",
    "\n",
    "\n",
    "- some mistakes they made in the regex patterns\n",
    "    - they only included this apostrophe ', and not this one ’\n",
    "        - so they missed some contractions\n",
    "    - they made the apostrophe splits according to the english language, -so it is language specific-\n",
    "        - so if we have apostrophes in other languages, they will also be split \n",
    "    - they should have added re.IGNORECASE to the re.compile, so that it matches the patterns regardless of the case\n",
    "        - for example if we have a text I'm, it will match the pattern 'm, but if we have I'M, it won't match the pattern 'm\n",
    "\n",
    "- why we are doing this? we will take our string, and instead of directly encoding it for tokenization, we will first split it up using the above patterns, then we take the elements of the list and process them independently by the tokenizer, and then all of the results are concatenated\n",
    "    - so we are only considering merges for each element in the list individually (that way we can never merge multiple elements we specifically split above)\n",
    "    - then after we done all the possible merging, we concatenate the results\n",
    "    - that is how we ensure the splits that we want to enforce (we will never be considering merges for multiple elements in the list, so whatever we split by force will never be merged together)\n",
    "        - so basically using the regex patterns to chunk up the text is just one way of enforcing that some merges not to happen\n",
    "    - mine: so in the above regex, we ensure that words, numbers, punctuations, extra spaces, are all split up into different elements therefore won't be merged together by the tokenizer (the tokenizer will only consider merges for each one of them individually)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'for', ' i', ' in', ' range', '(', ' 1', ',', ' 101', ' ):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "for i in range( 1, 101 ):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pattern, example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mine: notice that \\n and for in the line after it were not in the same element. because we \\n is a type of whitespace, but not a space which we specifically mention before each pattern (like words, numbers and punctuations)\n",
    "    - so \\n will be caught by `\\s` but not ` ?`\n",
    "\n",
    "- so notice that we are splitting every time a category changes, therefore will never be any merges between different categories\n",
    "\n",
    "- we might think that in order to train the tokenizer, openAi has used this to split up text then run PBE withen the chunks, but that is not exactly what happened (notice how extra spaces are grouped into 1 element by the regex, but then they never actually were merged as we saw in the beggining of this notebook) mine: which indicates that the vocabulary doesn't have merges for extra spaces\n",
    "    - the training process of the tokenizer was never released, they just released the inference code, which takes the merges and applies them (not the code that trained the tokenizer)\n",
    "    - so whatever it was, it wasn't as simple as chunk it up then run BPE on the individual chunks to build the tokenizer (we don't know how they trained the tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiktoken\n",
    "\n",
    "- as we said, this is the official tokenizer for the GPT series\n",
    "\n",
    "- we can use it (use their trained tokenizers) to encode and decode text (do inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185, 220, 220]\n",
      "[262, 24748, 1917, 12340, 262]\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "# GPT-2 (doesn' merge spaces)\n",
    "print(enc.encode(\"    hello world!!!  \"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "print(enc.encode(\"    hello world!!!   \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so what changed in the GPT-4 tokenizer?\n",
    "    - they changed the regular expression that they use to chunk up text\n",
    "    - they changed the regex pattern from what we saw above to a new one\n",
    "        - that was the major change in addition to a bunch of other special tokens\n",
    "        - they solved the case sensitivity issue regarding the apostrophes (which we talked about)\n",
    "        - they handled white spaces in different ways\n",
    "        - when they matched the numbers, they didn't match it fully as above, but up to 3 digits (which means they will never merge numbers that are more than 3 digits)\n",
    "            - in other words, there will never be a token that represents a number that is more than 3 digits\n",
    "            - they did so to prevent tokens that are very long number sequences\n",
    "            - we don't really know why they do any of this stuff, as none of this is documented\n",
    "    - the vocabulary size went from 50k to 100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-2 encoder.py\n",
    "\n",
    "- let's go through the code of GPT-2 encoder.py (the code that encodes text into token sequences)\n",
    "- Reference the GPT-2 [encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "Download the vocab.bpe and encoder.json files.\n",
    "    - they are loading 2 files, encoder.json and vocab.bpe\n",
    "        - these 2 files together constitute their saved tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\" which stored the bytes representation of the tokens\n",
    "\n",
    "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "# ^---- ~equivalent to our \"merges\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so they are saving and loading the 2 variables vocab and merges\n",
    "- using them we can do both encoding and decoding\n",
    "\n",
    "- they have something called byte_encoder() and byte_decoder() -in addition to encoder() and decoder()\n",
    "    - this is unfortunately a spirous implementation detail, so we will skip it\n",
    "\n",
    "- long story short, it is a messy code that they have, but algorithmically identical to what we have built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special tokens\n",
    "\n",
    "- in addition to the tokens that are coming from raw bytes and BPE merges, we can insert all kinds of tokens that we are going to use to\n",
    "    - delimite different parts of the data\n",
    "    - introduce to create a special structure of the token streams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " '10': 940,\n",
       " '11': 1157,\n",
       " '12': 1065,\n",
       " '13': 1485,\n",
       " '14': 1415,\n",
       " '15': 1314,\n",
       " '16': 1433,\n",
       " '17': 1558,\n",
       " '18': 1507,\n",
       " '19': 1129,\n",
       " '20': 1238,\n",
       " '21': 2481,\n",
       " '22': 1828,\n",
       " '23': 1954,\n",
       " '24': 1731,\n",
       " '25': 1495,\n",
       " '26': 2075,\n",
       " '27': 1983,\n",
       " '28': 2078,\n",
       " '29': 1959,\n",
       " '30': 1270,\n",
       " '31': 3132,\n",
       " '32': 2624,\n",
       " '33': 2091,\n",
       " '34': 2682,\n",
       " '35': 2327,\n",
       " '36': 2623,\n",
       " '37': 2718,\n",
       " '38': 2548,\n",
       " '39': 2670,\n",
       " '40': 1821,\n",
       " '41': 3901,\n",
       " '42': 3682,\n",
       " '43': 3559,\n",
       " '44': 2598,\n",
       " '45': 2231,\n",
       " '46': 3510,\n",
       " '47': 2857,\n",
       " '48': 2780,\n",
       " '49': 2920,\n",
       " '50': 1120,\n",
       " '51': 4349,\n",
       " '52': 4309,\n",
       " '53': 4310,\n",
       " '54': 4051,\n",
       " '55': 2816,\n",
       " '56': 3980,\n",
       " '57': 3553,\n",
       " '58': 3365,\n",
       " '59': 3270,\n",
       " '60': 1899,\n",
       " '61': 5333,\n",
       " '62': 5237,\n",
       " '63': 5066,\n",
       " '64': 2414,\n",
       " '65': 2996,\n",
       " '66': 2791,\n",
       " '67': 3134,\n",
       " '68': 3104,\n",
       " '69': 3388,\n",
       " '70': 2154,\n",
       " '71': 4869,\n",
       " '72': 4761,\n",
       " '73': 4790,\n",
       " '74': 4524,\n",
       " '75': 2425,\n",
       " '76': 4304,\n",
       " '77': 3324,\n",
       " '78': 3695,\n",
       " '79': 3720,\n",
       " '80': 1795,\n",
       " '81': 6659,\n",
       " '82': 6469,\n",
       " '83': 5999,\n",
       " '84': 5705,\n",
       " '85': 5332,\n",
       " '86': 4521,\n",
       " '87': 5774,\n",
       " '88': 3459,\n",
       " '89': 4531,\n",
       " '90': 3829,\n",
       " '91': 6420,\n",
       " '92': 5892,\n",
       " '93': 6052,\n",
       " '94': 5824,\n",
       " '95': 3865,\n",
       " '96': 4846,\n",
       " '97': 5607,\n",
       " '98': 4089,\n",
       " '99': 2079,\n",
       " '100': 3064,\n",
       " '101': 8784,\n",
       " '102': 15377,\n",
       " '103': 15197,\n",
       " '104': 13464,\n",
       " '105': 13348,\n",
       " '106': 15801,\n",
       " '107': 15982,\n",
       " '108': 15711,\n",
       " '109': 14454,\n",
       " '110': 11442,\n",
       " '111': 16243,\n",
       " '112': 14686,\n",
       " '113': 16616,\n",
       " '114': 16562,\n",
       " '115': 15363,\n",
       " '116': 18298,\n",
       " '117': 17657,\n",
       " '118': 16817,\n",
       " '119': 16315,\n",
       " '120': 10232,\n",
       " '121': 19244,\n",
       " '122': 18376,\n",
       " '123': 10163,\n",
       " '124': 17464,\n",
       " '125': 11623,\n",
       " '126': 19420,\n",
       " '127': 16799,\n",
       " '128': 12762,\n",
       " '129': 18741,\n",
       " '130': 12952,\n",
       " '131': 22042,\n",
       " '132': 19924,\n",
       " '133': 16945,\n",
       " '134': 19880,\n",
       " '135': 17059,\n",
       " '136': 20809,\n",
       " '137': 19708,\n",
       " '138': 20107,\n",
       " '139': 20219,\n",
       " '140': 15187,\n",
       " '141': 23756,\n",
       " '142': 23726,\n",
       " '143': 21139,\n",
       " '144': 18444,\n",
       " '145': 18781,\n",
       " '146': 20964,\n",
       " '147': 20198,\n",
       " '148': 18294,\n",
       " '149': 19442,\n",
       " '150': 8628,\n",
       " '151': 24309,\n",
       " '152': 17827,\n",
       " '153': 21395,\n",
       " '154': 21526,\n",
       " '155': 18742,\n",
       " '156': 21599,\n",
       " '157': 18458,\n",
       " '158': 21273,\n",
       " '159': 19707,\n",
       " '160': 14198,\n",
       " '161': 25948,\n",
       " '162': 25061,\n",
       " '163': 24136,\n",
       " '164': 23237,\n",
       " '165': 20986,\n",
       " '166': 23055,\n",
       " '167': 21940,\n",
       " '168': 14656,\n",
       " '169': 22172,\n",
       " '170': 17279,\n",
       " '171': 27192,\n",
       " '172': 23628,\n",
       " '173': 25399,\n",
       " '174': 22985,\n",
       " '175': 17430,\n",
       " '176': 24096,\n",
       " '177': 22413,\n",
       " '178': 23188,\n",
       " '179': 21738,\n",
       " '180': 15259,\n",
       " '181': 27057,\n",
       " '182': 24294,\n",
       " '183': 24839,\n",
       " '184': 22883,\n",
       " '185': 21652,\n",
       " '186': 25096,\n",
       " '187': 23451,\n",
       " '188': 20356,\n",
       " '189': 23362,\n",
       " '190': 19782,\n",
       " '191': 26492,\n",
       " '192': 17477,\n",
       " '193': 24943,\n",
       " '194': 22913,\n",
       " '195': 22186,\n",
       " '196': 25272,\n",
       " '197': 24991,\n",
       " '198': 22337,\n",
       " '199': 19104,\n",
       " '200': 2167,\n",
       " '201': 1264,\n",
       " '202': 19004,\n",
       " '203': 22416,\n",
       " '204': 18638,\n",
       " '205': 21261,\n",
       " '206': 22136,\n",
       " '207': 22745,\n",
       " '208': 21315,\n",
       " '209': 22567,\n",
       " '210': 21536,\n",
       " '211': 21895,\n",
       " '212': 21777,\n",
       " '213': 26427,\n",
       " '214': 22291,\n",
       " '215': 23349,\n",
       " '216': 20666,\n",
       " '217': 24591,\n",
       " '218': 28727,\n",
       " '219': 28896,\n",
       " '220': 17572,\n",
       " '221': 26115,\n",
       " '222': 23148,\n",
       " '223': 22047,\n",
       " '224': 24137,\n",
       " '225': 18182,\n",
       " '226': 24909,\n",
       " '227': 24403,\n",
       " '228': 23815,\n",
       " '229': 23539,\n",
       " '230': 19214,\n",
       " '231': 25667,\n",
       " '232': 24339,\n",
       " '233': 25429,\n",
       " '234': 24409,\n",
       " '235': 22370,\n",
       " '236': 24940,\n",
       " '237': 24693,\n",
       " '238': 23721,\n",
       " '239': 23516,\n",
       " '240': 16102,\n",
       " '241': 28872,\n",
       " '242': 27877,\n",
       " '243': 26660,\n",
       " '244': 25707,\n",
       " '245': 22995,\n",
       " '246': 26912,\n",
       " '247': 23753,\n",
       " '248': 23045,\n",
       " '249': 21626,\n",
       " '250': 9031,\n",
       " '251': 28072,\n",
       " '252': 22800,\n",
       " '253': 28592,\n",
       " '254': 24970,\n",
       " '255': 13381,\n",
       " '256': 11645,\n",
       " '257': 28676,\n",
       " '258': 25600,\n",
       " '259': 25191,\n",
       " '260': 21719,\n",
       " '261': 30057,\n",
       " '262': 29119,\n",
       " '263': 29558,\n",
       " '264': 18897,\n",
       " '265': 22980,\n",
       " '266': 25540,\n",
       " '267': 25674,\n",
       " '268': 25022,\n",
       " '269': 26276,\n",
       " '270': 20233,\n",
       " '271': 28977,\n",
       " '272': 29807,\n",
       " '273': 27367,\n",
       " '274': 28857,\n",
       " '275': 23195,\n",
       " '276': 27988,\n",
       " '277': 27019,\n",
       " '278': 25870,\n",
       " '279': 26050,\n",
       " '280': 21033,\n",
       " '281': 30368,\n",
       " '282': 32568,\n",
       " '283': 30290,\n",
       " '284': 30336,\n",
       " '285': 26279,\n",
       " '286': 27033,\n",
       " '287': 27800,\n",
       " '288': 25270,\n",
       " '289': 27693,\n",
       " '290': 24369,\n",
       " '291': 33551,\n",
       " '292': 32759,\n",
       " '293': 31675,\n",
       " '294': 27696,\n",
       " '295': 25710,\n",
       " '296': 27137,\n",
       " '297': 26561,\n",
       " '298': 27728,\n",
       " '299': 22579,\n",
       " '300': 6200,\n",
       " '301': 18938,\n",
       " '302': 22709,\n",
       " '303': 22572,\n",
       " '304': 21288,\n",
       " '305': 22515,\n",
       " '306': 20548,\n",
       " '307': 22996,\n",
       " '308': 21495,\n",
       " '309': 26895,\n",
       " '310': 26717,\n",
       " '311': 36244,\n",
       " '312': 27970,\n",
       " '313': 25838,\n",
       " '314': 33638,\n",
       " '315': 27936,\n",
       " '316': 33400,\n",
       " '317': 34125,\n",
       " '318': 36042,\n",
       " '319': 35175,\n",
       " '320': 19504,\n",
       " '321': 36453,\n",
       " '322': 37283,\n",
       " '323': 32637,\n",
       " '324': 33916,\n",
       " '325': 26582,\n",
       " '326': 39195,\n",
       " '327': 34159,\n",
       " '328': 34256,\n",
       " '329': 37967,\n",
       " '330': 26073,\n",
       " '331': 31697,\n",
       " '332': 32148,\n",
       " '333': 20370,\n",
       " '334': 31380,\n",
       " '335': 27326,\n",
       " '336': 29211,\n",
       " '337': 31496,\n",
       " '338': 28460,\n",
       " '339': 29626,\n",
       " '340': 23601,\n",
       " '341': 33660,\n",
       " '342': 31575,\n",
       " '343': 32118,\n",
       " '344': 33535,\n",
       " '345': 27712,\n",
       " '346': 30557,\n",
       " '347': 30995,\n",
       " '348': 28978,\n",
       " '349': 27371,\n",
       " '350': 14877,\n",
       " '351': 35273,\n",
       " '352': 33394,\n",
       " '353': 33319,\n",
       " '354': 32182,\n",
       " '355': 28567,\n",
       " '356': 32066,\n",
       " '357': 27277,\n",
       " '358': 31128,\n",
       " '359': 30743,\n",
       " '360': 15277,\n",
       " '361': 35195,\n",
       " '362': 35667,\n",
       " '363': 35447,\n",
       " '364': 26780,\n",
       " '365': 24760,\n",
       " '366': 32459,\n",
       " '367': 27824,\n",
       " '368': 27412,\n",
       " '369': 30803,\n",
       " '370': 20167,\n",
       " '371': 38056,\n",
       " '372': 36720,\n",
       " '373': 34770,\n",
       " '374': 31020,\n",
       " '375': 22318,\n",
       " '376': 32128,\n",
       " '377': 26514,\n",
       " '378': 30695,\n",
       " '379': 29088,\n",
       " '380': 23734,\n",
       " '381': 36626,\n",
       " '382': 36243,\n",
       " '383': 34741,\n",
       " '384': 22842,\n",
       " '385': 27203,\n",
       " '386': 21734,\n",
       " '387': 32220,\n",
       " '388': 30460,\n",
       " '389': 29769,\n",
       " '390': 25964,\n",
       " '391': 37710,\n",
       " '392': 32321,\n",
       " '393': 26007,\n",
       " '394': 34626,\n",
       " '395': 31010,\n",
       " '396': 34107,\n",
       " '397': 33372,\n",
       " '398': 31952,\n",
       " '399': 28771,\n",
       " '400': 7029,\n",
       " '401': 21844,\n",
       " '402': 32531,\n",
       " '403': 31552,\n",
       " '404': 26429,\n",
       " '405': 26598,\n",
       " '406': 29703,\n",
       " '407': 30120,\n",
       " '408': 26200,\n",
       " '409': 29416,\n",
       " '410': 33289,\n",
       " '411': 42224,\n",
       " '412': 39226,\n",
       " '413': 44103,\n",
       " '414': 37309,\n",
       " '415': 35038,\n",
       " '416': 35218,\n",
       " '417': 38547,\n",
       " '418': 39667,\n",
       " '419': 45068,\n",
       " '420': 27211,\n",
       " '421': 46636,\n",
       " '422': 44361,\n",
       " '423': 43356,\n",
       " '424': 40090,\n",
       " '425': 32114,\n",
       " '426': 42780,\n",
       " '427': 42363,\n",
       " '428': 40173,\n",
       " '429': 11785,\n",
       " '430': 31794,\n",
       " '431': 50080,\n",
       " '432': 45331,\n",
       " '433': 42117,\n",
       " '434': 47101,\n",
       " '435': 40064,\n",
       " '436': 43690,\n",
       " '437': 43284,\n",
       " '438': 43704,\n",
       " '439': 47106,\n",
       " '440': 25644,\n",
       " '441': 39710,\n",
       " '442': 39506,\n",
       " '443': 34938,\n",
       " '444': 30272,\n",
       " '445': 43489,\n",
       " '446': 27260,\n",
       " '447': 34825,\n",
       " '448': 31115,\n",
       " '449': 31911,\n",
       " '450': 17885,\n",
       " '451': 36330,\n",
       " '452': 37730,\n",
       " '453': 36625,\n",
       " '454': 34229,\n",
       " '455': 30505,\n",
       " '456': 29228,\n",
       " '457': 33032,\n",
       " '458': 29334,\n",
       " '459': 33459,\n",
       " '460': 34716,\n",
       " '461': 40652,\n",
       " '462': 39997,\n",
       " '463': 38380,\n",
       " '464': 44578,\n",
       " '465': 42018,\n",
       " '466': 42199,\n",
       " '467': 24669,\n",
       " '468': 38472,\n",
       " '469': 42947,\n",
       " '470': 27790,\n",
       " '471': 38339,\n",
       " '472': 37856,\n",
       " '473': 37804,\n",
       " '474': 38652,\n",
       " '475': 32576,\n",
       " '476': 35435,\n",
       " '477': 32883,\n",
       " '478': 29059,\n",
       " '479': 31714,\n",
       " '480': 22148,\n",
       " '481': 40271,\n",
       " '482': 40149,\n",
       " '483': 38783,\n",
       " '484': 34137,\n",
       " '485': 32642,\n",
       " '486': 34251,\n",
       " '487': 35133,\n",
       " '488': 33646,\n",
       " '489': 35890,\n",
       " '490': 31503,\n",
       " '491': 41289,\n",
       " '492': 40256,\n",
       " '493': 43134,\n",
       " '494': 39449,\n",
       " '495': 33781,\n",
       " '496': 37747,\n",
       " '497': 38073,\n",
       " '498': 36260,\n",
       " '499': 28324,\n",
       " '500': 4059,\n",
       " '501': 33548,\n",
       " '502': 35126,\n",
       " '503': 31938,\n",
       " '504': 33580,\n",
       " '505': 31654,\n",
       " '506': 35638,\n",
       " '507': 35378,\n",
       " '508': 33042,\n",
       " '509': 29022,\n",
       " '510': 33690,\n",
       " '511': 41647,\n",
       " '512': 25836,\n",
       " '513': 48645,\n",
       " '514': 47396,\n",
       " '515': 45969,\n",
       " '516': 47493,\n",
       " '517': 48170,\n",
       " '518': 44085,\n",
       " '519': 47785,\n",
       " '520': 31211,\n",
       " '522': 49542,\n",
       " '523': 49803,\n",
       " '524': 48057,\n",
       " '525': 39088,\n",
       " '526': 48531,\n",
       " '528': 49351,\n",
       " '529': 49721,\n",
       " '530': 38612,\n",
       " '533': 44994,\n",
       " '535': 44465,\n",
       " '536': 44468,\n",
       " '537': 46096,\n",
       " '538': 49561,\n",
       " '540': 35005,\n",
       " '544': 47576,\n",
       " '545': 45326,\n",
       " '546': 49489,\n",
       " '548': 49934,\n",
       " '549': 44966,\n",
       " '550': 22730,\n",
       " '551': 43697,\n",
       " '552': 40427,\n",
       " '553': 48096,\n",
       " '554': 44218,\n",
       " '555': 31046,\n",
       " '556': 37864,\n",
       " '557': 41948,\n",
       " '558': 40486,\n",
       " '559': 38605,\n",
       " '560': 34135,\n",
       " '561': 47915,\n",
       " '562': 43918,\n",
       " '563': 46572,\n",
       " '565': 47372,\n",
       " '568': 49211,\n",
       " '570': 39254,\n",
       " '571': 42875,\n",
       " '572': 48724,\n",
       " '573': 48638,\n",
       " '574': 46900,\n",
       " '575': 36189,\n",
       " '576': 37452,\n",
       " '577': 49447,\n",
       " '578': 38907,\n",
       " '579': 41734,\n",
       " '580': 39322,\n",
       " '581': 48630,\n",
       " '582': 46044,\n",
       " '583': 46239,\n",
       " '584': 46352,\n",
       " '585': 38905,\n",
       " '586': 29796,\n",
       " '587': 44617,\n",
       " '588': 39118,\n",
       " '589': 44169,\n",
       " '590': 36993,\n",
       " '591': 48952,\n",
       " '592': 45839,\n",
       " '593': 49051,\n",
       " '594': 46438,\n",
       " '595': 35124,\n",
       " '596': 45734,\n",
       " '597': 43239,\n",
       " '598': 41292,\n",
       " '599': 43452,\n",
       " '600': 8054,\n",
       " '601': 41706,\n",
       " '602': 31418,\n",
       " '603': 35642,\n",
       " '604': 31916,\n",
       " '605': 32417,\n",
       " '606': 33206,\n",
       " '607': 31980,\n",
       " '608': 28688,\n",
       " '609': 31751,\n",
       " '610': 39132,\n",
       " '612': 43610,\n",
       " '613': 47512,\n",
       " '614': 46841,\n",
       " '615': 47007,\n",
       " '616': 44214,\n",
       " '617': 47941,\n",
       " '618': 47448,\n",
       " '620': 38850,\n",
       " '623': 46872,\n",
       " '625': 26704,\n",
       " '626': 45191,\n",
       " '627': 49856,\n",
       " '628': 48200,\n",
       " '629': 48602,\n",
       " '630': 30005,\n",
       " '635': 48250,\n",
       " '640': 31102,\n",
       " '641': 42759,\n",
       " '642': 41290,\n",
       " '643': 41813,\n",
       " '644': 29173,\n",
       " '645': 49259,\n",
       " '646': 27720,\n",
       " '647': 33981,\n",
       " '648': 34287,\n",
       " '649': 33300,\n",
       " '650': 17544,\n",
       " '651': 40639,\n",
       " '652': 43193,\n",
       " '653': 46435,\n",
       " '654': 39111,\n",
       " '655': 35916,\n",
       " '656': 37466,\n",
       " '657': 37680,\n",
       " '658': 38431,\n",
       " '659': 36445,\n",
       " '660': 39885,\n",
       " '661': 47159,\n",
       " '662': 39380,\n",
       " '663': 45791,\n",
       " '665': 36879,\n",
       " '666': 27310,\n",
       " '667': 28933,\n",
       " '668': 35809,\n",
       " '669': 36657,\n",
       " '670': 43798,\n",
       " '671': 46250,\n",
       " '672': 43864,\n",
       " '673': 45758,\n",
       " '674': 45385,\n",
       " '675': 42444,\n",
       " '676': 42548,\n",
       " '677': 40179,\n",
       " '678': 30924,\n",
       " '679': 37601,\n",
       " '680': 37397,\n",
       " '681': 48564,\n",
       " '682': 43950,\n",
       " '683': 47521,\n",
       " '684': 41580,\n",
       " '685': 35978,\n",
       " '686': 33808,\n",
       " '687': 39925,\n",
       " '688': 34427,\n",
       " '689': 40523,\n",
       " '690': 35844,\n",
       " '691': 49541,\n",
       " '692': 46589,\n",
       " '693': 48528,\n",
       " '694': 45214,\n",
       " '695': 37381,\n",
       " '696': 38205,\n",
       " '697': 40035,\n",
       " '698': 39357,\n",
       " '699': 47325,\n",
       " '700': 9879,\n",
       " '701': 41583,\n",
       " '702': 36680,\n",
       " '703': 36809,\n",
       " '704': 32869,\n",
       " '705': 34801,\n",
       " '706': 35402,\n",
       " '707': 24038,\n",
       " '708': 32583,\n",
       " '709': 31495,\n",
       " '710': 43147,\n",
       " '712': 49517,\n",
       " '713': 50055,\n",
       " '714': 45722,\n",
       " '718': 45720,\n",
       " '720': 23906,\n",
       " '725': 45151,\n",
       " '727': 47760,\n",
       " '728': 48524,\n",
       " '729': 48555,\n",
       " '730': 43916,\n",
       " '733': 49995,\n",
       " '736': 49150,\n",
       " '740': 45598,\n",
       " '745': 50150,\n",
       " '747': 48882,\n",
       " '748': 48246,\n",
       " '750': 15426,\n",
       " '751': 48365,\n",
       " '752': 43665,\n",
       " '753': 44550,\n",
       " '754': 41874,\n",
       " '755': 38172,\n",
       " '756': 38219,\n",
       " '757': 39251,\n",
       " '758': 38569,\n",
       " '759': 38314,\n",
       " '760': 40761,\n",
       " '762': 48194,\n",
       " '763': 49641,\n",
       " '765': 29143,\n",
       " '767': 32059,\n",
       " '768': 30610,\n",
       " '770': 41820,\n",
       " '771': 46761,\n",
       " '772': 43571,\n",
       " '773': 46871,\n",
       " '774': 47582,\n",
       " '775': 34483,\n",
       " '776': 39509,\n",
       " '777': 29331,\n",
       " '778': 39761,\n",
       " '779': 40393,\n",
       " '780': 40873,\n",
       " '781': 49703,\n",
       " '782': 46519,\n",
       " '783': 50165,\n",
       " '784': 37688,\n",
       " '785': 41172,\n",
       " '786': 46302,\n",
       " '787': 41019,\n",
       " '789': 40401,\n",
       " '790': 37750,\n",
       " '792': 48156,\n",
       " '793': 44750,\n",
       " '794': 50242,\n",
       " '795': 41544,\n",
       " '796': 41060,\n",
       " '797': 44673,\n",
       " '798': 43240,\n",
       " '799': 45455,\n",
       " '800': 7410,\n",
       " '801': 41531,\n",
       " '802': 30863,\n",
       " '803': 43564,\n",
       " '804': 36088,\n",
       " '805': 28256,\n",
       " '806': 37988,\n",
       " '807': 36928,\n",
       " '808': 28362,\n",
       " '809': 34583,\n",
       " '810': 40215,\n",
       " '815': 49503,\n",
       " '820': 41739,\n",
       " '825': 47338,\n",
       " '830': 48341,\n",
       " '833': 48634,\n",
       " '840': 40675,\n",
       " '850': 25764,\n",
       " '855': 45432,\n",
       " '860': 45039,\n",
       " '864': 39570,\n",
       " '866': 42240,\n",
       " '870': 46951,\n",
       " '875': 31360,\n",
       " '877': 42802,\n",
       " '880': 41655,\n",
       " '882': 42980,\n",
       " '883': 49287,\n",
       " '884': 40353,\n",
       " '885': 44230,\n",
       " '886': 44980,\n",
       " '887': 46660,\n",
       " '888': 28011,\n",
       " '889': 39121,\n",
       " '893': 49682,\n",
       " '896': 48712,\n",
       " '899': 44093,\n",
       " '900': 12865,\n",
       " '901': 46815,\n",
       " '905': 44928,\n",
       " '909': 44675,\n",
       " '910': 43234,\n",
       " '911': 35549,\n",
       " '915': 40248,\n",
       " '916': 48894,\n",
       " '920': 37128,\n",
       " '925': 46351,\n",
       " '930': 45418,\n",
       " '940': 46899,\n",
       " '949': 48581,\n",
       " '950': 31027,\n",
       " '951': 50119,\n",
       " '952': 49234,\n",
       " '953': 49649,\n",
       " '954': 48372,\n",
       " '956': 50148,\n",
       " '960': 39277,\n",
       " '968': 38956,\n",
       " '969': 38819,\n",
       " '970': 43587,\n",
       " '975': 42716,\n",
       " '978': 32196,\n",
       " '980': 40022,\n",
       " '985': 42250,\n",
       " '986': 49087,\n",
       " '987': 44183,\n",
       " '989': 42520,\n",
       " '990': 34155,\n",
       " '992': 41561,\n",
       " '993': 44821,\n",
       " '994': 42691,\n",
       " '995': 33438,\n",
       " '996': 38565,\n",
       " '997': 39647,\n",
       " '998': 34808,\n",
       " '999': 17032,\n",
       " '1000': 12825,\n",
       " '1001': 47705,\n",
       " '1007': 44318,\n",
       " '1016': 27956,\n",
       " '1024': 35500,\n",
       " '1027': 40403,\n",
       " '1080': 24045,\n",
       " '1100': 42060,\n",
       " '1111': 26259,\n",
       " '1200': 27550,\n",
       " '1500': 33698,\n",
       " '1600': 36150,\n",
       " '1800': 39188,\n",
       " '1900': 48104,\n",
       " '1920': 40454,\n",
       " '1945': 41931,\n",
       " '1950': 42751,\n",
       " '1959': 45403,\n",
       " '1960': 38503,\n",
       " '1963': 45192,\n",
       " '1964': 46477,\n",
       " '1965': 45271,\n",
       " '1966': 44227,\n",
       " '1967': 42830,\n",
       " '1968': 42246,\n",
       " '1969': 38391,\n",
       " '1970': 30986,\n",
       " '1971': 41208,\n",
       " '1972': 41023,\n",
       " '1973': 40220,\n",
       " '1974': 40828,\n",
       " '1975': 38449,\n",
       " '1976': 38108,\n",
       " '1977': 37781,\n",
       " '1978': 37950,\n",
       " '1979': 33581,\n",
       " '1980': 23664,\n",
       " '1981': 35411,\n",
       " '1982': 30763,\n",
       " '1983': 29279,\n",
       " '1984': 28296,\n",
       " '1985': 29110,\n",
       " '1986': 28054,\n",
       " '1987': 27301,\n",
       " '1988': 26709,\n",
       " '1989': 25475,\n",
       " '1990': 19891,\n",
       " '1991': 24529,\n",
       " '1992': 23847,\n",
       " '1993': 24465,\n",
       " '1994': 22666,\n",
       " '1995': 21908,\n",
       " '1996': 22288,\n",
       " '1997': 21498,\n",
       " '1998': 21113,\n",
       " '1999': 18946,\n",
       " '2000': 11024,\n",
       " '2001': 14585,\n",
       " '2002': 16942,\n",
       " '2003': 16088,\n",
       " '2004': 15724,\n",
       " '2005': 14315,\n",
       " '2006': 13330,\n",
       " '2007': 12726,\n",
       " '2008': 11528,\n",
       " '2009': 10531,\n",
       " '2010': 10333,\n",
       " '2011': 9804,\n",
       " '2012': 6999,\n",
       " '2013': 6390,\n",
       " '2014': 4967,\n",
       " '2015': 4626,\n",
       " '2016': 5304,\n",
       " '2017': 5539,\n",
       " '2018': 7908,\n",
       " '2019': 23344,\n",
       " '2020': 42334,\n",
       " '2200': 34294,\n",
       " '2500': 44688,\n",
       " '3000': 23924,\n",
       " '3333': 24840,\n",
       " '4000': 27559,\n",
       " '5000': 27641,\n",
       " '6000': 43434,\n",
       " '6666': 19060,\n",
       " '7601': 42752,\n",
       " '8000': 33942,\n",
       " '9999': 24214,\n",
       " '10000': 49388,\n",
       " '20439': 47936,\n",
       " '70710': 42877,\n",
       " '76561': 48527,\n",
       " '200000': 33470,\n",
       " '66666666': 41977,\n",
       " '!': 0,\n",
       " '\"': 1,\n",
       " '#': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '<': 27,\n",
       " '=': 28,\n",
       " '>': 29,\n",
       " '?': 30,\n",
       " '@': 31,\n",
       " 'A': 32,\n",
       " 'B': 33,\n",
       " 'C': 34,\n",
       " 'D': 35,\n",
       " 'E': 36,\n",
       " 'F': 37,\n",
       " 'G': 38,\n",
       " 'H': 39,\n",
       " 'I': 40,\n",
       " 'J': 41,\n",
       " 'K': 42,\n",
       " 'L': 43,\n",
       " 'M': 44,\n",
       " 'N': 45,\n",
       " 'O': 46,\n",
       " 'P': 47,\n",
       " 'Q': 48,\n",
       " 'R': 49,\n",
       " 'S': 50,\n",
       " 'T': 51,\n",
       " 'U': 52,\n",
       " 'V': 53,\n",
       " 'W': 54,\n",
       " 'X': 55,\n",
       " 'Y': 56,\n",
       " 'Z': 57,\n",
       " '[': 58,\n",
       " '\\\\': 59,\n",
       " ']': 60,\n",
       " '^': 61,\n",
       " '_': 62,\n",
       " '`': 63,\n",
       " 'a': 64,\n",
       " 'b': 65,\n",
       " 'c': 66,\n",
       " 'd': 67,\n",
       " 'e': 68,\n",
       " 'f': 69,\n",
       " 'g': 70,\n",
       " 'h': 71,\n",
       " 'i': 72,\n",
       " 'j': 73,\n",
       " 'k': 74,\n",
       " 'l': 75,\n",
       " 'm': 76,\n",
       " 'n': 77,\n",
       " 'o': 78,\n",
       " 'p': 79,\n",
       " 'q': 80,\n",
       " 'r': 81,\n",
       " 's': 82,\n",
       " 't': 83,\n",
       " 'u': 84,\n",
       " 'v': 85,\n",
       " 'w': 86,\n",
       " 'x': 87,\n",
       " 'y': 88,\n",
       " 'z': 89,\n",
       " '{': 90,\n",
       " '|': 91,\n",
       " '}': 92,\n",
       " '~': 93,\n",
       " '¡': 94,\n",
       " '¢': 95,\n",
       " '£': 96,\n",
       " ...}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder) # 256 raw byte tokens + 50k merged tokens (they did 50k merges) + 1 special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['<|endoftext|>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- that is a special token (the last token), and it is used to delimite documents in the training set (to denote that the document has ended)\n",
    "    - so when we are creating the training data (mine: the LLM training set), we have all these documents, and we tokenize them and get a stream of tokens\n",
    "        - those tokens only range from 0 to 50256 \n",
    "    - then in between those documents, we put a special `<|endoftext|>` token \n",
    "        - we use this as a singnal to the language model, that the document has ended, and that what follows is unrelated to the document\n",
    "        - and the language model has to learn that from data, it needs to learn that this token usually means that it should wipe its memory of what came before (that what came before is not informative of what comes next)\n",
    "\n",
    "    - so, this token didn't actually go through the PBE algorithm\n",
    "        - we have special case handling for these special tokens, like registering them, add to the vocabulary, encode them (we look for them specifically in the text and replace them with their token id), \n",
    "\n",
    "- these special tokens are used pervasively, not just in base language models, but especially later in the fine tuning stage, we don't just want to delimite documents, but we want to delimite entire conversations between an assistant and a user \n",
    "\n",
    "- the default example in the tiktoken page is using not base model encoders, but fine-tuned model tokenizers\n",
    "\n",
    "![tok example 4](assets/tok_example_4.png)\n",
    "\n",
    "- notice the special tokens\n",
    "    - i am start \"imaginary monologue start\"\n",
    "    - i am end\n",
    "    - etc.\n",
    "\n",
    "- so we have a start and end of every single message, and alot of other special tokens to delimit these conversations and keep track of the flow of the messages here\n",
    "\n",
    "- we can actually extend tiktoken and create additional special tokens with new ids, and the library will swap them out when it sees them in the strings\n",
    "\n",
    "- in GPT-2 tokenizer, we have end of text token, and in GPT-4 tokenizer, we have also end of text and \n",
    "    - FIM_prefix\n",
    "    - FIM_middle\n",
    "    - FIM_suffix\n",
    "        - to learn more about them check the paper [efficient training of large language models to fill in the middle](https://arxiv.org/pdf/2207.14255) \n",
    "    - end of prompt\n",
    "\n",
    "\n",
    "- so, it is very common to train a language model, and add special tokens\n",
    "    - when we add special tokens, we have to do some model surgery to the transformer and all the parameters involved, as we are basically adding an integer\n",
    "        - so we want to make sure that the embedding matrix for the vocabulary has to be extended for them (these additional rows will be initialized with small random numbers or something like that)\n",
    "        - we also have to go to the final layer of the transformer and make sure that the final layer which projects to vocab_size is extended as well\n",
    "    - this is a very common operation that people do, especially if they like to fine-tune the model (taking it from a base model to a chat model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minbpe exercise\n",
    "\n",
    "\n",
    "At this point you have everything you need to build your own GPT-4 tokenizer. This is the [exercise progression](https://github.com/karpathy/minbpe/blob/master/exercise.md) you may wish to follow. You'll note that it is part of the [minbpe](https://github.com/karpathy/minbpe) repo, which is the solution to that exercise, and is a cleaned up version of the code above.\n",
    "\n",
    "\n",
    "- mine: i implemented both the basic tokenizer (which is implemented here) and Regex tokenizer \n",
    "\n",
    "- for the regex tokenizer\n",
    "    - we use the regex specified to split the text into chunks of different categories (words, numbers, punctuations, extra spaces)\n",
    "    - then we train the tokenizer on them\n",
    "        - we get the common pair in all the text chunks (but since we separated them we have no pairs of an element end with another element beggining)\n",
    "        - so we have no pairs that span multiple categories\n",
    "        - then we get the top pair in all text chunks \n",
    "        - then we merge that pair in all text chunks\n",
    "    - then we encode \n",
    "        - we split the text into chunks using the regex of different categories\n",
    "        - then we call encode on each chunk individually and concatenate the result tokens\n",
    "    - in decode \n",
    "        - fairly the same as the basic token, nothing changes\n",
    "\n",
    "- as for handling the special tokens\n",
    "    - we do that manually \n",
    "    - we first register the tokens using a function called register_special_tokens\n",
    "        - now they will be stored separately in a dictionary with special tokens as keys and their ids as values\n",
    "    - in the encode\n",
    "        - we also encode them manually by specifying the special tokens as regex patterns\n",
    "        - then separating the text whenever we encounter a special token \n",
    "        - now we have multiple text elements, some are special tokens and some are ordinary text\n",
    "        - for each text element if it is a special token we encode it using the special tokens dictionary, otherwise -normal text- we encode it using the ordinary encoder (which further splits the normal text into chunks of different categories then encodes them)\n",
    "    - in the decode\n",
    "        - for each token, we will either find it in the inverse of the special tokens dictionary (if it is a special token) or we will find it in the vocabulary (if it is a normal token)\n",
    "\n",
    "- so, we can say that we added 2 layers over the tokenizer we implemented in here\n",
    "    - the first layer concerns the normal text, as we use a regex to split different categories of text, then we train, encode and decode on these separate elements\n",
    "        - that way we will have no pair spanning multiple elements therefore no tokens spanning multiple categories\n",
    "    - the second layer is a special token handler, where we manually register special tokens in a separate dictionary, and we look for these special tokens in the text and manually replacing them with their ids before encoding the rest of the text, and in the decode we also check if the token exists in that separate dictionary and replace it with the special token\n",
    "    - there are even a lot of options in the encoder function (whether to allow all special tokens or some of them of none of them at all)\n",
    "    - so i think that we can train the text normally, then add the special tokens later for the encoding and decoding depending on our application, I still need to know whether we add the special tokens in the tokenizer training set or not (but so far it seems that we add them later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[198,\n",
       " 2000,\n",
       " 602,\n",
       " 304,\n",
       " 2134,\n",
       " 7,\n",
       " 220,\n",
       " 16,\n",
       " 11,\n",
       " 220,\n",
       " 4645,\n",
       " 21711,\n",
       " 262,\n",
       " 422,\n",
       " 602,\n",
       " 1034,\n",
       " 220,\n",
       " 18,\n",
       " 624,\n",
       " 220,\n",
       " 15,\n",
       " 323,\n",
       " 602,\n",
       " 1034,\n",
       " 220,\n",
       " 20,\n",
       " 624,\n",
       " 220,\n",
       " 15,\n",
       " 512,\n",
       " 286,\n",
       " 1194,\n",
       " 446,\n",
       " 99615,\n",
       " 60573,\n",
       " 1158,\n",
       " 262,\n",
       " 4508,\n",
       " 602,\n",
       " 1034,\n",
       " 220,\n",
       " 18,\n",
       " 624,\n",
       " 220,\n",
       " 15,\n",
       " 512,\n",
       " 286,\n",
       " 1194,\n",
       " 446,\n",
       " 99615,\n",
       " 1158,\n",
       " 262,\n",
       " 4508,\n",
       " 602,\n",
       " 1034,\n",
       " 220,\n",
       " 20,\n",
       " 624,\n",
       " 220,\n",
       " 15,\n",
       " 512,\n",
       " 286,\n",
       " 1194,\n",
       " 446,\n",
       " 60573,\n",
       " 1158,\n",
       " 262,\n",
       " 775,\n",
       " 512,\n",
       " 286,\n",
       " 1194,\n",
       " 1998,\n",
       " 340,\n",
       " 257,\n",
       " 100257]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "text = \"\"\"\n",
    "for i in range( 1, 101 ):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "    <|endoftext|>\"\"\"\n",
    "enc.encode(text, allowed_special=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 102,\n",
       " 111,\n",
       " 114,\n",
       " 258,\n",
       " 286,\n",
       " 290,\n",
       " 40,\n",
       " 32,\n",
       " 49,\n",
       " 44,\n",
       " 32,\n",
       " 292,\n",
       " 294,\n",
       " 257,\n",
       " 295,\n",
       " 258,\n",
       " 261,\n",
       " 32,\n",
       " 51,\n",
       " 263,\n",
       " 32,\n",
       " 48,\n",
       " 32,\n",
       " 276,\n",
       " 100,\n",
       " 258,\n",
       " 261,\n",
       " 32,\n",
       " 53,\n",
       " 263,\n",
       " 32,\n",
       " 48,\n",
       " 259,\n",
       " 264,\n",
       " 269,\n",
       " 272,\n",
       " 278,\n",
       " 280,\n",
       " 273,\n",
       " 257,\n",
       " 282,\n",
       " 258,\n",
       " 261,\n",
       " 32,\n",
       " 51,\n",
       " 263,\n",
       " 32,\n",
       " 48,\n",
       " 259,\n",
       " 264,\n",
       " 269,\n",
       " 272,\n",
       " 278,\n",
       " 273,\n",
       " 257,\n",
       " 282,\n",
       " 258,\n",
       " 261,\n",
       " 32,\n",
       " 53,\n",
       " 263,\n",
       " 32,\n",
       " 48,\n",
       " 259,\n",
       " 264,\n",
       " 269,\n",
       " 272,\n",
       " 280,\n",
       " 273,\n",
       " 257,\n",
       " 275,\n",
       " 115,\n",
       " 101,\n",
       " 259,\n",
       " 264,\n",
       " 269,\n",
       " 40,\n",
       " 105,\n",
       " 271,\n",
       " 260,\n",
       " 296]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minBPE import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.load('minBPE\\python_trained_models\\python_regex_tokenizer.model')\n",
    "tokenizer.encode(text, allowed_special=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: b'\\x00',\n",
       "  1: b'\\x01',\n",
       "  2: b'\\x02',\n",
       "  3: b'\\x03',\n",
       "  4: b'\\x04',\n",
       "  5: b'\\x05',\n",
       "  6: b'\\x06',\n",
       "  7: b'\\x07',\n",
       "  8: b'\\x08',\n",
       "  9: b'\\t',\n",
       "  10: b'\\n',\n",
       "  11: b'\\x0b',\n",
       "  12: b'\\x0c',\n",
       "  13: b'\\r',\n",
       "  14: b'\\x0e',\n",
       "  15: b'\\x0f',\n",
       "  16: b'\\x10',\n",
       "  17: b'\\x11',\n",
       "  18: b'\\x12',\n",
       "  19: b'\\x13',\n",
       "  20: b'\\x14',\n",
       "  21: b'\\x15',\n",
       "  22: b'\\x16',\n",
       "  23: b'\\x17',\n",
       "  24: b'\\x18',\n",
       "  25: b'\\x19',\n",
       "  26: b'\\x1a',\n",
       "  27: b'\\x1b',\n",
       "  28: b'\\x1c',\n",
       "  29: b'\\x1d',\n",
       "  30: b'\\x1e',\n",
       "  31: b'\\x1f',\n",
       "  32: b' ',\n",
       "  33: b'!',\n",
       "  34: b'\"',\n",
       "  35: b'#',\n",
       "  36: b'$',\n",
       "  37: b'%',\n",
       "  38: b'&',\n",
       "  39: b\"'\",\n",
       "  40: b'(',\n",
       "  41: b')',\n",
       "  42: b'*',\n",
       "  43: b'+',\n",
       "  44: b',',\n",
       "  45: b'-',\n",
       "  46: b'.',\n",
       "  47: b'/',\n",
       "  48: b'0',\n",
       "  49: b'1',\n",
       "  50: b'2',\n",
       "  51: b'3',\n",
       "  52: b'4',\n",
       "  53: b'5',\n",
       "  54: b'6',\n",
       "  55: b'7',\n",
       "  56: b'8',\n",
       "  57: b'9',\n",
       "  58: b':',\n",
       "  59: b';',\n",
       "  60: b'<',\n",
       "  61: b'=',\n",
       "  62: b'>',\n",
       "  63: b'?',\n",
       "  64: b'@',\n",
       "  65: b'A',\n",
       "  66: b'B',\n",
       "  67: b'C',\n",
       "  68: b'D',\n",
       "  69: b'E',\n",
       "  70: b'F',\n",
       "  71: b'G',\n",
       "  72: b'H',\n",
       "  73: b'I',\n",
       "  74: b'J',\n",
       "  75: b'K',\n",
       "  76: b'L',\n",
       "  77: b'M',\n",
       "  78: b'N',\n",
       "  79: b'O',\n",
       "  80: b'P',\n",
       "  81: b'Q',\n",
       "  82: b'R',\n",
       "  83: b'S',\n",
       "  84: b'T',\n",
       "  85: b'U',\n",
       "  86: b'V',\n",
       "  87: b'W',\n",
       "  88: b'X',\n",
       "  89: b'Y',\n",
       "  90: b'Z',\n",
       "  91: b'[',\n",
       "  92: b'\\\\',\n",
       "  93: b']',\n",
       "  94: b'^',\n",
       "  95: b'_',\n",
       "  96: b'`',\n",
       "  97: b'a',\n",
       "  98: b'b',\n",
       "  99: b'c',\n",
       "  100: b'd',\n",
       "  101: b'e',\n",
       "  102: b'f',\n",
       "  103: b'g',\n",
       "  104: b'h',\n",
       "  105: b'i',\n",
       "  106: b'j',\n",
       "  107: b'k',\n",
       "  108: b'l',\n",
       "  109: b'm',\n",
       "  110: b'n',\n",
       "  111: b'o',\n",
       "  112: b'p',\n",
       "  113: b'q',\n",
       "  114: b'r',\n",
       "  115: b's',\n",
       "  116: b't',\n",
       "  117: b'u',\n",
       "  118: b'v',\n",
       "  119: b'w',\n",
       "  120: b'x',\n",
       "  121: b'y',\n",
       "  122: b'z',\n",
       "  123: b'{',\n",
       "  124: b'|',\n",
       "  125: b'}',\n",
       "  126: b'~',\n",
       "  127: b'\\x7f',\n",
       "  128: b'\\x80',\n",
       "  129: b'\\x81',\n",
       "  130: b'\\x82',\n",
       "  131: b'\\x83',\n",
       "  132: b'\\x84',\n",
       "  133: b'\\x85',\n",
       "  134: b'\\x86',\n",
       "  135: b'\\x87',\n",
       "  136: b'\\x88',\n",
       "  137: b'\\x89',\n",
       "  138: b'\\x8a',\n",
       "  139: b'\\x8b',\n",
       "  140: b'\\x8c',\n",
       "  141: b'\\x8d',\n",
       "  142: b'\\x8e',\n",
       "  143: b'\\x8f',\n",
       "  144: b'\\x90',\n",
       "  145: b'\\x91',\n",
       "  146: b'\\x92',\n",
       "  147: b'\\x93',\n",
       "  148: b'\\x94',\n",
       "  149: b'\\x95',\n",
       "  150: b'\\x96',\n",
       "  151: b'\\x97',\n",
       "  152: b'\\x98',\n",
       "  153: b'\\x99',\n",
       "  154: b'\\x9a',\n",
       "  155: b'\\x9b',\n",
       "  156: b'\\x9c',\n",
       "  157: b'\\x9d',\n",
       "  158: b'\\x9e',\n",
       "  159: b'\\x9f',\n",
       "  160: b'\\xa0',\n",
       "  161: b'\\xa1',\n",
       "  162: b'\\xa2',\n",
       "  163: b'\\xa3',\n",
       "  164: b'\\xa4',\n",
       "  165: b'\\xa5',\n",
       "  166: b'\\xa6',\n",
       "  167: b'\\xa7',\n",
       "  168: b'\\xa8',\n",
       "  169: b'\\xa9',\n",
       "  170: b'\\xaa',\n",
       "  171: b'\\xab',\n",
       "  172: b'\\xac',\n",
       "  173: b'\\xad',\n",
       "  174: b'\\xae',\n",
       "  175: b'\\xaf',\n",
       "  176: b'\\xb0',\n",
       "  177: b'\\xb1',\n",
       "  178: b'\\xb2',\n",
       "  179: b'\\xb3',\n",
       "  180: b'\\xb4',\n",
       "  181: b'\\xb5',\n",
       "  182: b'\\xb6',\n",
       "  183: b'\\xb7',\n",
       "  184: b'\\xb8',\n",
       "  185: b'\\xb9',\n",
       "  186: b'\\xba',\n",
       "  187: b'\\xbb',\n",
       "  188: b'\\xbc',\n",
       "  189: b'\\xbd',\n",
       "  190: b'\\xbe',\n",
       "  191: b'\\xbf',\n",
       "  192: b'\\xc0',\n",
       "  193: b'\\xc1',\n",
       "  194: b'\\xc2',\n",
       "  195: b'\\xc3',\n",
       "  196: b'\\xc4',\n",
       "  197: b'\\xc5',\n",
       "  198: b'\\xc6',\n",
       "  199: b'\\xc7',\n",
       "  200: b'\\xc8',\n",
       "  201: b'\\xc9',\n",
       "  202: b'\\xca',\n",
       "  203: b'\\xcb',\n",
       "  204: b'\\xcc',\n",
       "  205: b'\\xcd',\n",
       "  206: b'\\xce',\n",
       "  207: b'\\xcf',\n",
       "  208: b'\\xd0',\n",
       "  209: b'\\xd1',\n",
       "  210: b'\\xd2',\n",
       "  211: b'\\xd3',\n",
       "  212: b'\\xd4',\n",
       "  213: b'\\xd5',\n",
       "  214: b'\\xd6',\n",
       "  215: b'\\xd7',\n",
       "  216: b'\\xd8',\n",
       "  217: b'\\xd9',\n",
       "  218: b'\\xda',\n",
       "  219: b'\\xdb',\n",
       "  220: b'\\xdc',\n",
       "  221: b'\\xdd',\n",
       "  222: b'\\xde',\n",
       "  223: b'\\xdf',\n",
       "  224: b'\\xe0',\n",
       "  225: b'\\xe1',\n",
       "  226: b'\\xe2',\n",
       "  227: b'\\xe3',\n",
       "  228: b'\\xe4',\n",
       "  229: b'\\xe5',\n",
       "  230: b'\\xe6',\n",
       "  231: b'\\xe7',\n",
       "  232: b'\\xe8',\n",
       "  233: b'\\xe9',\n",
       "  234: b'\\xea',\n",
       "  235: b'\\xeb',\n",
       "  236: b'\\xec',\n",
       "  237: b'\\xed',\n",
       "  238: b'\\xee',\n",
       "  239: b'\\xef',\n",
       "  240: b'\\xf0',\n",
       "  241: b'\\xf1',\n",
       "  242: b'\\xf2',\n",
       "  243: b'\\xf3',\n",
       "  244: b'\\xf4',\n",
       "  245: b'\\xf5',\n",
       "  246: b'\\xf6',\n",
       "  247: b'\\xf7',\n",
       "  248: b'\\xf8',\n",
       "  249: b'\\xf9',\n",
       "  250: b'\\xfa',\n",
       "  251: b'\\xfb',\n",
       "  252: b'\\xfc',\n",
       "  253: b'\\xfd',\n",
       "  254: b'\\xfe',\n",
       "  255: b'\\xff',\n",
       "  256: b'  ',\n",
       "  257: b'   ',\n",
       "  258: b' i',\n",
       "  259: b':\\n',\n",
       "  260: b'    ',\n",
       "  261: b' %',\n",
       "  262: b' =',\n",
       "  263: b' ==',\n",
       "  264: b'       ',\n",
       "  265: b' p',\n",
       "  266: b' pr',\n",
       "  267: b' pri',\n",
       "  268: b' prin',\n",
       "  269: b' print',\n",
       "  270: b'zz',\n",
       "  271: b')\\n',\n",
       "  272: b'(\"',\n",
       "  273: b'\")\\n',\n",
       "  274: b' e',\n",
       "  275: b' el',\n",
       "  276: b'an',\n",
       "  277: b'Fi',\n",
       "  278: b'Fizz',\n",
       "  279: b'Bu',\n",
       "  280: b'Buzz',\n",
       "  281: b' eli',\n",
       "  282: b' elif',\n",
       "  283: b' f',\n",
       "  284: b' fo',\n",
       "  285: b' for',\n",
       "  286: b' in',\n",
       "  287: b' r',\n",
       "  288: b' ran',\n",
       "  289: b' rang',\n",
       "  290: b' range',\n",
       "  291: b'10',\n",
       "  292: b'101',\n",
       "  293: b' )',\n",
       "  294: b' ):\\n',\n",
       "  295: b' if',\n",
       "  296: b'<|endoftext|>'},\n",
       " {'<|endoftext|>': 296})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab, tokenizer.special_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so now we do the same behavious as tiktoken, but in addition we provide a train function (they only provide encode/decode)\n",
    "\n",
    "- and the difference between our tokenizer and GPT-4 tokenizer is only a difference of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Piece Library intro\n",
    "\n",
    "- we are now going to move away from tiktoken and the way OpenAI tokenizes its strings\n",
    "- now we will discuss one more very commonly used library for working with LLMs tokenizers, which is `Sentence Piece`\n",
    "- `SentencePiece` is very commonly used in language models, because unlike tiktoken, it can do both training and inference (encode/decode), and it is quite efficient at both\n",
    "- it supports a number of algorithms for training vocabularies, one of them is the BPE algorithm we have been looking at\n",
    "- is is used both by LLama and Misteral series, and many other models\n",
    "- it is on [github](https://github.com/google/sentencepiece)\n",
    "\n",
    "- the big difference with sentence piece (and we look at example), is that they have a different order of operations\n",
    "    - in tiktoken, code points -> utf-8 encodings -apply BPE on utf-8 encodings-> token sequence\n",
    "    - in sentence piece, they work direcly on the the codepoints themselves!\n",
    "        - so code points -apply BPE on code points-> token sequence\n",
    "    - if we happen to run out of code points, like some rare code points that just don't come up often, and the rarity is determined by the `character_coverage` hyper parameter, then these code points will be mapped to `<unk>` token or if we have `byte_fallback` set to true, then they will be mapped to utf-8 encodings, then the individual bytes will be translated to tokens (special byte tokens that get added to the vocabulary)\n",
    "    - so it uses PBE on code points, and falls back to bytes for rare codepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a toy.txt file with some random text\n",
    "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "  f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what is crazy about sentencePiece is that they have a ton of options and configurations\n",
    "- the reason for that is because it has been around for a while, and it tries to handle a large diversity of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=\"toy.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=400,\n",
    "  # normalization\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we tried to setup sentence piece in a way that is very similar to LLama 2 tokenizer\n",
    "    - we took the tokenizer.model file that meta released and inspected all the options \n",
    "\n",
    "- normalization used to be very prevelent (normalization rules like lowercasing, removing accents, etc.), mine: standardization\n",
    "    - in LLMs we don't prefer to do any of it, because we want to not touch the data and keep them in the raw form as much as possible\n",
    "\n",
    "- sentencepiece was probably developed early in the days when there was an idea of training the tokenizer on a bunch of independent sentences, \n",
    "    - so it has hyperparameters like\n",
    "        - input sentence length (maximum number of training sentences)\n",
    "        - max sentence length (maximum bytes per sentence)\n",
    "        - shiffle input sentence\n",
    "    - so for it sentences are like the individual training examples\n",
    "    - but in the context of LLMs this is very weird and sperious, it is really hard to define what a sentence is, just treat the file as a giant steam of text \n",
    "\n",
    "- it has a lot of treatment about rare words (code points)\n",
    "\n",
    "- it has a lot of rules for splitting digits, whitespace, numbers (merge rules)\n",
    "    - equivalent to tiktoken using regex to split up categories\n",
    "\n",
    "- they also have special tokens\n",
    "    - they hardcode the \n",
    "        - unk token 0 (must exist)\n",
    "        - beginning of sentence token 1\n",
    "        - end of sentence token 2\n",
    "        - pad token 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['en', 259],\n",
       " ['▁t', 260],\n",
       " ['ce', 261],\n",
       " ['in', 262],\n",
       " ['ra', 263],\n",
       " ['▁a', 264],\n",
       " ['de', 265],\n",
       " ['er', 266],\n",
       " ['▁s', 267],\n",
       " ['ent', 268],\n",
       " ['or', 269],\n",
       " ['pr', 270],\n",
       " ['▁m', 271],\n",
       " ['▁u', 272],\n",
       " ['ing', 273],\n",
       " ['▁th', 274],\n",
       " ['ence', 275],\n",
       " ['entence', 276],\n",
       " ['Pi', 277],\n",
       " ['ed', 278],\n",
       " ['em', 279],\n",
       " ['ex', 280],\n",
       " ['is', 281],\n",
       " ['iz', 282],\n",
       " ['la', 283],\n",
       " ['on', 284],\n",
       " ['st', 285],\n",
       " ['▁S', 286],\n",
       " ['Pie', 287],\n",
       " ['end', 288],\n",
       " ['ext', 289],\n",
       " ['▁an', 290],\n",
       " ['▁pr', 291],\n",
       " ['▁to', 292],\n",
       " ['▁un', 293],\n",
       " ['▁the', 294],\n",
       " ['Piece', 295],\n",
       " ['▁Sentence', 296],\n",
       " ['▁SentencePiece', 297],\n",
       " ['.]', 298],\n",
       " ['Ne', 299],\n",
       " ['ag', 300],\n",
       " ['do', 301],\n",
       " ['ec', 302],\n",
       " ['gu', 303],\n",
       " ['ic', 304],\n",
       " ['ir', 305],\n",
       " ['it', 306],\n",
       " ['ly', 307],\n",
       " ['to', 308],\n",
       " ['▁(', 309],\n",
       " ['▁[', 310],\n",
       " ['▁f', 311],\n",
       " ['▁n', 312],\n",
       " ['▁w', 313],\n",
       " ['.])', 314],\n",
       " ['age', 315],\n",
       " ['del', 316],\n",
       " ['ion', 317],\n",
       " ['ken', 318],\n",
       " ['lan', 319],\n",
       " ['ral', 320],\n",
       " ['wor', 321],\n",
       " ['yst', 322],\n",
       " ['▁Ne', 323],\n",
       " ['▁al', 324],\n",
       " ['▁de', 325],\n",
       " ['▁is', 326],\n",
       " ['▁ma', 327],\n",
       " ['▁mo', 328],\n",
       " ['izer', 329],\n",
       " ['rain', 330],\n",
       " ['ural', 331],\n",
       " ['▁and', 332],\n",
       " ['▁lan', 333],\n",
       " ['▁pre', 334],\n",
       " ['guage', 335],\n",
       " ['ystem', 336],\n",
       " ['▁text', 337],\n",
       " ['▁model', 338],\n",
       " ['▁train', 339],\n",
       " ['kenizer', 340],\n",
       " ['▁system', 341],\n",
       " ['▁language', 342],\n",
       " ['▁training', 343],\n",
       " ['.,', 344],\n",
       " ['BP', 345],\n",
       " ['Ku', 346],\n",
       " ['ab', 347],\n",
       " ['as', 348],\n",
       " ['at', 349],\n",
       " ['by', 350],\n",
       " ['co', 351],\n",
       " ['es', 352],\n",
       " ['et', 353],\n",
       " ['if', 354],\n",
       " ['ig', 355],\n",
       " ['im', 356],\n",
       " ['ke', 357],\n",
       " ['lo', 358],\n",
       " ['nr', 359],\n",
       " ['oc', 360],\n",
       " ['e', 361],\n",
       " ['▁', 362],\n",
       " ['n', 363],\n",
       " ['t', 364],\n",
       " ['i', 365],\n",
       " ['r', 366],\n",
       " ['a', 367],\n",
       " ['o', 368],\n",
       " ['s', 369],\n",
       " ['d', 370],\n",
       " ['c', 371],\n",
       " ['l', 372],\n",
       " ['u', 373],\n",
       " ['g', 374],\n",
       " ['m', 375],\n",
       " ['p', 376],\n",
       " ['.', 377],\n",
       " ['h', 378],\n",
       " ['-', 379],\n",
       " ['w', 380],\n",
       " ['y', 381],\n",
       " ['P', 382],\n",
       " ['S', 383],\n",
       " ['b', 384],\n",
       " ['f', 385],\n",
       " ['k', 386],\n",
       " [')', 387],\n",
       " ['x', 388],\n",
       " ['z', 389],\n",
       " ['(', 390],\n",
       " ['N', 391],\n",
       " ['[', 392],\n",
       " [']', 393],\n",
       " ['v', 394],\n",
       " [',', 395],\n",
       " ['/', 396],\n",
       " ['B', 397],\n",
       " ['E', 398],\n",
       " ['K', 399]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we see the 256 byte tokens (because we turned on byte_fallback to true), then we see the merges (we see the parent token not the token pair that was merged to get it), then after the merges comes the individual tokens and their ids (individual code point tokens)\n",
    "    - so we have\n",
    "    - special tokens\n",
    "    - byte tokens\n",
    "    - merge tokens (of the code points it encountered in the training set)\n",
    "    - individual code point tokens (that it encountered in the training set)\n",
    "\n",
    "- the tokens that are extremely rare (determined by character_coverage, which is currently set to 0.99995), so if a code point occurred a single time out of like a million code points, it will be mapped to the unk token (won't be added to the individual code points)\n",
    "\n",
    "- mine: i think when we encounter a code point that is not in the merge tokens or individual code point tokens, it will be mapped to the unk token, or encoded to utf-8 then use the byte tokens to encode it (if we turn on byte_fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362, 378, 361, 372, 358, 362, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(\"hello 안녕하세요\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'h', 'e', 'l', 'lo', '▁', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>']\n"
     ]
    }
   ],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notice that the korean letters, since they were not part of the training set\n",
    "    - so sentencepiece is encountering code points it has not seen during training \n",
    "    - and so they don't have a token associated with them (mine: not in the merge tokens or individual code point tokens)\n",
    "    - so suddenly these are unk tokens, but because we set byte_fallback to true, the vocabulary contains tokens for each byte, and so these letters are encoded as bytes using utf-8, then encode the bytes\n",
    "\n",
    "- notice that we also have extra space before hello, where is this coming from?\n",
    "    - it came from add_dummy_prefix = True, which adds a dummy space in the beginning of the text in order to treat world in \"world\" and \"hello world\" as the same token\n",
    "    - so they want to unify them as a single token\n",
    "    - in tiktoken and openAI, a word might be a different token than  space+word, and the LLM has to learn from data that they are actually very similar concept (it has to learn that it is the same word but one at the beginning of the sentence and the other is at the middle of it)\n",
    "    \n",
    "    ![tiktoken space](assets/tiktoken_space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's remove byte_fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=\"toy.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=400,\n",
    "  # normalization\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995, # the percentage of characters covered by the model\n",
    "  byte_fallback=False,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['en', 3],\n",
       " ['▁t', 4],\n",
       " ['ce', 5],\n",
       " ['in', 6],\n",
       " ['ra', 7],\n",
       " ['▁a', 8],\n",
       " ['de', 9],\n",
       " ['er', 10],\n",
       " ['▁s', 11],\n",
       " ['ent', 12],\n",
       " ['or', 13],\n",
       " ['pr', 14],\n",
       " ['▁m', 15],\n",
       " ['▁u', 16],\n",
       " ['ing', 17],\n",
       " ['▁th', 18],\n",
       " ['ence', 19],\n",
       " ['entence', 20],\n",
       " ['Pi', 21],\n",
       " ['ed', 22],\n",
       " ['em', 23],\n",
       " ['ex', 24],\n",
       " ['is', 25],\n",
       " ['iz', 26],\n",
       " ['la', 27],\n",
       " ['on', 28],\n",
       " ['st', 29],\n",
       " ['▁S', 30],\n",
       " ['Pie', 31],\n",
       " ['end', 32],\n",
       " ['ext', 33],\n",
       " ['▁an', 34],\n",
       " ['▁pr', 35],\n",
       " ['▁to', 36],\n",
       " ['▁un', 37],\n",
       " ['▁the', 38],\n",
       " ['Piece', 39],\n",
       " ['▁Sentence', 40],\n",
       " ['▁SentencePiece', 41],\n",
       " ['.]', 42],\n",
       " ['Ne', 43],\n",
       " ['ag', 44],\n",
       " ['do', 45],\n",
       " ['ec', 46],\n",
       " ['gu', 47],\n",
       " ['ic', 48],\n",
       " ['ir', 49],\n",
       " ['it', 50],\n",
       " ['ly', 51],\n",
       " ['to', 52],\n",
       " ['▁(', 53],\n",
       " ['▁[', 54],\n",
       " ['▁f', 55],\n",
       " ['▁n', 56],\n",
       " ['▁w', 57],\n",
       " ['.])', 58],\n",
       " ['age', 59],\n",
       " ['del', 60],\n",
       " ['ion', 61],\n",
       " ['ken', 62],\n",
       " ['lan', 63],\n",
       " ['ral', 64],\n",
       " ['wor', 65],\n",
       " ['yst', 66],\n",
       " ['▁Ne', 67],\n",
       " ['▁al', 68],\n",
       " ['▁de', 69],\n",
       " ['▁is', 70],\n",
       " ['▁ma', 71],\n",
       " ['▁mo', 72],\n",
       " ['izer', 73],\n",
       " ['rain', 74],\n",
       " ['ural', 75],\n",
       " ['▁and', 76],\n",
       " ['▁lan', 77],\n",
       " ['▁pre', 78],\n",
       " ['guage', 79],\n",
       " ['ystem', 80],\n",
       " ['▁text', 81],\n",
       " ['▁model', 82],\n",
       " ['▁train', 83],\n",
       " ['kenizer', 84],\n",
       " ['▁system', 85],\n",
       " ['▁language', 86],\n",
       " ['▁training', 87],\n",
       " ['.,', 88],\n",
       " ['BP', 89],\n",
       " ['Ku', 90],\n",
       " ['ab', 91],\n",
       " ['as', 92],\n",
       " ['at', 93],\n",
       " ['by', 94],\n",
       " ['co', 95],\n",
       " ['es', 96],\n",
       " ['et', 97],\n",
       " ['if', 98],\n",
       " ['ig', 99],\n",
       " ['im', 100],\n",
       " ['ke', 101],\n",
       " ['lo', 102],\n",
       " ['nr', 103],\n",
       " ['oc', 104],\n",
       " ['of', 105],\n",
       " ['om', 106],\n",
       " ['ot', 107],\n",
       " ['pa', 108],\n",
       " ['pl', 109],\n",
       " ['po', 110],\n",
       " ['pu', 111],\n",
       " ['re', 112],\n",
       " ['ry', 113],\n",
       " ['sp', 114],\n",
       " ['ss', 115],\n",
       " ['su', 116],\n",
       " ['te', 117],\n",
       " ['ub', 118],\n",
       " ['ws', 119],\n",
       " ['▁d', 120],\n",
       " ['▁g', 121],\n",
       " ['▁v', 122],\n",
       " ['BPE', 123],\n",
       " ['Sen', 124],\n",
       " ['abu', 125],\n",
       " ['bas', 126],\n",
       " ['cod', 127],\n",
       " ['det', 128],\n",
       " ['ect', 129],\n",
       " ['ene', 130],\n",
       " ['ens', 131],\n",
       " ['ere', 132],\n",
       " ['erm', 133],\n",
       " ['erv', 134],\n",
       " ['ich', 135],\n",
       " ['ior', 136],\n",
       " ['ith', 137],\n",
       " ['its', 138],\n",
       " ['ize', 139],\n",
       " ['oce', 140],\n",
       " ['ram', 141],\n",
       " ['rat', 142],\n",
       " ['raw', 143],\n",
       " ['rom', 144],\n",
       " ['sup', 145],\n",
       " ['▁by', 146],\n",
       " ['▁do', 147],\n",
       " ['▁et', 148],\n",
       " ['▁im', 149],\n",
       " ['▁ne', 150],\n",
       " ['▁of', 151],\n",
       " ['▁on', 152],\n",
       " ['▁pu', 153],\n",
       " ['▁us', 154],\n",
       " ['▁wh', 155],\n",
       " ['Kudo', 156],\n",
       " ['ecif', 157],\n",
       " ['ents', 158],\n",
       " ['ined', 159],\n",
       " ['inly', 160],\n",
       " ['ised', 161],\n",
       " ['lary', 162],\n",
       " ['lows', 163],\n",
       " ['pair', 164],\n",
       " ['pend', 165],\n",
       " ['plem', 166],\n",
       " ['post', 167],\n",
       " ['rely', 168],\n",
       " ['twor', 169],\n",
       " ['word', 170],\n",
       " ['▁dir', 171],\n",
       " ['▁end', 172],\n",
       " ['▁ext', 173],\n",
       " ['▁for', 174],\n",
       " ['▁not', 175],\n",
       " ['▁raw', 176],\n",
       " ['▁sub', 177],\n",
       " ['▁voc', 178],\n",
       " ['Sennr', 179],\n",
       " ['based', 180],\n",
       " ['encod', 181],\n",
       " ['igram', 182],\n",
       " ['ocess', 183],\n",
       " ['twork', 184],\n",
       " ['▁byte', 185],\n",
       " ['▁deto', 186],\n",
       " ['▁does', 187],\n",
       " ['▁from', 188],\n",
       " ['▁gene', 189],\n",
       " ['▁make', 190],\n",
       " ['▁size', 191],\n",
       " ['▁that', 192],\n",
       " ['▁with', 193],\n",
       " ['determ', 194],\n",
       " ['ecific', 195],\n",
       " ['ension', 196],\n",
       " ['postpr', 197],\n",
       " ['ration', 198],\n",
       " ['superv', 199],\n",
       " ['▁prior', 200],\n",
       " ['▁units', 201],\n",
       " ['▁where', 202],\n",
       " ['abulary', 203],\n",
       " ['▁Neural', 204],\n",
       " ['▁allows', 205],\n",
       " ['▁depend', 206],\n",
       " ['▁direct', 207],\n",
       " ['▁implem', 208],\n",
       " ['▁mainly', 209],\n",
       " ['▁neural', 210],\n",
       " ['▁purely', 211],\n",
       " ['Sennrich', 212],\n",
       " ['encoding', 213],\n",
       " ['entences', 214],\n",
       " ['ocessing', 215],\n",
       " ['specific', 216],\n",
       " ['▁Network', 217],\n",
       " ['▁subword', 218],\n",
       " ['▁systems', 219],\n",
       " ['▁unigram', 220],\n",
       " ['▁unsuperv', 221],\n",
       " ['determined', 222],\n",
       " ['▁extension', 223],\n",
       " ['▁sentences', 224],\n",
       " ['▁tokenizer', 225],\n",
       " ['▁generation', 226],\n",
       " ['▁implements', 227],\n",
       " ['▁vocabulary', 228],\n",
       " ['▁detokenizer', 229],\n",
       " ['▁unsupervised', 230],\n",
       " ['postprocessing', 231],\n",
       " ['▁predetermined', 232],\n",
       " ['PE', 233],\n",
       " ['Se', 234],\n",
       " ['])', 235],\n",
       " ['ai', 236],\n",
       " ['ak', 237],\n",
       " ['al', 238],\n",
       " ['am', 239],\n",
       " ['an', 240],\n",
       " ['ar', 241],\n",
       " ['aw', 242],\n",
       " ['ba', 243],\n",
       " ['bu', 244],\n",
       " ['bw', 245],\n",
       " ['ca', 246],\n",
       " ['ch', 247],\n",
       " ['ci', 248],\n",
       " ['ct', 249],\n",
       " ['di', 250],\n",
       " ['eP', 251],\n",
       " ['el', 252],\n",
       " ['ep', 253],\n",
       " ['eu', 254],\n",
       " ['fi', 255],\n",
       " ['fo', 256],\n",
       " ['fr', 257],\n",
       " ['ge', 258],\n",
       " ['gr', 259],\n",
       " ['ha', 260],\n",
       " ['he', 261],\n",
       " ['ie', 262],\n",
       " ['io', 263],\n",
       " ['le', 264],\n",
       " ['ll', 265],\n",
       " ['ma', 266],\n",
       " ['me', 267],\n",
       " ['mi', 268],\n",
       " ['mo', 269],\n",
       " ['mp', 270],\n",
       " ['ms', 271],\n",
       " ['nc', 272],\n",
       " ['nd', 273],\n",
       " ['ne', 274],\n",
       " ['ng', 275],\n",
       " ['ni', 276],\n",
       " ['nl', 277],\n",
       " ['nn', 278],\n",
       " ['no', 279],\n",
       " ['ns', 280],\n",
       " ['nt', 281],\n",
       " ['od', 282],\n",
       " ['oe', 283],\n",
       " ['ok', 284],\n",
       " ['os', 285],\n",
       " ['ow', 286],\n",
       " ['pe', 287],\n",
       " ['rd', 288],\n",
       " ['ri', 289],\n",
       " ['rk', 290],\n",
       " ['rm', 291],\n",
       " ['ro', 292],\n",
       " ['rv', 293],\n",
       " ['se', 294],\n",
       " ['si', 295],\n",
       " ['sy', 296],\n",
       " ['th', 297],\n",
       " ['ti', 298],\n",
       " ['tp', 299],\n",
       " ['tr', 300],\n",
       " ['ts', 301],\n",
       " ['tw', 302],\n",
       " ['ua', 303],\n",
       " ['ud', 304],\n",
       " ['ul', 305],\n",
       " ['un', 306],\n",
       " ['up', 307],\n",
       " ['ur', 308],\n",
       " ['us', 309],\n",
       " ['vi', 310],\n",
       " ['vo', 311],\n",
       " ['wh', 312],\n",
       " ['wi', 313],\n",
       " ['wo', 314],\n",
       " ['xt', 315],\n",
       " ['ys', 316],\n",
       " ['yt', 317],\n",
       " ['ze', 318],\n",
       " ['▁N', 319],\n",
       " ['▁b', 320],\n",
       " ['▁e', 321],\n",
       " ['▁i', 322],\n",
       " ['▁l', 323],\n",
       " ['▁o', 324],\n",
       " ['▁p', 325],\n",
       " ['▁r', 326],\n",
       " ['Net', 327],\n",
       " ['Neu', 328],\n",
       " ['ain', 329],\n",
       " ['air', 330],\n",
       " ['byt', 331],\n",
       " ['cab', 332],\n",
       " ['ceP', 333],\n",
       " ['ces', 334],\n",
       " ['dep', 335],\n",
       " ['din', 336],\n",
       " ['dir', 337],\n",
       " ['doe', 338],\n",
       " ['ece', 339],\n",
       " ['eci', 340],\n",
       " ['ede', 341],\n",
       " ['ely', 342],\n",
       " ['ems', 343],\n",
       " ['enc', 344],\n",
       " ['eni', 345],\n",
       " ['enn', 346],\n",
       " ['era', 347],\n",
       " ['fic', 348],\n",
       " ['for', 349],\n",
       " ['gen', 350],\n",
       " ['gra', 351],\n",
       " ['her', 352],\n",
       " ['imp', 353],\n",
       " ['ine', 354],\n",
       " ['ini', 355],\n",
       " ['inl', 356],\n",
       " ['lar', 357],\n",
       " ['lem', 358],\n",
       " ['low', 359],\n",
       " ['men', 360],\n",
       " ['e', 361],\n",
       " ['▁', 362],\n",
       " ['n', 363],\n",
       " ['t', 364],\n",
       " ['i', 365],\n",
       " ['r', 366],\n",
       " ['a', 367],\n",
       " ['o', 368],\n",
       " ['s', 369],\n",
       " ['d', 370],\n",
       " ['c', 371],\n",
       " ['l', 372],\n",
       " ['u', 373],\n",
       " ['g', 374],\n",
       " ['m', 375],\n",
       " ['p', 376],\n",
       " ['.', 377],\n",
       " ['h', 378],\n",
       " ['-', 379],\n",
       " ['w', 380],\n",
       " ['y', 381],\n",
       " ['P', 382],\n",
       " ['S', 383],\n",
       " ['b', 384],\n",
       " ['f', 385],\n",
       " ['k', 386],\n",
       " [')', 387],\n",
       " ['x', 388],\n",
       " ['z', 389],\n",
       " ['(', 390],\n",
       " ['N', 391],\n",
       " ['[', 392],\n",
       " [']', 393],\n",
       " ['v', 394],\n",
       " [',', 395],\n",
       " ['/', 396],\n",
       " ['B', 397],\n",
       " ['E', 398],\n",
       " ['K', 399]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all the byte tokens disappeared, and have a lot more merges now (since we have a lot more slots in the vocabulary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362, 378, 252, 102, 362, 0]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(\"hello 안녕하세요\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'h', 'el', 'lo', '▁', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the entire korean string is unk\n",
    "- so keep in mind that this would feed into the language model, and imagine what happens when all unrecognized characters just because they are rare will mapped to unk (disaster)\n",
    "- so we have to use byte_fallback because we want to be able to encode every string that we encounter, even if it is a rare string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is the raw protocol buffer representation of the tokenizer that LLama 2 trained\n",
    "    - if we want our tokenization to look like that of LLama 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Llama 2 tokenizer proto**\n",
    "If you'd like to export the raw protocol buffer for the `tokenizer.model` released by meta, this is a [helpful issue](https://github.com/google/sentencepiece/issues/121). And this is the result:\n",
    "\n",
    "```\n",
    "normalizer_spec {\n",
    "  name: \"identity\"\n",
    "  precompiled_charsmap: \"\"\n",
    "  add_dummy_prefix: true\n",
    "  remove_extra_whitespaces: false\n",
    "  normalization_rule_tsv: \"\"\n",
    "}\n",
    "\n",
    "trainer_spec {\n",
    "  input: \"/large_experiments/theorem/datasets/MERGED/all.test1.merged\"\n",
    "  model_prefix: \"spm_model_32k_200M_charcov099995_allowWSO__v2\"\n",
    "  model_type: BPE\n",
    "  vocab_size: 32000\n",
    "  self_test_sample_size: 0\n",
    "  input_format: \"text\"\n",
    "  character_coverage: 0.99995\n",
    "  input_sentence_size: 200000000\n",
    "  seed_sentencepiece_size: 1000000\n",
    "  shrinking_factor: 0.75\n",
    "  num_threads: 80\n",
    "  num_sub_iterations: 2\n",
    "  max_sentence_length: 4192\n",
    "  shuffle_input_sentence: true\n",
    "  max_sentencepiece_length: 16\n",
    "  split_by_unicode_script: true\n",
    "  split_by_whitespace: true\n",
    "  split_by_number: true\n",
    "  treat_whitespace_as_suffix: false\n",
    "  split_digits: true\n",
    "  allow_whitespace_only_pieces: true\n",
    "  vocabulary_output_piece_score: true\n",
    "  hard_vocab_limit: true\n",
    "  use_all_vocab: false\n",
    "  byte_fallback: true\n",
    "  required_chars: \"\"\n",
    "  unk_id: 0\n",
    "  bos_id: 1\n",
    "  eos_id: 2\n",
    "  pad_id: -1\n",
    "  unk_surface: \" \\342\\201\\207 \"\n",
    "  unk_piece: \"<unk>\"\n",
    "  bos_piece: \"<s>\"\n",
    "  eos_piece: \"</s>\"\n",
    "  pad_piece: \"<pad>\"\n",
    "  train_extremely_large_corpus: false\n",
    "  enable_differential_privacy: false\n",
    "  differential_privacy_noise_level: 0.0\n",
    "  differential_privacy_clipping_threshold: 0\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- our summary for sentence piece\n",
    "    - there is a lot of historical baggage in the library, and it has a lot of options and concepts that are slightly confusing \n",
    "    - otherwise it is fairly commonly used in the industry because it be both trained and used for inference\n",
    "    - it has few quirks (weird things you have to be aware of), like the byte fallback, and the normalization rules, and the way it treats sentences\n",
    "    - it is not very well documented\n",
    "        - so we have to visualize things and try to understand what is happening here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Aspects and Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to set vocabulary set? revisiting gpt.py transformer\n",
    "\n",
    "- how to set the vocabulary size? and what are some of the considerations around it?\n",
    "\n",
    "- for this we will go back to the architecture we implemented `gpt.py` in the last notebook\n",
    "    - in there, it was a character level, and only supporting the characters appearing in the tinyshakespeare file\n",
    "\n",
    "- first, what does vocab_size affect in the architecture?\n",
    "\n",
    "    ![vocab_size_in_our_gpt](assets/vocab_size_in_our_gpt.png)\n",
    "\n",
    "    1. the embedding table of the tokens (specifically the number of rows)\n",
    "        - so as the vocabulary size increases, this table will grow (we will be adding rows) \n",
    "    2. at the end of the transformer (the language model head), which is a linear layer that produces the logits (which becomes the probabiilities for the next tokens in the sequence)\n",
    "        - so every single token will produce an additional dot product in the linear layer \n",
    "\n",
    "- so, why can't vocab size be infinie? \n",
    "    1. your token embedding table will grow (more parameters)\n",
    "        - so we could be worried that we are going to be under-training some of them\n",
    "        - intuitevly, if we have extremely large vocabulary size (say million tokens), then everyone of them will come up more and more rarely in the training data (mine: as we are distinguishing them into more and more tokens)\n",
    "        - and so we will be seeing fewer and fewer examples of each token, and the vectors associated with them might be under-trained (haven't seen enough examples to learn a good representation)\n",
    "    2. your linear layer will grow (more parameters and computation)\n",
    "    3. we said that we will attend to more text (as the token spans more characters), which is a good thing, but if we will attend to too much text, the model wouldn't have much time to think (we are squishing too much information into a single token)\n",
    "        - and the forward pass will not be enough to process that information appropriately\n",
    "        - mine: imagine if an entire python script is a single token :D it will be too much for the model to process many of these tokens\n",
    "\n",
    "- so, the vocab_size is an empirical hyperparameter, and it seems like in the SOTA architectures today, this is usually in the high 10,000s or somewhere around the 100,000 today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training new tokens\n",
    "\n",
    "- what if we want to take a pre-trained model (mine: that has a certain vocabulary) and we want to extend the vocab size?\n",
    "    - this is very common, soemtimes when people fine-tune them, a lot more special tokens get introduced on top of the base model to maintain the metadata and the structure of the conversations and so on\n",
    "    - we might want to throw in more special tokens for example for using a browser, or any other tool\n",
    "\n",
    "- so, it is totally possile to add a token, all we have to do is to \n",
    "    - resize the embedding table (add more rows), initialize them from scratch\n",
    "    - extend the weight insie the language model head (the final linear layer that maps to vocab size)\n",
    "    - they can be done very easily (both are resizing operations)\n",
    "\n",
    "- so it is very common to freeze the base model, then introduce the new parameters for the new tokens, then only train them to introduce the new tokens to the architecture\n",
    "    - and so we can freeze arbitrary parts of the model, or train arbitrary parts of the model\n",
    "\n",
    "- actually, there is an entire design space (worth a whole video) of applications in terms of introducing new tokens into the vocabulary, that go way beyound adding special tokens for special new functionality\n",
    "    - one of them is a paper called [learning to compress promts with Gist tokens](https://arxiv.org/pdf/2304.08467)\n",
    "    - the rough idea is that. suppose that you are using language models in a setting that requires very long prompts \n",
    "        - they slow everything down because we have to encode them then attend over them then decode them \n",
    "    - so in the paper, they introduce new tokens, and then train the model by distilaltion (mine: meaning that they train the model to predict the new tokens, and then they freeze the model and train the new tokens to predict the old tokens)\n",
    "        - so  they are keeping the entire model frozen, and they are only training the representations of the new tokens (the embeddings)\n",
    "        - and we are optimizing on the new tokens such that the behavior of the language model is identical to the model that has a very long prompt\n",
    "        - so it is a compression technique of compressing that very long prompt into few new gist tokens\n",
    "        - and at test time, we can discard that old prompt and swap in the new tokens (which are much shorter), and the model will behave as if it has seen the long prompt\n",
    "        - the parameters that we are training are just the token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal (image, video, audio) tokeniztion with vector quantization\n",
    "\n",
    "- there is a lot of momentum recently in how we actually could construct transformers that simultaneously process not just text, but also images, videos, and audio (in how do we feed in these modalities into the transformer and potentially predict these modalities as well)\n",
    "    \n",
    "\n",
    "- do we have to change the architecture in some fundamental way? what a lot of people are starting to converge towards is that we are not changing the architecture, but we stick with the transformer, and just tokenize the input domains then call the day and pretend it's jsut text tokens and do everything else identical\n",
    "\n",
    "- for example there is an early paper [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/pdf/2012.09841), it has a nice graphic for how to take an image and chunkate it into integers (these will basically become the tokens of images)\n",
    "    - these tokens can be hard tokens (force them to be integers)\n",
    "    - or soft tokens, where they are required to be discrete, but force them to go through bottlenecks (like in autoencoders) \n",
    "\n",
    "- anther paper came out from openAI [SORA](https://openai.com/index/sora/), they talk breifly about how LLMs have text tokens, SORA has visual patches\n",
    "    - so they came up with a way to chuncate vidoes into tokens with their own vocabularies\n",
    "    - and then we can either process discrete tokens say with auto-regressive models, or even soft tokens with diffusion models\n",
    "    - all of that is being actively worked on and designed on, and it is beyond the scope of this video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting and explaining the quirks of LLM tokenization\n",
    "\n",
    "- now that we got into the tokenization algorithm and we understand now a lot about how it works, let's loop back around to the beginning of this notebook and go through some of these bullet points, and see why they happen\n",
    "\n",
    "- Why we need good tokenization? \n",
    "    - why can't LLM spell words? `Tokenization`\n",
    "        - because of a very long tokens (if a token contains too many characters, the LLM understands it as a single entity), \n",
    "\n",
    "        ![very long token](assets/very_long_token.png)\n",
    "        \n",
    "        - so there is too much crammed into the single token, and the model should not be very good at tasks related to spelling of this token\n",
    "        \n",
    "        ![spelling quirk](assets/spelling_quirk.png)\n",
    "        because this is what the model sees\n",
    "        \n",
    "        ![spelling quirk](assets/spelling_quirk_tokens.png)\n",
    "\n",
    "    - why can't LLM do super simple string processing tasks like reversing a string? `Tokenization`\n",
    "        - for the same reason\n",
    "        \n",
    "        ![reverse the string quirk](assets/reverse_string_quirk.png)\n",
    "\n",
    "        - and to make sure that the reason for the above 2 points are actually due to tokenization, we asked them to list them letter by letter separated by spaces -now they are in multiple tokens- and then asked them again\n",
    "        \n",
    "        ![long tokens separated out](assets/long_tokens_separated_out.png)\n",
    "\n",
    "        - when it lists them letter by letter, they separated into individual characters, and it is easier for it to process them\n",
    "    - why is LLM worse at non-english languages? `Tokenization`\n",
    "        - we talked about it above\n",
    "    - why is LLM bad at simple arithmetic? `Tokenization`\n",
    "        - that has to do with the tokenization of numbers, the algorithm for addition is character-level\n",
    "        \n",
    "        ![addition algorithm](assets/addition_algorithm.png)\n",
    "        \n",
    "        - we have to refer to the specific digits (we add the ones then the tens then the hundreds), but since these numbers are represented completely arbitrarily based on whatever happened in the PBE algorithm, the model unfortunately sometimes sees a token for all 4 digits, sometimes for 3 digits, sometimes for 2 or even 1, and it is all arbitrary\n",
    "            - this is definitely headwind for the model to deal with it\n",
    "        - there is an entire blog post about this [integer tokenization is insane](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)\n",
    "        - when meta trained the LLAama 2 algorithm using sentencepiece, they made sure to split up all the digits for llama 2 (to improve simple arithmetic)\n",
    "         \n",
    "    - why did GPT-2 have more than necessary trouble coding in python? `Tokenization`\n",
    "        - we talked about it above, handling spaces in python is terrible\n",
    "    - why did my LLM abrubtly halt when it sees the string `<endoftext>`? `Tokenization`\n",
    "        \n",
    "        - ![printing special tokens](assets/printing_special_token.png)\n",
    "        \n",
    "        - we don't know what they are doing under the hood, but they potentially parse this as an actual token instead of a string -individual pieces of it not a special token-\n",
    "            - so it might be that when they call .encode, they are passing in the allowed_special, and they are allowing these tokens in the user prompt\n",
    "            - but the user prompt is an attacker control text, so you would want not to parse these tokens in the user prompt\n",
    "        - howerver, there is something wrong going wrong on here \n",
    "    - what is this weird warning i get about a \"trailing whitespace\"? `Tokenization`\n",
    "        - if we used playgroun (which is something that chatGPT provide that is similar to base models) because the space characters are always a prefix to these tokens in GPT, but when we add a training space and encode, the space at the end becoems a token 220, this space was supposed to be a prefix to the next token, but now it is a token itself, and that gets us out of distribution (the model has seen very little data of actual spaces by itself, specially as a training whitespace)\n",
    "    \n",
    "    - back to the playground, which is something close to base models (document completer), we bet it has never seen .DefaultCellSty in its training set\n",
    "        \n",
    "        ![weird combination](assets/weird_combination.png)\n",
    "        \n",
    "        - the above combination is very very rare that the model has probably never seen in its training set, so what happens when we ask it to complete it?\n",
    "        \n",
    "        ![broken model](assets/broken_model.png)\n",
    "        \n",
    "        - the model immediately emitted the end of text token, and had no completion\n",
    "        - again we are off the data distribution, and the model is predicting arbitrary things (it is confused)\n",
    "\n",
    "    - so you can see these issues arise from us messing with its tokenization\n",
    "        - we are completing things from its next token that it should predict (like trailing whitespace)\n",
    "        - or removing some characters from long tokens (like .DefaultCellSty)\n",
    "        - all of these are issues related to partial tokens\n",
    "        - if we dig into the tiktoken repo, there is a lot of special case handling \"unstable tokens\" and it is not documented anywhere, but there is a ton of code dealing with unstable tokens\n",
    "        - \n",
    "    - why the LLM break if I ask it about \"SolidGoldMagikarp\"? `Tokenization`\n",
    "        - very famous [solid gold magikarp](https://www.alignmentforum.org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation) \n",
    "        - what this person did, he went to the token embeddings table and clustered the tokens based on their embedding representation\n",
    "        - he noticed that there is a cluster of tokens that look really starnge (like solidgoldmagikarp), so what are these tokens and where do they come from?\n",
    "            - they noiced that if they asked the model about these tokens, then we get a variety of broken LLM behavior\n",
    "        - this again comes down to tokenization, solidgoldmagikarp is a reddit user, and what is thought to have happened is that the tokenization dataset was very different from the LLM training dataset\n",
    "            - so potentially, the tokenization dataset had a ton of reddit data, where the user SolidGoldMagikarp was mentioned in the text alot, and because it occurred a lot in the tokenization dataset, these tokens would end up getting merged into solidgoldmagikarp\n",
    "                - so a dedicated token for solidgoldmagikarp in the 50k tokens in gpt2\n",
    "            - then later when we train the model, this data from reddit was not present, and so in the entire training set never occurrs, therefore its embedding is still initialized at random \n",
    "            - and so at test time if we evoke this token again, we are basically blucking our a row that is completely untrained, and feeds to the transformer to create undefined behavior\n",
    "    - why should I prefer to use YAML over JSON with LLMs? `Tokenization`\n",
    "        - people noticed that different kinds of lnguages or formats are more or less efficient in GPT\n",
    "        - just like how english is more efficient in GPT than other languages, some foramts like YAML are more efficient than JSON for the same reason (tokenizer training set)\n",
    "        \n",
    "        ![json_vs_yaml](assets/json_vs_yaml.png)\n",
    "        \n",
    "        - notice that the same information in both yaml and json, but they are encoded in few tokens in yaml (the token spans more characters), but in json they are encoded in more tokens (the token spans fewer characters)\n",
    "\n",
    "    - why is LLM not actually end-to-end Language modeling? `Tokenization`\n",
    "    - what is the real root of suffering? `Tokenization` :D\n",
    "\n",
    "\n",
    "- so tokenization density is something that we have to care about and worry about all the time, and try to find efficient coding schemes, and spend alot of time in tiktokenizer and measure the different tokens' efficiencies, for all different formats and settings and so on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final recommendations\n",
    "\n",
    "- Don't brush off tokenization. A lot of footguns and sharp edges here. Security issues. Safety issues.\n",
    "- Eternal glory to anyone who can delete tokenization as a required step in LLMs.\n",
    "- In your own application:\n",
    "  - if you can reuse the GPT-4 tokens and the vocabulary, then that is something you should consider doing (using tiktoken)\n",
    "  - but for some reason if you want to train your own tokenizer and vocabulary from scratch, then use PBE with sentencepiece\n",
    "    - even though some things are hateful like the bytefallback and all the settings, which are very easy to miscalibrate them, ending up cropping your sentences \n",
    "    - so be very careful with the settings and try to copy paste exactly what meta did, and spend alot of time going with the hyper parameters \n",
    "    - however, the algorithm itself and the fact they are using PBE on code points is inferior to what openAI does\n",
    "  - maybe the best thing is to wait for minPBE to become as efficient as sentencepiece (so that we can use something similar to tiktoken but with the ability to train the tokenizer)\n",
    "    - currently minPBE is in python\n",
    "\n",
    "\n",
    "## Also worth looking at\n",
    "\n",
    "- [Huggingface Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer). I didn't cover it in detail in the lecture because the algorithm (to my knowledge) is very similar to sentencepiece, but worth potentially evaluating for use in practice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
