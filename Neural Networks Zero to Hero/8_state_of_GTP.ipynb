{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## How to train GPT-Assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emerging recipes for training GPT\n",
    "\n",
    "- keep in mind that this is all very new and rapidly evolving\n",
    "\n",
    "![training gpt pipeline](assets/training_gpt_pipeline.png)\n",
    "\n",
    "- we have 4 major stages, that and they follow each other serially \n",
    "    - pretraining (1 stage)\n",
    "    - fine-tuning (3 stages)\n",
    "        - supervised fine-tuning (SFT)\n",
    "        - reward modeling\n",
    "        - reinforcement learning\n",
    "- in each stage we have a dataset, an algorithm, resulting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this stage is where all the computational work basically happens\n",
    "    - this is 99% of the training and compute time\n",
    "- this is where we deal with internet scale data, thousands of GPUs, and months of training\n",
    "- the resulting model of this stage is called the `\"base model\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Collection\n",
    "\n",
    "- we will gather a large amount of data\n",
    "- example of what is called a data mixture that comes from the paper released by Meta (rleased the LLama model)\n",
    "\n",
    "![image.png](assets/dataset_collection.png)\n",
    "\n",
    "- we have common crawl (web scraping)\n",
    "- C4 (also web scraping)\n",
    "- then some high quality datasets as well\n",
    "    - GitHub\n",
    "    - Wikipedia\n",
    "    - Books\n",
    "    - and so on\n",
    "\n",
    "- these are all mixed up together and sampled according to some proportions (that forms the data mixture that used for pretraining)\n",
    "\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "![image-2.png](assets/tokenization.png)\n",
    "\n",
    "\n",
    "#### Hyperparameter examples for pretraining stage \n",
    "![image-3.png](assets/pre_training_examples.png)\n",
    "\n",
    "- these are roughly the orders of magnitude that we are dealing with when we are pretraining these models\n",
    "    - the vocabulary size is a couple of 10K tokens\n",
    "    - the context length is around 2048 tokens (nowadays even 100,000) this governs the maximum length of the input that the model will look at when it is trying to predict the next token\n",
    "    - notice that LLama has 65B parameters, much smaller than GPT-3 which has 175B parameters, but is significantly more powerful because it is trained for significantly longer and on more data (1.4T tokens!! instead of 300B tokens for GPT-3)\n",
    "        - so, we should not judge the power of the model by the number of parameters alone\n",
    "    \n",
    "\n",
    "#### Pretraining\n",
    "\n",
    "![image-4.png](assets/pre_training_batches.png)\n",
    "\n",
    "- we take all the documents and append the `EOS` token at the end of each document, now we have concatenated all documents into one long stream of tokens\n",
    "- we then chunk this stream into chunks of a context length and create batches of them (just like what we did in the previous notebook)\n",
    "- so we basically took these documents and packed them into rows and we delimited them by the `EOS` token, telling the transformer where a new document begins\n",
    "\n",
    "- the rest is as we studied in the previous notebook\n",
    "\n",
    "![image-5.png](assets/training_example.png)\n",
    "\n",
    "\n",
    "#### Training curve examples\n",
    "\n",
    "- these are the kind of plots we look at when we are doing model pre-training (the loss curve)\n",
    "\n",
    "![image-6.png](assets/training_curve_examples.png)\n",
    "\n",
    "#### Model result (base model)\n",
    "\n",
    "- the first thing that the field noticed is that the base model learns very powerful and general representations, and that it is possible to fine-tune them for any arbitrary task we want\n",
    "\n",
    "- so if we are interested in sentiment classification\n",
    "    - the old approach is to collect a bunch of +ve and -ve reviews and train an nlp model for that\n",
    "    - the new approach is to go off and do large language model pre-training, train a base model, and then fine-tune it on the sentiment classification task using few examples\n",
    "        - it works very well in practice because the base model was forced to multi-task a huge amount of tasks in the language modeling task (it is forced to understand a lot about the structure of language and the different concepts just for the sake of predicting the next token)\n",
    "\n",
    "- all that was GPT-1\n",
    "\n",
    "- around the time of GPT-2, people noticed that actually even better than fine-tuning, we can actually prompt these base models effectively\n",
    "    - these are language models, and they want to complete documents,so we can trick them into performing tasks just by arranging the prompt in a specific way\n",
    "\n",
    "    - for example to get the base model to make question-answering, we can just give it a document of a passage then multiple Q-A pairs (called few-shot prompt), then a question and it will actually complete the document by answering the question\n",
    "\n",
    "- an example of prompt engineering a base model\n",
    "![image-7.png](assets/prompt_engineering_base_model.png)\n",
    "\n",
    "- this has kicked off the era of prompting over fine-tuning and seeing that this actually can work extremely well on a lot of problems even without any fine-tuning\n",
    "\n",
    "- since then, we have seen a lot of base models being released, not all of them are available, and currently the best base models are the LLama series from meta (not commercially licensed)\n",
    "    - that is because GPT-3 and GPT-4 are not base models, they are GPT-Assistants (fine-tuned models)\n",
    "\n",
    "- so, notice that the base models are not assistants, they don't want to answer our questions or do tasks for us, they just want to complete documents\n",
    "    - so, if we tell them write a poem about something, or tell us a story about something, they will probably complete it with more questions or undefined behavior\n",
    "    - but if we prompt them with a part of a poem or a story, they will complete it\n",
    "\n",
    "![image-8.png](assets/base_models_are_not_assistants.png)\n",
    "\n",
    "- we can even trick base models into being assistants\n",
    "    - we can do this by creating a specific few-shot promot that makes it look like there is a conversation going on between a human and assistant, and they are exchanging information, then put our query at the end and the base model will condition itself on the document (condition itself into being a helpful assistant and answer)\n",
    "        - although this is not very reliable, and doesn't work super well in practice\n",
    "\n",
    "![image-9.png](assets/few_shot_prompt_base_models.png)\n",
    "- so, we have a different path to make actual GPT assistants, not just base models (document completers)\n",
    "    - so that takes us into the 3 fine-tuning stages (mine: so do we do the fine-tuning if we want them to be assistants?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Fine-Tuning (SFT)\n",
    "\n",
    "- in the supervised fine-tuning stage, we are going to collect small but high quality datasets\n",
    "- we asked human contractors to gather data of the form prompt -> ideal response\n",
    "    - collected 10K-100K of these\n",
    "- so, we still do language modeling, nothing is changed algorithmically, we are just swapping out a training set\n",
    "    - so, it used to be internet documents, which is high quantity and low quality\n",
    "    - now it is high quality and low quantity (Q-A prompt-response data)\n",
    "\n",
    "- after training on this stage we get an SFT model\n",
    "    - we can deploy them, and they are actual assistants and they work to some extent\n",
    "\n",
    "\n",
    "#### SFT Dataset example\n",
    "\n",
    "- here is something that a human contractor might come up with\n",
    "\n",
    "![image.png](assets/sft_dataset_example.png)\n",
    "\n",
    "in the image we see a prompt and the ideal response by a human following some instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLHF models: reinforcement learning from human feedback\n",
    "\n",
    "- that consists of both stage 3 and 4\n",
    "    - reward modeling\n",
    "    - reinforcement learning\n",
    "\n",
    "\n",
    "#### Reward Modeling\n",
    "- in the reward modeling step, we will shift the data collection to be of the form of comparisons\n",
    "    - we have the same prompt, and then we take the SFT model (already trained on human data) and create multiple completions\n",
    "    - then we ask people to rank these completions\n",
    "        - this can take people even hours for single prompt -> cmpletions pair\n",
    "\n",
    "    ![image.png](assets/RM_dataset.png)\n",
    "    \n",
    "    - then, we follow that with something that looks very much like binary classification on all the possible pairs between these completions\n",
    "    ![image-2.png](assets/reward_modeling.png)\n",
    "        - we lay out the prompt in rows, the prompt is identical (same prompt in blue), with the different completions (in yellow), then append a special reward token (in green) to each completion (mine: a reward token followed by the actual score)\n",
    "            - and train the transformer at the single green token (mine: that is why we said it is something like binary classification)\n",
    "            - so the transformer will predict some reward for how good that completion is for that prompt \n",
    "                - and we use the ranks given by people to make it predict higher rewards for the better completions (we formulate it into a loss function) and train the model to make reward prediction that is consistent with the ground truth coming from the comparisons of the contractors\n",
    "        - now it will be able to score how good a completion is for a prompt\n",
    "    - once trained, this is not a model that we deploy, it is a model that we use to generate rewards for the next stage\n",
    "\n",
    "\n",
    "#### Reinforcement Learning\n",
    "\n",
    "\n",
    "- we can now score the quality of any completion for any prompt\n",
    "- so, we get a large collection of prompts, and now we do reinforcement learning with respect to the reward model\n",
    "\n",
    "![image-3.png](assets/RL_learning.png)\n",
    "\n",
    "- now we take the model initialized at the SFT, we take a single prompt, and we generate completions, and then append the reward token\n",
    "- then read-off the reward according to the trained reward model now\n",
    "    - it tells us the quality of every single completion for every prompt\n",
    "- then we apply the same language modeling on the yellow tokens (completions) but weighted by rewards given by the reward model\n",
    "    - for example in the above image, the first completion is given a reward of 1.0 (high reward), so all the tokens we sampled in the first row will be reinforced and get higher probabilities for the future\n",
    "    - the second completion was given a reward of -1.2 (bad reward), so for every single token we sampled in the second row will get lower probabilities for the future\n",
    "\n",
    "- then all the completions will score high according to the reward model trained on the previous stage\n",
    "\n",
    "#### Why we want to do RLHF?\n",
    "\n",
    "- it just works better \n",
    "    - humans just prefer tokens that come from RLHF models, compared to SFT models, and also compared to base models that are prompted to be assistants\n",
    "\n",
    "![image-4.png](assets/performance_vs_finetuning.png)\n",
    "\n",
    "- it is easy to compare vs generate \n",
    "    - a contractor might find it hard to prepare a good poem for the SFT, but will find it easy to compare between poems generated (the dataset for the reward model)\n",
    "    - so it is a better way to laverage human judgment to create better models\n",
    "\n",
    "\n",
    "- note that there are some cases where base models are better than RLHF models\n",
    "    - people noticed that RLHF models (and SFT) lose some diversity in the completions\n",
    "        - in other words, they output samples with lower variations (since we alligned them towards some tasks)\n",
    "    - so, if we have a case where we have n things and want to generate more things like it (makemore), the base models are much better \n",
    "\n",
    "![image-5.png](assets/mode_collapse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so we have base models\n",
    "- we have SFT models\n",
    "    - like vicuna-13B and so on\n",
    "- we have RLHF models\n",
    "    - like ChatGPT, and claude\n",
    "\n",
    "so the best assistants are RLHF models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to use GPT-Assistance effectively for our applications\n",
    "\n",
    "- we are done with the training, and now we want to see how to apply GPT assistants to our problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Text Generation vs LLM Text Generation\n",
    "\n",
    "- how would a human generate this sentence `California's population is 53 times that of Alaska`? \n",
    "    - first let's get their populations\n",
    "        - I know that I don't know their populations in my head, so I will look it up\n",
    "            - [uses wikipedia]: ok california has 39.5M\n",
    "            - [uses wikipedia]: ok alaska has 0.7M\n",
    "    - now I should divide them by each other\n",
    "        - I know that I can't divide that using my head, (as we have decimals) so i will rely on a calculator\n",
    "            - [uses calculator]: 39.2/0.74 = 53\n",
    "    - maybe I do some reflection on the result, does 53 make sense? it is quite a large fraction\n",
    "        - but then California is the most populous state, and alaska is the least populous state, so it makes sense\n",
    "    - now we write the sentence, we might delete and modify the phrase (as we are wiritng we have this separate process of inspecting what we are writing, and judging whether it is good or not, and if not we may delete or reframe it)\n",
    "\n",
    "\n",
    "- how would a GPT genearte the same sentence?\n",
    "\n",
    "    ![image.png](assets/sentence_generation_llms.png)\n",
    "\n",
    "    - it just goes \"Token Token Token Token\"\n",
    "    - each token is roughly the same amount of computational work (unlike us we spend different times generating different parts of the sentence, like the calculation of 53)\n",
    "        - they don't very shallow when generating these tokens though, they have like 80 layers of reasoning, but still not too much\n",
    "    - they are just like token simulators (even after RLHF, they are just simulating more responses that are more pleasing to humans), and they don't know when they don't have information -unlike humans-\n",
    "        - they don't know what they are good at and what they are not good at, they just do their best to imitate the next token\n",
    "    - they also don't reflect on their generations, or apply sanity check to them, or correct their mistakes by deleting some tokens \n",
    "    - howver, they do have cognetive advantage over humans (they do have a fact-based knowledge across a vast number of areas, as they have like billions of parameters -so that is a lot of storage for alot of facts)\n",
    "        - so they have relatively large and perfect working memory \n",
    "        - so whatever fits into the context window is immediately available for the transformer -through the self-attention mechanism-\n",
    "            - so it is like a perfect memory where all information is already accessible but this memory is finite\n",
    "\n",
    "- why did we make this comparisons?\n",
    "    - that is because prompting is just making up for the differences between the LLMs and humans\n",
    "    - if our tasks require reasoning, we can't expect the transformer to do too much reasoning per token, so we have to spread out the reasoning across more and more tokens\n",
    "        - so we can't give the model a very complicated question and expect it to answer it in a single token (there is not enough time to think)\n",
    "        - in other words, models need tokens to think\n",
    "            - so we may have few shot prompts that show the transformer how the reasoning should be done (give it examples of the reasoning and it will immitate it)\n",
    "            \n",
    "            ![image-2.png](assets/few_shot_cot.png)\n",
    "            \n",
    "            -  we can tell it things like \"let's think this step by step\" because this conditions the transformer into showing its work and generate multiple tokens, eventually allowing it to do more reasoning\n",
    "            \n",
    "            ![image-3.png](assets/encourage_the_model_to_think.png)\n",
    "\n",
    "\n",
    "\n",
    "- another approach is something called `self-consistency`\n",
    "    - just like what we do when we write, we can self-reflect and correct our mistakes\n",
    "    - so, we may do a similar behaviour by getting multiple answers from the model, then have some process of finding the ones that are good and keeping them or do some voting \n",
    "        - mine: so it doesn't really correct its mistakes but we sample from large number of answers so that if it made a mistake it will be corrected by the other answers (majority of the answers)\n",
    "\n",
    "    ![image-4.png](assets/ensemble_multiple_attempts.png)\n",
    "\n",
    "\n",
    "- another approach is to simply ask them for reflection :D\n",
    "    - they actually know -specially later models like GPT-4 - when they screw up\n",
    "    - we had to prompt it because without us promting them for reflection, they don't know to revisit their answers and correct them\n",
    "\n",
    "![image-5.png](assets/self_reflection_in_later_llms.png)\n",
    "\n",
    "- prople are still playing with prompting and trying to figure out how to get the best out of these models (and bring back the abilities that we have over the LLMs in our thinking)\n",
    "    - a paper suggested `tree of thought`\n",
    "    - we maintain multiple completions for a given prompt, and score them along the way and keep the ones that make sense\n",
    "    - generally, people are exploring more techniques \n",
    "\n",
    "\n",
    "- notice that LLMs don't want to succeed :D they want to imitate, we want to succeed and we should ask for it\n",
    "    - when transformers are trained, they have training set of different quality in their data (for example we could train it on physics answers, and there could be wrong answers and expert answers that are right)\n",
    "        - so the transformer can't tell the difference between the low-quality solutions and the high-quality solutions, they will just imitate all of them (it is a language model)\n",
    "        - and so at test time, we have to ask it a good performance\n",
    "            - for example we tell it let's look at it step by step -for reasoning- to make sure we get the right answer\n",
    "            - or telling it that you are an expert of something, so it get the right answer\n",
    "\n",
    "- `tool use / plugins` another thing, when we are trying to solve problems, we lean on tools to get some information, so people thought about doing the same thing to LLMs\n",
    "    - like give them calculators, code interpreters, and so on\n",
    "    - the ability to search \n",
    "    - and since they don't know when they don't have an infomation, we can tell them specifically in the prompt\n",
    "        - like you are not very good at arithmetic, so whenever you are dealing with large numbers, use the calculator\n",
    "\n",
    "- `retreival augmented LLMs`\n",
    "\n",
    "    ![image-6.png](assets/retreival_augmented_llms.png)\n",
    "    - we went from a world of retrieval-only (searching ourselves for infrmation on the internet, no LLms), to a world of memory-only (LLMs)\n",
    "    - but there is an entire space in between of retreival-augmented models, and it works very well in practice\n",
    "        - as we mentioned, the context window of the transformer is its working memory, and if we load it with any information that is relevant to the task, it will work very well\n",
    "        - so, a lot of are interested in retreival-augmented generation \n",
    "\n",
    "    - example for it in the image is llama index (which is a data connector)\n",
    "        - and we can index all the relevant data and make it accessible to LLMs\n",
    "            - we take relevant documents\n",
    "            - split them to chunks\n",
    "            - embed all of them\n",
    "            - store that in a vector store\n",
    "            - and at test time, we make a query to the vector store and fetch relevant chunks and stuff them into the prompt and generate \n",
    "\n",
    "        - it is similar to us (we don't do everything from memory, we reference some documents or documentation to help us)\n",
    "            - like we have some memory about some of it but it is better to look it up in details, and that is the inspiration for wanting LLMs to do that too\n",
    "\n",
    "\n",
    "- `constraint prompting`\n",
    "    - that is a technique for forcing a certain templates in the output of LLMs\n",
    "    - like enfocring that the output from LLMs is Json like the example below\n",
    "    \n",
    "    ![image-7.png](assets/constraint_prompting.png)\n",
    "    ![image-8.png](assets/constraint_prompting2.png)\n",
    "\n",
    "    - they go in and mess with the probabilities of all the different tokens that come out of the transformer and they clamp those tokens and the transformer will only fill in the blanks, and they can enforce additional restrictions on what can go into these blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning and default recommendations\n",
    "\n",
    "- it is the case that we can get really far with prompt engineering (one-shot or few-shot prompt), but it is also possible to fine-tune the models (actually changing the weights, like SFT -supervised fine-tuning- and RLHF -reinforcement learning from human feedback-)\n",
    "\n",
    "    ![image.png](assets/prompt_engineering_vs_fine_tuning.png)\n",
    "\n",
    "    - it is becoming a lot more accessible to do fine-tuning in practice, thanks to a number of techniques that have been developed and have libraries for \n",
    "        - for example, parameter efficient fine-tuning (PEFT) like LoRA\n",
    "        - make sure that you are only fine-tuning small sparse pieces of the model (most of the model is kept clamped or fixed at the base model, and some pieces of it are allowed to change)\n",
    "        - this works pretty well imperically and makes it much cheaper as we tune small pieces of the model\n",
    "            - beacuse most of the model is fixed, we can use very low precision inference for computing those parts, as they will not be updated by gradient descent\n",
    "        - we also have a number of high-quality base models available (like LLama), although it is not commercially licensed right now\n",
    "    - fine-tuning is a lot more technically involved. It requires a lot more expertise to do right, and requires human contractors for datasets and/or synthetic data pipelines\n",
    "        - However, SFT is achievable, as it is relatively straight forward (same thing but with different task-related data)\n",
    "        - but RLHF is complicated and research territory, and it is harder to get to work\n",
    "            - so, it is not advisable for someone to try and roll his own RLHF implementation, as these things are unstable and not beginner-friendly\n",
    "            - they also can potentially change rapidly\n",
    "\n",
    "- so, these are the default recommendations (mine: for using them in applications)\n",
    "\n",
    "![image-2.png](assets/default_recommendations.png)\n",
    "\n",
    "- look for prompt engineering online \n",
    "    - after squeezing out prompt engineering (which we should stick with for a while), look at potentially fine-tuning a model on your application, but expect it to be a lot more slower \n",
    "\n",
    "![image-3.png](assets/use_cases.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
