{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf5a0e2",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "- univariate linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21ee6e",
   "metadata": {},
   "source": [
    "## univariate linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29aada3",
   "metadata": {},
   "source": [
    "### data representation \n",
    "| a single feature (its unit)     | the target (its unit) |\n",
    "| -------------------| ------------------------ |\n",
    "| 1.0               | 300                      |\n",
    "| 2.0               | 500                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23511d",
   "metadata": {},
   "source": [
    "### model representation \n",
    "$$ \\hat{y}(x) = wx + b \\tag{1}$$ (parameters w and b)\n",
    "### cost function (the squared error formula)\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (\\hat{y}^{(i)} - y^{(i)})^2 \\tag{2}$$ \n",
    "### applying cost function on linear regression (to measure the best b and w -with the lowest cost- )\n",
    "- the specific cost function of linear regression\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (wx^{(i)} + b - y^{(i)})^2$$\n",
    "### Gradient decent (to minimize any function)\n",
    "- For any function J with 2 parameters w and b\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "### apply Gradient descent on the specific cost function of linear regression\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39932fc0",
   "metadata": {},
   "source": [
    "## Apply on problem\n",
    "As in the lecture, you will use the motivating example of housing price prediction.  \n",
    "This lab will use a simple data set with only two data points - a house with 1000 square feet(sqft) sold for \\\\$300,000 and a house with 2000 square feet sold for \\\\$500,000. These two points will constitute our *data or training set*. In this lab, the units of size are 1000 sqft and the units of price are 1000s of dollars.\n",
    "\n",
    "| Size (1000 sqft)     | Price (1000s of dollars) |\n",
    "| -------------------| ------------------------ |\n",
    "| 1.0               | 300                      |\n",
    "| 2.0               | 500                      |\n",
    "\n",
    "You would like to fit a linear regression model (shown above as the blue straight line) through these two points, so you can then predict price for other houses - say, a house with 1200 sqft.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12757bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d843fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_predict(x,w,b):\n",
    "    \"\"\"\"\n",
    "    predicts the target given the input and parameters, we have single feature and single target\n",
    "    Args:\n",
    "      X (scalar): The input feature \n",
    "      w,b (scalars): The parameters of single variable linear regression model\n",
    "    returns\n",
    "      y (scalar): The predicted value from the model\n",
    "    \"\"\"\n",
    "    y = w * x + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70e463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_compute_cost(x,y,w,b):\n",
    "    \"\"\"\"\n",
    "    Computes the cost value for some parameters w and b\n",
    "    Args:\n",
    "      x (ndarray (m,)): The input feature for all the data \n",
    "      y (ndarray (m,)): The labels -actual true values- for the input feature\n",
    "      w,b (scalar): The model parameters \n",
    "    returns\n",
    "      cost (scalar): The cost value for the given parameters for the data \n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        err =  (w * x[i] + b - y[i])**2\n",
    "        cost += err\n",
    "    cost = cost / (2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11922376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_compute_gradient(x,y,w,b):\n",
    "    \"\"\"\"\n",
    "    Compute the derivative term in the gradient descent step\n",
    "    Args:\n",
    "      x (ndarray (m,)): The input feature for all the data \n",
    "      y (ndarray (m,)): The labels -actual true values- for the input feature\n",
    "      w,b (scalar): The model parameters \n",
    "    returns\n",
    "      dj_db (scalar): the derivative of the cost function with respect to b\n",
    "      dj_dw (scalar): the derivative of the cost function with respect to w\n",
    "    \"\"\"\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    m = x.shape[0]\n",
    "    for i in range(m):\n",
    "        dj_dw = (w * x[i] + b - y[i]) * x[i]\n",
    "        dj_db = (w * x[i] + b - y[i])\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    return dj_db,dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fc648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_gradient_descent(x,y,w_in,b_in,alpha,iteration_num,compute_gradient,compute_cost):\n",
    "    \"\"\"\n",
    "    Compute the best parameters for linear regression model with single feature\n",
    "    Args:\n",
    "      x (ndarray (m,)): the data of the input feature\n",
    "      y (ndarray (m,)): The labels -actual true values- for the input feature\n",
    "      w_in,b_in (scalar): The model initial parameters\n",
    "      alpha (scalar): the learning rate - a parameter -\n",
    "      iteration_num (scalar): the number of iterations in the gradient descent -to prevent resource consumption-\n",
    "      compute_gradient: function to compute the gradient in each step\n",
    "      compute_cost: function to compute the cost after each updated parameters w and b \n",
    "    returns\n",
    "      w,b (scalar): the best parameters for the model data \n",
    "      \n",
    "    \"\"\"\n",
    "    # Initialization before starting the algorithm\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(iteration_num):\n",
    "        dj_db,dj_dw = compute_gradient(x,y,w,b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Optional compute the cost every while\n",
    "        if i% math.ceil(iteration_num/10) == 0:\n",
    "            print(f\"The cost at iteration number {i} is {compute_cost(x,y,w,b)}\")\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91179fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost at iteration number 0 is 80803.125\n",
      "The cost at iteration number 10000 is 6.866245319043687e-25\n",
      "The cost at iteration number 20000 is 6.866245319043687e-25\n",
      "The cost at iteration number 30000 is 6.866245319043687e-25\n",
      "The cost at iteration number 40000 is 6.866245319043687e-25\n",
      "The cost at iteration number 50000 is 6.866245319043687e-25\n",
      "The cost at iteration number 60000 is 6.866245319043687e-25\n",
      "The cost at iteration number 70000 is 6.866245319043687e-25\n",
      "The cost at iteration number 80000 is 6.866245319043687e-25\n",
      "The cost at iteration number 90000 is 6.866245319043687e-25\n",
      "The best parametes found are w = 199.99999999999943 and b = 99.99999999999972\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([1.0, 2.0])\n",
    "y_train = np.array([300.0, 500.0])\n",
    "w_in = 0\n",
    "b_in = 0\n",
    "alpha = 0.01\n",
    "iteration_num = 100000\n",
    "w,b = univariate_gradient_descent(X_train,y_train,w_in,b_in,alpha,iteration_num,univariate_compute_gradient,univariate_compute_cost)\n",
    "print(f\"The best parametes found are w = {w} and b = {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78905ee1",
   "metadata": {},
   "source": [
    "## Vectorized linear regression single variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7ef130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_compute_cost_vect(x,y,w,b): #correct\n",
    "    \"\"\"\"\n",
    "    Computes the cost value for some parameters w and b\n",
    "    Args:\n",
    "      x (ndarray (m,)): The input feature for all the data \n",
    "      y (ndarray (m,)): The labels -actual true values- for the input feature\n",
    "      w,b (scalar): The model parameters \n",
    "    returns\n",
    "      cost (scalar): The cost value for the given parameters for the data \n",
    "    \"\"\"\n",
    "    error = np.subtract(np.add(np.multiply(x,w),b),y)\n",
    "    error_sqrd = np.square(error)\n",
    "    error_sqrd /= 2\n",
    "    return error_sqrd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013cc21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "err = univariate_compute_cost_vect(X_train,y_train,200,100)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56ad51cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_compute_gradient_vect(x,y,w,b): #correct\n",
    "    \"\"\"\"\n",
    "    Compute the derivative term in the gradient descent step\n",
    "    Args:\n",
    "      x (ndarray (m,)): The input feature for all the data \n",
    "      y (ndarray (m,)): The labels -actual true values- for the input feature\n",
    "      w,b (scalar): The model parameters \n",
    "    returns\n",
    "      dj_db (scalar): the derivative of the cost function with respect to b\n",
    "      dj_dw (scalar): the derivative of the cost function with respect to w\n",
    "    \"\"\"\n",
    "    dj_db = np.subtract(np.add(np.multiply(x,w),b),y)\n",
    "    dj_dw = np.multiply(dj_db,x)\n",
    "    return dj_db.mean(),dj_dw.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c11e74",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6639d282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost at iteration number 0 is 80803.125\n",
      "The cost at iteration number 1000 is 8.25446428560369e-18\n",
      "The cost at iteration number 2000 is 6.866245319043687e-25\n",
      "The cost at iteration number 3000 is 6.866245319043687e-25\n",
      "The cost at iteration number 4000 is 6.866245319043687e-25\n",
      "The cost at iteration number 5000 is 6.866245319043687e-25\n",
      "The cost at iteration number 6000 is 6.866245319043687e-25\n",
      "The cost at iteration number 7000 is 6.866245319043687e-25\n",
      "The cost at iteration number 8000 is 6.866245319043687e-25\n",
      "The cost at iteration number 9000 is 6.866245319043687e-25\n",
      "The cost at iteration number 0 is 79274.8125\n",
      "The cost at iteration number 1000 is 3.4125109319154174\n",
      "The cost at iteration number 2000 is 0.7928950684538176\n",
      "The cost at iteration number 3000 is 0.1842287401041018\n",
      "The cost at iteration number 4000 is 0.04280544807338754\n",
      "The cost at iteration number 5000 is 0.0099458226969803\n",
      "The cost at iteration number 6000 is 0.0023109065217603642\n",
      "The cost at iteration number 7000 is 0.0005369378798549629\n",
      "The cost at iteration number 8000 is 0.00012475722583692557\n",
      "The cost at iteration number 9000 is 2.8987273914308145e-05\n",
      "The w from the looping algorithm is 199.99999999999943 and the b is 99.99999999999972 it took 0.06249284744262695\n",
      "The w from the vectorized algorithm is 199.99285075131766 and the b is 100.011567727362 it took 0.40625691413879395\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "X_train = np.array([1.0, 2.0])\n",
    "y_train = np.array([300.0, 500.0])\n",
    "w_in = 0\n",
    "b_in = 0\n",
    "alpha = 0.01\n",
    "iteration_num = 10000\n",
    "tic1 = time.time()\n",
    "w,b = univariate_gradient_descent(X_train,y_train,w_in,b_in,alpha,iteration_num,univariate_compute_gradient,univariate_compute_cost)\n",
    "toc1 = time.time()\n",
    "tic2 = time.time()\n",
    "wv,bv = univariate_gradient_descent(X_train,y_train,w_in,b_in,alpha,iteration_num,univariate_compute_gradient_vect,univariate_compute_cost_vect)\n",
    "toc2 = time.time()\n",
    "print(f\"The w from the looping algorithm is {w} and the b is {b} it took {toc1-tic1}\")\n",
    "print(f\"The w from the vectorized algorithm is {wv} and the b is {bv} it took {toc2-tic2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a57e4",
   "metadata": {},
   "source": [
    "## Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31611c4b",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.3\"></a>\n",
    "## 1.3 Notation\n",
    "Here is a summary of some of the notation you will encounter, updated for multiple features.  \n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example maxtrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b24fca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    acc = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        acc += x[i] * w[i]\n",
    "    acc += b\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0eb5c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    cost = 0\n",
    "    m = X.shape[0]\n",
    "    for i in range(m):\n",
    "        y_prid = np.dot(w,X[i]) + b\n",
    "        cost = cost + (y_prid - y[i])**2\n",
    "    cost = cost/ (2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "557c4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): # This is one iteration in the GD Algorithm\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    dj_db = 0\n",
    "    dj_dw = np.zeros_like(w)\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    for i in range(m):\n",
    "        dj_db += (np.dot(w,X[i]) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += (np.dot(w,X[i]) + b - y[i]) * X[i][j]\n",
    "    dj_db /= m\n",
    "    dj_dw /= m\n",
    "    return dj_db,dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1848ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    w = copy.deepcopy(w_in) #why?\n",
    "    b = b_in\n",
    "    cost_history = []\n",
    "    for i in range(num_iters):\n",
    "        dj_db,dj_dw = gradient_function(X,y,w,b)\n",
    "        w = w - alpha * dj_dw # Vector of size n\n",
    "        b = b - alpha * dj_db # Scalar \n",
    "        if i < 100000:\n",
    "            cost_history.append(cost_function(X,y,w,b))\n",
    "            \n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {cost_history[-1]:8.2f}   \")\n",
    "    return w,b,cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e5eff2",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2\"></a>\n",
    "# 2 Problem Statement\n",
    "\n",
    "You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  \n",
    "\n",
    "Please run the following code cell to create your `X_train` and `y_train` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9c18b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45],[1416,3,2,40],[852,2,1,35]])\n",
    "y_train = np.array([460,232,178])\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "52d5ac59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2104,    5,    1,   45],\n",
       "       [1416,    3,    2,   40],\n",
       "       [ 852,    2,    1,   35]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1b44146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459.9999976194083"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_wb = predict_single_loop(X_train[0], w_init, b_init)\n",
    "f_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f5bd7935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.667216819864136e-12\n",
      "8.32604675173059e-12\n",
      "9.347342657379977e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5578904428966628e-12"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(X_train, y_train, w_init, b_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171ad44",
   "metadata": {},
   "source": [
    "**Expected Result**: Cost at optimal w : 1.5578904045996674e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9c64578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.6739251501955248e-06,\n",
       " array([-2.72623577e-03, -6.27197263e-06, -2.21745578e-06, -6.92403391e-05]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(X_train, y_train, w_init, b_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ea597",
   "metadata": {},
   "source": [
    "**Expected Result**:   \n",
    "dj_db at initial w,b: -1.6739251122999121e-06  \n",
    "dj_dw at initial w,b:   \n",
    " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7cb0778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration   10: Cost   696.96   \n",
      "Iteration   20: Cost   696.85   \n",
      "Iteration   30: Cost   696.74   \n",
      "Iteration   40: Cost   696.64   \n",
      "Iteration   50: Cost   696.53   \n",
      "Iteration   60: Cost   696.42   \n",
      "Iteration   70: Cost   696.31   \n",
      "Iteration   80: Cost   696.21   \n",
      "Iteration   90: Cost   696.10   \n",
      "b,w found by gradient descent: -0.00,[ 0.20234987  0.00079467 -0.0009851  -0.00212511] \n",
      "prediction: 425.65, target value: 460\n",
      "prediction: 286.44, target value: 232\n",
      "prediction: 172.33, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 100\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb130e",
   "metadata": {},
   "source": [
    "**Expected Result**:    \n",
    "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07]   \n",
    "prediction: 426.19, target value: 460  \n",
    "prediction: 286.17, target value: 232  \n",
    "prediction: 171.47, target value: 178  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8401b5",
   "metadata": {},
   "source": [
    "## Vector notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26386b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_vect(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    P = np.dot(x,w) + b\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30d8c739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459.9999976194083"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_wb = predict_vect(X_train[0], w_init, b_init)\n",
    "f_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "11e5c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_vect(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    wT = w.reshape((1,-1))\n",
    "    yprid = np.matmul(wT,X.T).T + b\n",
    "    y = y.reshape((-1,1))\n",
    "    sqrd_err = (yprid - y)**2\n",
    "    return sqrd_err.mean() / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5ceb77ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5578904428966628e-12"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost_vect(X_train, y_train, w_init, b_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "58515ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_vect(X, y, w, b): # This is one iteration in the GD Algorithm\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    wT = w.reshape((1,-1))\n",
    "    yprid = np.matmul(wT,X.T).T + b\n",
    "    y = y.reshape((-1,1))\n",
    "    err = yprid - y\n",
    "    dj_dw = np.matmul(err.T,X)\n",
    "    dj_db = err.mean()\n",
    "    dj_dw /= m\n",
    "    return dj_db,dj_dw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "980340d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.72623577e-03 -6.27197263e-06 -2.21745578e-06 -6.92403391e-05]\n",
      "-1.6739251501955248e-06\n"
     ]
    }
   ],
   "source": [
    "dj_db, dj_dw = compute_gradient_vect(X_train, y_train, w_init, b_init)\n",
    "print(dj_dw,dj_db,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5044c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.72623577e-03 -6.27197263e-06 -2.21745578e-06 -6.92403391e-05]\n",
      "-1.6739251501955248e-06\n"
     ]
    }
   ],
   "source": [
    "dj_db,dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(dj_dw,dj_db,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ef40abb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration   10: Cost   696.96   \n",
      "Iteration   20: Cost   696.85   \n",
      "Iteration   30: Cost   696.74   \n",
      "Iteration   40: Cost   696.64   \n",
      "Iteration   50: Cost   696.53   \n",
      "Iteration   60: Cost   696.42   \n",
      "Iteration   70: Cost   696.31   \n",
      "Iteration   80: Cost   696.21   \n",
      "Iteration   90: Cost   696.10   \n",
      "b,w found by gradient descent: -0.00,[ 0.20234987  0.00079467 -0.0009851  -0.00212511] \n",
      "prediction: 425.65, target value: 460\n",
      "prediction: 286.44, target value: 232\n",
      "prediction: 172.33, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 100\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost_vect, compute_gradient_vect, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27457401",
   "metadata": {},
   "source": [
    "**Expected Result**:    \n",
    "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07]   \n",
    "prediction: 426.19, target value: 460  \n",
    "prediction: 286.17, target value: 232  \n",
    "prediction: 171.47, target value: 178  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa82884d",
   "metadata": {},
   "source": [
    "## Compare speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "41c8b4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration   10: Cost   696.96   \n",
      "Iteration   20: Cost   696.85   \n",
      "Iteration   30: Cost   696.74   \n",
      "Iteration   40: Cost   696.64   \n",
      "Iteration   50: Cost   696.53   \n",
      "Iteration   60: Cost   696.42   \n",
      "Iteration   70: Cost   696.31   \n",
      "Iteration   80: Cost   696.21   \n",
      "Iteration   90: Cost   696.10   \n",
      "Iteration    0: Cost  2529.46   \n",
      "Iteration   10: Cost   696.96   \n",
      "Iteration   20: Cost   696.85   \n",
      "Iteration   30: Cost   696.74   \n",
      "Iteration   40: Cost   696.64   \n",
      "Iteration   50: Cost   696.53   \n",
      "Iteration   60: Cost   696.42   \n",
      "Iteration   70: Cost   696.31   \n",
      "Iteration   80: Cost   696.21   \n",
      "Iteration   90: Cost   696.10   \n",
      "The w from the looping algorithm is [ 0.20234987  0.00079467 -0.0009851  -0.00212511] and the b is -0.00011745590317761635 it took 0.046881914138793945\n",
      "The w from the vectorized algorithm is [ 0.20234987  0.00079467 -0.0009851  -0.00212511] and the b is -0.00011745590317761635 it took 0.015610694885253906\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic1 = time.time()\n",
    "w_final1, b_final1, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "toc1 = time.time()\n",
    "tic2 = time.time()\n",
    "w_final2, b_final2, J_hist2 = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost_vect, compute_gradient_vect, \n",
    "                                                    alpha, iterations)\n",
    "toc2 = time.time()\n",
    "print(f\"The w from the looping algorithm is {w_final} and the b is {b_final1} it took {toc1-tic1}\")\n",
    "print(f\"The w from the vectorized algorithm is {w_final2} and the b is {b_final2} it took {toc2-tic2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
